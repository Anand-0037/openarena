Directory structure:
‚îî‚îÄ‚îÄ bittensor-ideathon/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ dashboard.html
    ‚îú‚îÄ‚îÄ demo.py
    ‚îú‚îÄ‚îÄ hackathon-details.md
    ‚îú‚îÄ‚îÄ INCENTIVE_MECHANISM.md
    ‚îú‚îÄ‚îÄ leaderboard_data.json
    ‚îú‚îÄ‚îÄ openarena-pitch-deck.md
    ‚îú‚îÄ‚îÄ openarena-proposal.md
    ‚îú‚îÄ‚îÄ openarena-social-post.md
    ‚îú‚îÄ‚îÄ openarena-video-script.md
    ‚îú‚îÄ‚îÄ WHITE_PAPER.md
    ‚îú‚îÄ‚îÄ winning-strategy.md
    ‚îú‚îÄ‚îÄ winning-strategy.md.resolved
    ‚îú‚îÄ‚îÄ deliverables/
    ‚îÇ   ‚îú‚îÄ‚îÄ pitch_deck.md
    ‚îÇ   ‚îú‚îÄ‚îÄ social_thread.md
    ‚îÇ   ‚îî‚îÄ‚îÄ video_script.md
    ‚îú‚îÄ‚îÄ neurons/
    ‚îÇ   ‚îú‚îÄ‚îÄ miner.py
    ‚îÇ   ‚îî‚îÄ‚îÄ validator.py
    ‚îú‚îÄ‚îÄ openarena/
    ‚îÇ   ‚îú‚îÄ‚îÄ protocol.py
    ‚îÇ   ‚îî‚îÄ‚îÄ utils/
    ‚îÇ       ‚îî‚îÄ‚îÄ crypto.py
    ‚îú‚îÄ‚îÄ submission/
    ‚îÇ   ‚îú‚îÄ‚îÄ Incentive_Mechanism.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_PitchDeck.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_SourceCode.tar.gz
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_VideoScript.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_Whitepaper.md
    ‚îÇ   ‚îî‚îÄ‚îÄ SUBMISSION.md
    ‚îî‚îÄ‚îÄ tests/
        ‚îî‚îÄ‚îÄ test_entropy.py

================================================
FILE: README.md
================================================
# OpenArena: The Coliseum of Intelligence

![OpenArena Logo](openarena_logo.png)

> **"The only true measure of intelligence is the ability to adapt to the unknown."**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Bittensor](https://img.shields.io/badge/Bittensor-Subnet-blue)](https://bittensor.com/)

---

## üèõÔ∏è Overview

**OpenArena** is a decentralized, adversarial benchmarking platform built on **Bittensor**. It solves the "Crisis of Evaluation" in AI by moving beyond static datasets (which models memorize) to dynamic, human-generated challenges.

We introduce **Proof of Intelligence (PoI)**: A mechanism where miners are ranked not by their ability to answer fixed questions, but by their ability to generalize to novel, high-complexity problems submitted by the world's best data scientists via **KaggleIngest**.

## üöÄ Key Features

- **Dynamic Evaluation**: Challenges are constantly evolving, preventing overfitting and memorization.
- **KaggleIngest Portal**: Exclusive bridge onboarding 15M+ Kaggle data scientists to monetize their expertise by breaking models.
- **Brier Score Calibration**: A rigorous scoring rule that penalizes hallucinations. Miners must know what they don't know.
- **Commit-Reveal Mechanism**: Cryptographically secure protocol to prevent front-running and plagiarism.

## üõ†Ô∏è Architecture

### The Arena (Validator)

The Validator acts as the "Gamemaster," orchestrating the flow of challenges and verifying the integrity of the competition.

- **Entropy Source**: Derivates unpredictability from on-chain block hashes.
- **Scoring Engine**: Implements the Brier Score decomposition for accuracy and calibration.

### The Gladiator (Miner)

Miners are the AI models entering the arena.

- **Adaptive Inference**: Leverages state-of-the-art LLMs (Llama 3, Mistral, GPT-4o) to solve reasoning tasks.
- **Self-Correction**: Internal loops to verify answers before commitment.

## ‚ö° Quick Start

### Prerequisites

- Python 3.10+
- Bittensor
- Torch & Transformers

### Installation

```bash
git clone https://github.com/your-username/openarena.git
cd openarena
pip install -r requirements.txt
pip install -e .
```

### Running a Miner

```bash
python neurons/miner.py --netuid <your_netuid> --wallet.name <your_wallet> --wallet.hotkey <your_hotkey> --logging.debug
```

### Running a Validator

```bash
python neurons/validator.py --netuid <your_netuid> --wallet.name <your_wallet> --wallet.hotkey <your_hotkey> --logging.debug
```

## üìú Roadmap

- [x] **Phase 1: Foundation**: Core Commit-Reveal Protocol, Basic Scoring.
- [ ] **Phase 2: The Bridge**: KaggleIngest Integration & Bounty Smart Contracts.
- [ ] **Phase 3: The Coliseum**: 3D Visualization of Model Battles.
- [ ] **Phase 4: AGI**: Recursive Self-Improvement Loops.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built for the Bittensor Ideathon 2026.**



================================================
FILE: dashboard.html
================================================
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>OpenArena Leaderboard (Powered by KaggleIngest)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
      body {
        background-color: #0f1117;
        color: #e2e8f0;
        font-family: "Inter", sans-serif;
      }
      .card {
        background-color: #1e293b;
        border: 1px solid #334155;
      }
    </style>
  </head>
  <body class="p-8">
    <!-- Header -->
    <div class="flex justify-between items-center mb-8">
      <div>
        <h1
          class="text-3xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-blue-400 to-purple-500"
        >
          OpenArena
          <span class="text-sm text-gray-400 font-mono">Testnet Beta</span>
        </h1>
        <p class="text-gray-400">Decentralized Adversarial Evaluation</p>
      </div>
      <div class="flex gap-4">
        <span
          class="px-4 py-2 bg-green-900/30 text-green-400 rounded border border-green-800 animate-pulse"
        >
          ‚óè Live Phase: Commit
        </span>
        <span class="px-4 py-2 bg-blue-600 rounded font-bold"
          >Connect Wallet</span
        >
      </div>
    </div>

    <!-- Stats Grid -->
    <div class="grid grid-cols-1 md:grid-cols-4 gap-6 mb-8">
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Active Miners</h3>
        <p class="text-2xl font-bold">256</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Current Epoch</h3>
        <p class="text-2xl font-bold">#4,291</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Avg Generalization Score</h3>
        <p class="text-2xl font-bold text-yellow-400">0.872</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Next Challenge</h3>
        <p class="text-md font-mono text-purple-400">MATH::Polynomial_Roots</p>
      </div>
    </div>

    <!-- Main Content -->
    <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
      <!-- Leaderboard -->
      <div class="lg:col-span-2 card rounded-xl overflow-hidden">
        <div class="p-6 border-b border-gray-700 flex justify-between">
          <h2 class="text-xl font-bold">Live Generalization Leaderboard</h2>
          <button class="text-sm text-blue-400 hover:text-blue-300">
            View All
          </button>
        </div>
        <table class="w-full text-left">
          <thead class="bg-gray-800 text-gray-400">
            <tr>
              <th class="p-4">Rank</th>
              <th class="p-4">Miner ID</th>
              <th class="p-4">Score (G)</th>
              <th class="p-4">Latency</th>
              <th class="p-4">Status</th>
            </tr>
          </thead>
          <tbody id="leaderboard-body" class="divide-y divide-gray-700">
            <!-- Rows will be injected here -->
          </tbody>
        </table>
      </div>

      <!-- Live Feed -->
      <div class="card rounded-xl p-6">
        <h2 class="text-xl font-bold mb-4">Live Activity Log</h2>
        <div
          id="activity-log"
          class="space-y-3 font-mono text-sm max-h-[400px] overflow-y-auto"
        >
          <!-- Logs injected here -->
        </div>
      </div>
    </div>

    <script>
      // Start Mock Data Injection immediately
      const miners = [
        { id: "5H...x92", score: 0.982, lat: "45ms", status: "Verified" },
        { id: "5C...k21", score: 0.941, lat: "120ms", status: "Verified" },
        { id: "5D...j88", score: 0.81, lat: "410ms", status: "Verified" },
        { id: "5K...m11", score: 0.0, lat: "-", status: "Slash (Copycat)" },
      ];

      const tbody = document.getElementById("leaderboard-body");
      miners.forEach((m, i) => {
        const row = `
                <tr class="hover:bg-gray-800/50">
                    <td class="p-4 font-bold text-gray-500">#${i + 1}</td>
                    <td class="p-4 font-mono text-blue-300">${m.id}</td>
                    <td class="p-4 font-bold ${m.score > 0.9 ? "text-green-400" : "text-gray-300"}">${m.score.toFixed(3)}</td>
                    <td class="p-4 text-gray-400">${m.lat}</td>
                    <td class="p-4">
                        <span class="px-2 py-1 rounded text-xs ${m.score > 0 ? "bg-green-900 text-green-300" : "bg-red-900 text-red-300"}">
                            ${m.status}
                        </span>
                    </td>
                </tr>
            `;
        tbody.innerHTML += row;
      });

      // Mock Live Activity
      const logs = document.getElementById("activity-log");
      const actions = [
        "Miner 5H...x92 committed hash 0x7f2...",
        "Validator generated new task: 12 + 99",
        "Miner 5C...k21 revealed answer: 111",
        "Miner 5K...m11 penalized for late reveal",
        "New Block Finalized #4392",
      ];

      actions.forEach((a, i) => {
        setTimeout(() => {
          const div = document.createElement("div");
          div.className =
            "p-2 bg-black/20 rounded border-l-2 border-blue-500 animate-fade-in";
          div.innerHTML = `<span class="text-gray-500">[${new Date().toLocaleTimeString()}]</span> ${a}`;
          logs.prepend(div);
        }, i * 800);
      });
    </script>
  </body>
</html>



================================================
FILE: demo.py
================================================
import time
import json
import random
import hashlib
from typing import List, Dict

# Mock Protocol
class Synapse:
    def __init__(self, query: str):
        self.query = query
        self.answer = None
        self.salt = None
        self.commitment = None
        self.miner_id = None
        self.score = 0.0

# Mock Validator
class Validator:
    def __init__(self, uid, name):
        self.uid = uid
        self.name = name

    def generate_task(self) -> str:
        ops = ['+', '-', '*']
        a, b = random.randint(10, 99), random.randint(1, 9)
        op = random.choice(ops)
        return f"{a} {op} {b}"

    def solve_ground_truth(self, task: str) -> str:
        return str(eval(task))

    def evaluate(self, task: str, synapse: Synapse) -> float:
        # Check Commitment
        expected_hash = hashlib.sha256(f"{synapse.answer}{synapse.salt}{synapse.miner_id}".encode()).hexdigest()
        if synapse.commitment != expected_hash:
             # Malformed/Fake commit
            return 0.0

        ground_truth = self.solve_ground_truth(task)
        if synapse.answer == ground_truth:
            return 1.0
        return 0.0

# Mock Miner (Base Class)
class MockMiner:
    def __init__(self, uid: int, name: str):
        self.uid = uid
        self.name = name
        self.mempool = [] # Simulating visibility of others' commits

    def commit(self, task: str) -> tuple:
        # Default honest behavior
        answer = str(eval(task))
        salt = str(random.randint(1000, 9999))
        commitment = hashlib.sha256(f"{answer}{salt}{self.uid}".encode()).hexdigest()
        return commitment, answer, salt

    def reveal(self, task: str, answer: str, salt: str) -> dict:
        return {"answer": answer, "salt": salt}

# Malicious Miner: The Front-Runner (Copycat)
class FrontRunnerMiner(MockMiner):
    def commit(self, task: str) -> tuple:
        # Tries to copy, but can't see the salt/answer until Reveal phase.
        # This simulates the failure of front-running in Commit-Reveal.
        # He submits a random hash hoping to get lucky or replay an old one.
        fake_ans = "0"
        salt = "0000"
        commitment = hashlib.sha256(f"{fake_ans}{salt}{self.uid}".encode()).hexdigest()
        return commitment, fake_ans, salt

# Simulation Loop
def run_simulation(epochs=15):
    validator = Validator(0, "Validator_Main")

    miners = [
        MockMiner(0, "Miner_Alphazero (Honest)"),
        MockMiner(1, "Miner_GPT4 (Honest)"),
        FrontRunnerMiner(2, "Miner_Copycat (Malicious)"),
        MockMiner(3, "Miner_Lazy (Random)"),
    ]

    leaderboard_data = []

    # Weight Tracking
    miner_weights = {m.uid: 0.5 for m in miners} # Start equal

    print(f"--- Starting Adversarial Simulation ({epochs} Epochs) ---")

    for epoch in range(epochs):
        task = validator.generate_task()
        print(f"\n[Epoch {epoch+1}] New Task: {task}")

        # Phase 1: Commit
        commits = {}
        secrets = {}
        for miner in miners:
            com, ans, salt = miner.commit(task)
            commits[miner.uid] = com
            secrets[miner.uid] = (ans, salt)

            # Simulate Copycat trying to peek (but only seeing hashes)
            if isinstance(miner, FrontRunnerMiner):
                 print(f"  > {miner.name} is scanning mempool... only sees hashes.")

        # Phase 2: Reveal
        epoch_scores = {}
        for miner in miners:
            synapse = Synapse(task)
            synapse.miner_id = miner.uid
            synapse.commitment = commits[miner.uid]

            # Miner reveals
            revealed = miner.reveal(task, *secrets[miner.uid])
            synapse.answer = revealed['answer']
            synapse.salt = revealed['salt']

            # Validator scores
            score = validator.evaluate(task, synapse)

            # Additional logic for "Lazy" miner
            if miner.name == "Miner_Lazy (Random)":
                synapse.answer = str(random.randint(0,100))
                score = validator.evaluate(task, synapse) # Re-evaluate with wrong answer

            epoch_scores[miner.uid] = score

            # Update Moving Average Weights (Yuma-lite)
            alpha = 0.2
            miner_weights[miner.uid] = (1 - alpha) * miner_weights[miner.uid] + (alpha * score)

            status = "Verified" if score > 0 else "Failed/Slashed"
            print(f"  > {miner.name}: {status} (Score: {score:.2f})")

            leaderboard_data.append({
                "epoch": epoch,
                "miner": miner.name,
                "score": miner_weights[miner.uid], # Plotting Weight Convergence
                "latency": random.uniform(0.1, 0.5)
            })

    # Export
    with open("leaderboard_data.json", "w") as f:
        json.dump(leaderboard_data, f, indent=2)

    print("\n--- Simulation Complete. Weights Converged. ---")

if __name__ == "__main__":
    run_simulation()



================================================
FILE: hackathon-details.md
================================================
# Bittensor Subnet Ideathon: The Ultimate Guide (OpenArena Edition)

> [!IMPORTANT]
> **Strategic Pivot**: We are now fully committed to **OpenArena**, leveraging your KaggleIngest background to solve the AI Benchmark Crisis. This document contains everything you need to win.

---

## 1. The Strategy: Why OpenArena Wins

### üß† The Core Thesis

Static benchmarks (GSM8K, MMLU) are dead because models memorize them.
**OpenArena** is the "Continuous Kaggle":

- **Validators** generate _fresh_, unseen tasks every epoch.
- **Miners** must generalize instantly.
- **Proof of Intelligence** = Ability to adapt to new entropy, not recite old data.

### üõ°Ô∏è Mechanism Design (The "Heart")

- **Scoring**: $S = \text{Accuracy} \times \text{Calibration} - \text{Latency}$.
- **Anti-Gaming**: Commit-Reveal scheme prevents front-running.
- **Business Model**: AI Labs pay TAO to stress-test their models against our "Red Team" validators.

---

## 2. Logistics & Requirements

### üìÖ Timeline

| Phase                   | Start Date   | End Date     | Description                                       |
| :---------------------- | :----------- | :----------- | :------------------------------------------------ |
| **Registration**        | Dec 23, 2025 | Feb 25, 2026 | Sign up phase.                                    |
| **Round I: Ideathon**   | Dec 23, 2025 | Feb 28, 2026 | **Design Phase**. Deliver a PDF Proposal + Video. |
| **Round II: Hackathon** | Mar 2, 2026  | Mar 30, 2026 | **Execution Phase**. Implement on Testnet.        |

### üìù Submission Breakdown (Round I)

1.  **OpenArena Whitepaper** (PDF):
    - **Thesis**: "The End of Static Benchmarks."
    - **Math**: The Generalization Score function.
    - **Architecture**: Miner/Validator Commit-Reveal flow.
2.  **Video**: Show the "Live Leaderboard" (powered by KaggleIngest).
3.  **Deck**: 10 slides on the $10B Evaluation Market.

---

## 3. Execution Roadmap (Round II)

### The "Golden Path" MVP

If selected for Round II, build **only** this loop:

1.  **Validator**: Generates a random math/logic puzzle -> Encrypts Ground Truth.
2.  **Miner**: Solves -> Commits Hash -> Reveals Solution.
3.  **UI**: KaggleIngest dashboard showing live "Generalization Scores."

> [!TIP]
> **Adversarial Hardening**: The key to winning is proving you can't be cheated. We will emphasize the **Cryptographic Commit-Reveal** protection in all documentation.



================================================
FILE: INCENTIVE_MECHANISM.md
================================================
# OpenArena: Incentive Mechanism Design

## 1. Core Philosophy: Proof of Generalization

Unlike traditional subnets that reward _weight availability_ or _loss on a fixed dataset_, OpenArena rewards **Generalization**.
We define Generalization ($G$) as the ability of a miner $i$ to minimize loss $\mathcal{L}$ on a distribution $D_t$ that is disjoint from all prior distributions $\{D_0, ..., D_{t-1}\}$.

$$ G*i(t) = \mathbb{E}*{x \sim D_t} [ S(M_i(x), y^*) ] $$

## 2. The Reward Function ($R$)

The reward for miner $i$ at epoch $t$ is calculated as an aggregate of their performance across $K$ tasks.

$$ R*i = \sigma \left( \sum*{k=1}^{K} w*k \cdot \left( \alpha \cdot \underbrace{\mathcal{A}(y*{ik}, y^\*_k)}_{\text{Accuracy}} + \beta \cdot \underbrace{\mathcal{C}(c*{ik}, y*{ik})}_{\text{Calibration}} - \gamma \cdot \underbrace{\mathcal{L}(l_{ik})}\_{\text{Latency}} \right) \right) $$

### 2.1 Component Definitions

#### Accuracy ($\mathcal{A}$)

For Generative Tasks (e.g., Summarization), we use a semantic similarity metric (BERTScore) or Levenshtein Distance ($Lev$).
$$ \mathcal{A}_{text} = 1 - \frac{Lev(y_{ik}, y^_*k)}{\max(|y*{ik}|, |y^_\_k|)} $$

For Logic/Math Tasks, we use a binary score:
$$ \mathcal{A}_{logic} = \mathbb{I}(y_{ik} == y^\*\_k) $$

#### Calibration ($\mathcal{C}$)

We incentivize miners to know their own uncertainty using the **Brier Score**.
Miners submit a confidence $c_{ik} \in [0, 1]$.
$$ \mathcal{C} = 1 - (c*{ik} - \mathcal{A}*{logic})^2 $$
_Rationale_: A miner that is 100% confident but wrong is penalized heavily. A miner that is 50% confident and wrong is penalized less.

#### Latency ($\mathcal{L}$)

Speed is critical for real-world utility. We apply an exponential decay penalty based on the time delta $\Delta t$ relative to the fastest correct submission $t_{min}$.
$$ \mathcal{L} = e^{\lambda (t*{ik} - t*{min})} - 1 $$

## 3. Consensus Mechanism (Yuma)

The final weight $W_i$ set on the Bittensor blockchain is a consensus of the normalized rewards from all validators $v \in V$.

$$ W*i = \frac{\sum*{v \in V} S*v \cdot R*{vi}}{\sum*{j \in M} \sum*{v \in V} S*v \cdot R*{vj}} $$

Where $S_v$ is the stake of validator $v$ (V-Trust).
Miners with the highest $W_i$ receive the largest emission of $TAO.

## 4. Sustainability: The Efficiency Multiplier ($\mathcal{E}$)

To ensure long-term sustainability and prevent the subnet from becoming just "who has the most H100s", we introduce an **Efficiency Multiplier**.
This favors miners who achieve high accuracy with lower latency (proxy for model efficiency) and consistent uptime.

$$ R\_{final} = R_i \times \mathcal{E}\_i $$

Where $\mathcal{E}$ boosts miners who consistently solve "Flash Challenges" (sub-200ms tasks) which are impossible for API wrappers to route in time.

## 5. Anti-Gaming & Adversarial Hardening

### 5.1 The Commit-Reveal Scheme (Anti-Front-Running)

To prevent "Copycat Mining" (listening to the mempool), we strictly enforce a two-phase process:

1. **Commit Phase**: Miner $i$ submits $H_i = \text{SHA256}(y_{ik} || \text{salt} || \text{hotkey}_i)$.
2. **Reveal Phase**: Miner $i$ submits $y_{ik}, \text{salt}$.
3. **Verification**: Validator checks $H_i' == H_i$. If mismatch, $R_i = 0$.

### 5.2 Flash Challenges (Anti-Wrapper)

Validators randomly inject "Flash Tasks" with a strict $T_{max} = 200ms$.

- **Goal**: Filter out miners who are just wrapping GPT-4/Claude via API (network latency > 200ms).
- **Penalty**: Failure to respond in time $\to$ Score penalty $\gamma$ increases.

### 5.3 High-Entropy Generation (Anti-Lookup)

Tasks are generated procedurally with random seeds, ensuring $P(Task_t \in \text{TrainingSet}) \approx 0$.

- _Math_: Random coefficients.
- _Logic_: Randomly generated rulesets.



================================================
FILE: leaderboard_data.json
================================================
[
  {
    "epoch": 0,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.6000000000000001,
    "latency": 0.1779961225778374
  },
  {
    "epoch": 0,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.6000000000000001,
    "latency": 0.2260547286770072
  },
  {
    "epoch": 0,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.4,
    "latency": 0.42816569126833437
  },
  {
    "epoch": 0,
    "miner": "Miner_Lazy (Random)",
    "score": 0.4,
    "latency": 0.3451018315649287
  },
  {
    "epoch": 1,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.6800000000000002,
    "latency": 0.13495944646620664
  },
  {
    "epoch": 1,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.6800000000000002,
    "latency": 0.3250927723571596
  },
  {
    "epoch": 1,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.32000000000000006,
    "latency": 0.2859713861665302
  },
  {
    "epoch": 1,
    "miner": "Miner_Lazy (Random)",
    "score": 0.32000000000000006,
    "latency": 0.17195844026448573
  },
  {
    "epoch": 2,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.7440000000000002,
    "latency": 0.4325155942340365
  },
  {
    "epoch": 2,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.7440000000000002,
    "latency": 0.48716244127292485
  },
  {
    "epoch": 2,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.25600000000000006,
    "latency": 0.16149136799756392
  },
  {
    "epoch": 2,
    "miner": "Miner_Lazy (Random)",
    "score": 0.25600000000000006,
    "latency": 0.11665523406938326
  },
  {
    "epoch": 3,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.7952000000000001,
    "latency": 0.382426105641567
  },
  {
    "epoch": 3,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.7952000000000001,
    "latency": 0.4896377972814958
  },
  {
    "epoch": 3,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.20480000000000007,
    "latency": 0.3756796340973141
  },
  {
    "epoch": 3,
    "miner": "Miner_Lazy (Random)",
    "score": 0.20480000000000007,
    "latency": 0.38800551035441666
  },
  {
    "epoch": 4,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8361600000000002,
    "latency": 0.2412278643189834
  },
  {
    "epoch": 4,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8361600000000002,
    "latency": 0.4437119138788751
  },
  {
    "epoch": 4,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.16384000000000007,
    "latency": 0.38899145174946836
  },
  {
    "epoch": 4,
    "miner": "Miner_Lazy (Random)",
    "score": 0.16384000000000007,
    "latency": 0.12554785344385355
  },
  {
    "epoch": 5,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8689280000000001,
    "latency": 0.19372422866653294
  },
  {
    "epoch": 5,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8689280000000001,
    "latency": 0.3085929276654661
  },
  {
    "epoch": 5,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.13107200000000005,
    "latency": 0.4115953267986575
  },
  {
    "epoch": 5,
    "miner": "Miner_Lazy (Random)",
    "score": 0.13107200000000005,
    "latency": 0.3290462563471851
  },
  {
    "epoch": 6,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8951424000000001,
    "latency": 0.2683404735835466
  },
  {
    "epoch": 6,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8951424000000001,
    "latency": 0.11966590829196573
  },
  {
    "epoch": 6,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.10485760000000005,
    "latency": 0.4144742142997404
  },
  {
    "epoch": 6,
    "miner": "Miner_Lazy (Random)",
    "score": 0.10485760000000005,
    "latency": 0.10067328064373814
  },
  {
    "epoch": 7,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9161139200000001,
    "latency": 0.1721770009561337
  },
  {
    "epoch": 7,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9161139200000001,
    "latency": 0.4787433436349503
  },
  {
    "epoch": 7,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.08388608000000004,
    "latency": 0.47276186860969693
  },
  {
    "epoch": 7,
    "miner": "Miner_Lazy (Random)",
    "score": 0.08388608000000004,
    "latency": 0.1264771865777375
  },
  {
    "epoch": 8,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9328911360000001,
    "latency": 0.3121905644720486
  },
  {
    "epoch": 8,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9328911360000001,
    "latency": 0.44482428780089145
  },
  {
    "epoch": 8,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.06710886400000003,
    "latency": 0.3185804104042277
  },
  {
    "epoch": 8,
    "miner": "Miner_Lazy (Random)",
    "score": 0.06710886400000003,
    "latency": 0.15638594507542708
  },
  {
    "epoch": 9,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9463129088000002,
    "latency": 0.2177532814190792
  },
  {
    "epoch": 9,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9463129088000002,
    "latency": 0.11530520382456128
  },
  {
    "epoch": 9,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.05368709120000003,
    "latency": 0.24314636902218467
  },
  {
    "epoch": 9,
    "miner": "Miner_Lazy (Random)",
    "score": 0.05368709120000003,
    "latency": 0.46758288691755945
  },
  {
    "epoch": 10,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9570503270400001,
    "latency": 0.20948750243262812
  },
  {
    "epoch": 10,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9570503270400001,
    "latency": 0.4454572161500582
  },
  {
    "epoch": 10,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.042949672960000025,
    "latency": 0.25394487540931704
  },
  {
    "epoch": 10,
    "miner": "Miner_Lazy (Random)",
    "score": 0.042949672960000025,
    "latency": 0.1790624583828751
  },
  {
    "epoch": 11,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9656402616320001,
    "latency": 0.18377145742195952
  },
  {
    "epoch": 11,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9656402616320001,
    "latency": 0.1959496593006459
  },
  {
    "epoch": 11,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.03435973836800002,
    "latency": 0.3710789184823503
  },
  {
    "epoch": 11,
    "miner": "Miner_Lazy (Random)",
    "score": 0.03435973836800002,
    "latency": 0.45599955391476843
  },
  {
    "epoch": 12,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9725122093056002,
    "latency": 0.35154369664265284
  },
  {
    "epoch": 12,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9725122093056002,
    "latency": 0.3065066657940856
  },
  {
    "epoch": 12,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.027487790694400018,
    "latency": 0.3003858490497625
  },
  {
    "epoch": 12,
    "miner": "Miner_Lazy (Random)",
    "score": 0.027487790694400018,
    "latency": 0.3788329503721992
  },
  {
    "epoch": 13,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9780097674444803,
    "latency": 0.2206317988769877
  },
  {
    "epoch": 13,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9780097674444803,
    "latency": 0.22989815868243246
  },
  {
    "epoch": 13,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.021990232555520017,
    "latency": 0.12718786633265966
  },
  {
    "epoch": 13,
    "miner": "Miner_Lazy (Random)",
    "score": 0.021990232555520017,
    "latency": 0.4671292434024441
  },
  {
    "epoch": 14,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9824078139555843,
    "latency": 0.2403259208236556
  },
  {
    "epoch": 14,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9824078139555843,
    "latency": 0.19853007674303635
  },
  {
    "epoch": 14,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.017592186044416015,
    "latency": 0.4391603429901778
  },
  {
    "epoch": 14,
    "miner": "Miner_Lazy (Random)",
    "score": 0.017592186044416015,
    "latency": 0.1323444746860719
  }
]


================================================
FILE: openarena-pitch-deck.md
================================================
# OpenArena: Pitch Deck Outline

**Target Audience:** Judges, Investors, AI Researchers
**Goal:** Prove that OpenArena allows Bittensor to capture the $10B+ AI Evaluation Market.

---

## Slide 1: Title Slide

**Visual:** A gladiatorial arena where the "gladiators" are Neural Networks. One is adapting to a shifting landscape; the other is stuck, memorizing a map that no longer exists.
**Headline:** OpenArena: The Decentralized Adversarial Evaluation Protocol
**Sub-headline:** Proof of Intelligence is Generalization, not Memorization.
**Presenter Notes:** "Static benchmarks are dead. We are building the world's first protocol that measures distinct, adaptive intelligence."

---

## Slide 2: The Problem: The "Benchmark Saturation" Crisis

**Visual:** A graph showing MMLU scores saturating at 90%+, while "Real World Utility" remains flat.
**Key Points:**

- **Goodhart's Law:** "When a measure becomes a target, it ceases to be a good measure."
- **Contamination:** Reasoning datasets (GSM8K) are leaked into training data.
- **The Result:** Models are memorizing, not thinking. We have no way to distinguish a cheat from a genius.

---

## Slide 3: The Solution: Continuous Adversarial Evaluation

**Visual:** A loop Diagram.

1. **Validator** generates a _fresh_, never-before-seen task (e.g., specific news summary from today).
2. **Miner** solves it instantly.
3. **Score** is awarded based on accuracy + novelty.
   **Key Points:**

- **Dynamic Data:** Tasks change _every epoch_. You cannot memorize what hasn't happened yet.
- **VRF-Backed Entropy:** Tasks ($T_t$) are cryptographically derived from block hashes ($H_b$).
  $$ T_t = f(\text{SHA256}(H_b \parallel K_v)) $$
- **Adversarial:** Validators actively try to stump miners.
- **Living Benchmark:** A score that evolves in real-time.

---

## Slide 4: Mechanism: Validators as "Game Masters"

**Visual:** Iconography of a "Dungeon Master" rolling dice. The dice represent random task parameters (Logic, Code, Text).
**Key Points:**

- **Task Oracle:** Validators pull real-time data or generate synthetic logic puzzles.
- **No Fixed Dataset:** Unlike other subnets, there is no "training set." The world is the test set.
- **Yuma Consensus:** Validators must agree on the difficulty and correctness, preventing subjective bias.

---

## Slide 5: The Math: Proof of Generalization

**Visual:** The Equation (simplified).
$$ S*i = \underbrace{\alpha \cdot \text{Acc}(y*{ij}, y^\*_j)}_{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t*{ij})}*{\text{Latency}} $$
**Key Points:**

- **Novelty Bonus:** Higher rewards for solving tasks with high "entropy" (unlikelihood).
- **Latency Penalty:** Real-world AI must be fast.
- **Calibration:** Miners bet on their own confidence. High confidence + Wrong Answer = Massive Penalty.

---

## Slide 6: Adversarial Hardening: Stopping the Cheaters

**Visual:** A shield deflecting arrows.
**Key Points:**

- **Commit-Reveal:** Prevents "front-running" (miners copying answers from the mempool).
- **Entropy Penalty ($E_v$):** Lazy Validators dealing easy tasks lose consensus power.
  $$ E*v = D*{KL}(P*t \parallel P*{t-1}) < \epsilon \implies \text{Slash}(W_v) $$
- **Flash Challenges:** High-speed bursts to detect if a miner is just an API wrapper for GPT-4 (latency analysis).

---

## Slide 7: Business Model: Evaluation-as-a-Service

**Visual:** An API Dashboard showing "OpenArena Certified."
**Key Points:**

- **The Customer:** AI Labs (OpenAI, Anthropic, Meta) need independent verification.
- **The Product:** "Private Evaluation Rounds." Pay TAO to run your model against our Gauntlet.
- **The Flywheel:** Formal Fee Split ensures value accrual:
  $$ F*{dist} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{val} + 0.2 \cdot F*{miner} $$

---

## Slide 8: Go-To-Market: The Kaggle Bridge

**Visual:** A mockup of **KaggleIngest** (our existing platform) displaying OpenArena scores next to traditional Kaggle leaderboards.
**Key Points:**

- **Target Audience:** The 15M+ Data Scientists on Kaggle.
- **Strategy:** "The Contest That Never Ends."
- **Integration:** One-click submission from Kaggle Notebooks to OpenArena Miners.

---

## Slide 9: Roadmap

**Visual:** Timeline.

- **Round I (Now):** Strategy & Design (Completed).
- **Round II (Code):** "Stub" Subnet. Basic Validator/Miner loop with Synthetic Tasks.
- **Round III (Live):** Testnet Launch. First game: "Arithmetic Generalization."
- **Q4 2026:** Mainnet Integration & Kaggle Dashboard Live.

---

## Slide 10: The Team & Vision

**Visual:** Photos of the team (or avatars).
**Key Points:**

- **Background:** Expert Kaggle Grandmasters & Blockchain Engineers.
- **Why Us:** We understand both the data science (Kaggle) and the mechanism design (Bittensor).
- **Closing Ask:** "Help us kill static benchmarks. Support OpenArena."



================================================
FILE: openarena-proposal.md
================================================
# OpenArena: Subnet Design Proposal

**The Decentralized Adversarial Evaluation Protocol**

---

## 1. Executive Summary

**OpenArena** is a decentralized subnet designed to solve the "Benchmark Saturation" crisis in AI. By turning model evaluation into a continuous, adversarial game, OpenArena incentivizes the creation of models that **generalize** rather than **memorize**.

Current benchmarks (GSM8K, MMLU) are static and leaked into training data. OpenArena Validators generate _fresh_, unseen tasks every epoch, forcing Miners to demonstrate real-time adaptability. This transforms model evaluation from a one-time static test into a verifiable, continuous digital commodity.

---

## 2. Incentive & Mechanism Design

### 2.1 The Philosophy: Proof of Generalization

The goal is to reward intelligence that can adapt to new distributions.

- **Dynamic Tasks**: Validators use a "Task Generator Oracle" to create creating novel prompts (e.g., "Summarize this news article from 5 minutes ago," "Solve this randomly generated math puzzle").
- **Adversarial Scoring**: Miners are penalized heavily for overfitting or latency.

### 2.2 The Reward Function

A miner $i$'s reward $R_i$ is calculated per epoch based on their performance across $N$ tasks:

$$ R*i = \sigma \left( \sum*{j=1}^{N} \bigg[ \alpha \cdot \text{Acc}(y_{ij}, y^*_j) \cdot \underbrace{(1 - \text{Sim}(y_{ij}, \text{TrainingData}))}_{\text{Novelty Bonus}} \bigg] - \gamma \cdot \text{Latency}\_i \right) $$

Where:

- $\sigma$: Sigmoid function for normalization.
- $\text{Acc}(y_{ij}, y^*_j)$: Accuracy metric (0/1 for exact match, or Levenshtein/BERTScore for text).
- $\text{Sim}$: Similarity check to penalize regurgitation of known training data.
- $\text{Latency}_i$: Time taken to respond.

### 2.3 Mechanisms to Discourage Quality Degradation

- **Commit-Reveal Scheme**: To prevent "Front-Running" (miners copying answers from the mempool), miners first submit a hash `H(Answer + Salt)`, then reveal the `Answer` in a subsequent block.
- **Flash Challenges**: Random high-frequency bursts of tasks to test system latency and throughput.
- **Validator Consensus (Yuma)**: Weights are set based on the consensus of multiple independent validators, filtering out malicious or lazy evaluators.

---\n\n## 3. Miner Design\n\n### 3.1 Miner Tasks\nMiners act as \"General Purpose Solvers.\" They listen for `Synapse` objects containing diverse tasks:\n1. **Logic/Math**: \"Solve for x: $3x^2 + 2x - 5 = 0$ (integers only).\"\n2. **Text Summarization**: \"Summarize the key geopolitical events in this real-time text stream.\"\n3. **Code Generation**: \"Write a Python function to parse this specific messy JSON format.\"\n\n### 3.2 Performance Dimensions\nMiners are evaluated on:\n- **Accuracy**: Correctness of the answer against the Validator's hidden ground truth.\n- **Latency**: Speed of inference (critical for real-world usability).\n- **Calibration**: Confidence scores included in metadata (Miners effectively \"bet\" on their own correctness).\n\n### 3.3 Expected Input/Output\n- **Input**: JSON Synapse containing `task_type`, `query_content`, `timestamp`.\n- **Output**: JSON Response containing `answer`, `confidence_score`, `proof_of_work_nonce` (if applicable).\n\n---\n\n## 4. Validator Design\n\n### 4.1 Evaluation Methodology\nValidators are the \"Game Masters.\"\n1. **Task Generation**: Use a combination of Synthetic Generators (Algorithmic Logic) and Oracle Data (NewsAPI, Twitter API) to create non-googleable queries.\n2. **Ground Truth Management**: Store the correct answer (or evaluation rubric) locally and securely.\n3. **Scoring**: Compare Miner reveals against Ground Truth.\n4. **Weight Setting**: Distribute scores to the Bittensor chain via `set_weights`.\n\n### 4.2 Incentive Alignment\nValidators are incentivized to hold accurate consensus. If a validator's scoring diverges significantly from the median (Yuma Consensus), they lose V-Trust and dividends. This prevents validators from favoring specific miners arbitrarily.\n\n---\n\n## 5. Business Logic & Market Rationale\n\n### 5.1 The Problem\n- **$10B+ Evaluation Market**: AI Labs (OpenAI, Anthropic) spend billions on \"Red Teaming\" and human evaluation.\n- **Trust Crisis**: No one believes static benchmark scores anymore.\n- **Data Contamination**: We need a way to test models on data they _definitely_ haven't seen.\n\n### 5.2 Competitors\n- **Traditional**: Scale AI, Labelbox (Centralized, expensive, slow human loops).\n- **Bittensor**: Other subnets focus on _training_ or _inference_, not _adversarial evaluation_. OpenArena is the \"referee\" for the entire ecosystem.\n\n### 5.3 Long-Term Sustainability\n- **External Revenue**: AI Labs pay in TAO to run \"Private Evaluation Rounds\" on the OpenArena subnet to test their internal checkpoints before release.\n- **Data Monetization**: The \"Hardest Tasks\" that stump miners become a high-value dataset for future training (\"Hard Negative Mining\").\n\n---\n\n## 6. Go-To-Market Strategy\n\n### 6.1 Initial Users\n- **Kaggle Community**: Leverage the \"Continuous Kaggle\" narrative to attract top data scientists as miners.\n- **Open Source Model Builders**: HuggingFace developers who need a quick, independent score for their `Llama-3-FineTine`.\n\n### 6.2 Distribution\n- **Dashboard Integration**: We will integrate OpenArena scores directly into a **KaggleIngest** dashboard, visualizing \"Generalization Score over Time.\"\n- **Twitter Bot**: A bot that auto-replies to \"SOTA claims\" with \"OpenArena Score: 42% (Failed Generalization).\"\n\n### 6.3 Bootstrapping\n- **Miner Bounty**: Bonus rewards for the Top 10 miners in Week 1.\n- **Validator Onboarding**: Partner with established validator pools (TaoStats, Opentensor) by providing a robust, one-click Docker container for validation.\n



================================================
FILE: openarena-social-post.md
================================================
# OpenArena: Social Introduction (X/Twitter Thread)

**Thread üßµ**

1/ üö® Benchmarks are dead.
GSM8K is memorized. MMLU is leaked.
We are flying blind. We can no longer distinguish between a model that _thinks_ and a model that _recites_.

It‚Äôs time for a new standard.
Introducing **OpenArena**: The Decentralized Adversarial Evaluation Protocol on @bittensor\_ . üëá

2/ The concept is simple:
Static tests = Memorization.
Dynamic games = Intelligence.

**OpenArena** turns evaluation into a continuous, adversarial game. Validators are "Game Masters," generating fresh, never-before-seen tasks every epoch.
Miners must adaptive. They must **generalize**.

3/ üß† **Proof of Generalization**
Most subnets reward you for being "close" to a reference.
We reward you for solving the _unknown_.
Our scoring function balances Accuracy, Latency, and a "Novelty Bonus."
If you can solve a problem type that didn't exist 5 minutes ago, you win.

4/ üõ°Ô∏è **Adversarial Hardening**
Cheating is the enemy of truth.
‚Ä¢ Commit-Reveal Scheme (No front-running)
‚Ä¢ High-Entropy Task Generation (No memorization)
‚Ä¢ Flash Latency Challenges (No API wrappers)

We are building the hardest arena in AI.

5/ üíº **The Business Model**
"Evaluation as a Service."
AI Labs spend billions on red-teaming. OpenArena offers a decentralized, unbiased, 24/7 audit mechanism.
We provide the "Truth Signal" the market is desperate for.

6/ ü§ù **Kaggle x Bittensor**
Built by a team of Kaggle Grandmasters. We are integrating OpenArena directly into our **KaggleIngest** dashboard.
The world's best data scientists. The world's best incentive mechanism.
One Arena.

7/ We are submitting for the @bittensor\_ Ideathon.
Read the whitepaper here: [Link]
Watch the vision: [Link]

The static era is over. Let the games begin. ‚öîÔ∏è
#Bittensor #OpenArena #AI #DeAI $TAO



================================================
FILE: openarena-video-script.md
================================================
# OpenArena: Video Script (5-8 Minutes)

**Tone:** Urgent, Technical, Visionary.
**Visual Style:** Dark mode, neo-brutalist typography, fast-paced editing.

---

## 0:00 - 1:00: The Benchmark Crisis (The Hook)

**(Visual: A montage of headlines: "GPT-4 Passes Bar Exam," "Gemini Smashes MMLU." Then, a glitch effect. The headlines burn away to reveal text: "MEMORIZATION != INTELLIGENCE.")**

**Narrator:**
"Every week, a new model claims State of the Art. 90% on MMLU. 99% on GSM8K. But when you use them... they break."

**(Visual: Screen recording of a model failing a simple, slightly altered logic puzzle.)**

**Narrator:**
"Why? Because benchmarks are dead. They are static. They are leaked. Today's AI isn't learning to think; it's learning to memorize the test. We face a crisis of measurement. If we can't measure intelligence, we can't build it."

**(Visual: The Bittensor Logo pulsating. Text: "ENTER OPENARENA.")**

---

## 1:00 - 2:30: Introducing OpenArena (The Solution)

**(Visual: A futuristic dashboard. A stream of data tokens flowing into a central brain. The data changes color constantly.)**

**Narrator:**
"Introducing OpenArena: The world's first Decentralized Adversarial Evaluation Protocol. Built on Bittensor."

**(Visual: Split screen. Left side: "Old Way" (Static PDF test). Right side: "OpenArena" (Shifting, dynamic geometric shapes representing tasks).)**

**Narrator:**
"OpenArena is not a test. It is a game. A game where the rules change every single epoch. Validators act as Game Masters, generating fresh, never-before-seen challenges‚Äîfrom real-time news summarization to chaotic synthetic logic hurdles."

**(Visual: Text Overlay: "PROOF OF GENERALIZATION.")**

**Narrator:**
"To win, Miners cannot memorize. They must generalize. They must solve problems they have strictly never seen before. This generates a new metric: The Generalization Score."

---

## 2:30 - 4:30: Discussion of Mechanism (The Tech)

**(Visual: Animated Diagram of the Validator-Miner Loop.)**

**Narrator:**
"Here is how it works.
Step 1: The Validator generates a Synapse. Let's say, a complex math problem using a brand new syntax defined _in the prompt itself_."

**(Visual: Code snippet of a Python script validating a calculation.)**

**Narrator:**
"Step 2: Miners race. They parse the query, reason through the new syntax, and compute the answer."

**(Visual: A shield icon appearing over the Miner.)**

**Narrator:**
"But wait. How do we stop cheating? Front-running?
We implement a **Commit-Reveal Scheme**. Miners submit a hash of their answer first. Only after the window closes do they reveal the truth. If you try to copy the smart kid in class, you fail."

**(Visual: The Formula. $S = A \times NB - L$.)**

**Narrator:**
"We weight this by simple, brutal math. Accuracy. Novelty Bonus. Latency. If you represent the status quo, you earn dust. If you demonstrate novel reasoning, you earn Alpha."

---

## 4:30 - 5:30: The Business Case (The Why)

**(Visual: Logos of OpenAI, Anthropic, Meta.)**

**Narrator:**
"Why does this matter? Because the industry is flying blind. AI Labs are spending billions on human evaluation that is slow and biased. OpenArena offers 'Evaluation as a Service.' A decentralized red-team that never sleeps."

**(Visual: A spinning coin with the OpenArena logo.)**

**Narrator:**
"We turn truthful evaluation into a digital commodity. We don't just measure the best AI. We incentivize its creation."

---

## 5:30 - End: Call to Action

**(Visual: The Team slides. Kaggle Grandmaster badges. Github commit graphs.)**

**Narrator:**
"We are the team bridging Kaggle and Bittensor. We know data. We know competition.
Static benchmarks are history. The Arena is open."

**(Visual: Fade to Black. URL: `github.com/opentensor/openarena`.)**



================================================
FILE: WHITE_PAPER.md
================================================
# OpenArena: The Decentralized Adversarial Evaluation Protocol

**"The Proof of Intelligence"**

> [!IMPORTANT]
> **Core Thesis**: Static benchmarks are dead. Intelligence is not the ability to memorize a fixed dataset; it is the ability to generalize to new, unseen distributions. OpenArena is a continuous, adversarial stress-test for AI models, turning evaluation into a verifiable digital commodity.

---

## 1. Introduction: The Crisis of Evaluation

Modern AI has a **Goodhart's Law** problem: "When a measure becomes a target, it ceases to be a good measure."

- **Contamination**: Public datasets (GSM8K, MMLU) leak into training data.
- **Saturation**: Top models score 90%+ on benchmarks but fail in production.
- **Trust**: Who validates the validator?

**OpenArena** solves this by creating a **Dynamic Adversarial Evaluation Game**.

- **Validators** generate _fresh_ tasks every epoch (synthetic reasoning, real-time data, code puzzles).
- **Miners** must solve these unseen tasks instantly.
- **Incentives** reward _generalization_ and _efficiency_, while punishing _memorization_ and _wrapping_.

### 1.1 Core Thesis: Proof of Intelligence

We define "Intelligence" not as knowledge retrieval, but as **Generalization Efficiency**:

> _The ability to solve novel, high-entropy tasks with minimum latency and compute._

This shift allows us to distinguish between a 100B parameter model that memorized the internet and a 7B parameter model that can actually _reason_.

---

## 2. Technical Architecture

### 2.1 The Flow of Intelligence

```mermaid
sequenceDiagram
    participant V as Validator (Game Master)
    participant M as Miner (Solver)
    participant C as Chain (Bittensor)

    Note over V, M: Epoch N Starts (Block 0-360)

    rect rgb(20, 20, 20)
        Note right of V: 1. Generate Dynamic Task<br/>(e.g. "Solve random math puzzle")
        V->>V: Hash(Task) + Encrypt(GroundTruth)
        V->>M: Broadcast Synapse (Task Only)
    end

    rect rgb(40, 40, 40)
        Note right of M: 2. Compute Solution<br/>(LLM Inference / Code Exec)
        M->>M: Hash(Answer + Salt)
        M->>C: Commit Hash (Prevents Front-running)
    end

    rect rgb(20, 20, 20)
        Note right of V: 3. Reveal Phase
        M->>V: Reveal Answer
        V->>V: Verify Hash matches Commit
        V->>V: Score(Answer, GroundTruth)
    end

    rect rgb(60, 20, 20)
        Note right of C: 4. Yuma Consensus
        V->>C: Set Weights (W_i)
        C->>M: Distribute TAO Rewards
    end
```

### 2.2 Component Roles

| Role          | Responsibility                                                                     | Incentive                                                                        |
| :------------ | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Miner**     | Solve arbitrary tasks (Text, Code, Math) with high accuracy and low latency.       | Maximizes Reward ($R$) by optimizing inference speed and model generalization.   |
| **Validator** | Generate high-entropy, non-repeatable tasks. Evaluate miner solutions objectively. | Maximizes Dividends ($D$) by attracting high-quality miners and staking support. |

---

## 3. Incentive Mechanism (The Math)

The core innovation is the **Generalization Score ($S$)**.

### 3.1 The Scoring Function

For a set of $N$ tasks in an epoch, a miner $i$'s score $S_i$ is:

$$ S*i = \underbrace{\alpha \cdot \frac{1}{N} \sum*{j=1}^{N} \text{Acc}(y\*{ij}, y^\*_{j})}*{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij}, \text{Acc}*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t_{ij})}\*{\text{Latency Penalty}} $$

Where:

- $\text{Acc}(y_{ij}, y^*_{j})$: Accuracy metric (0 or 1 for exact match, or Levenshtein/BLEU for text).
- $\text{Cal}$: **Calibration Score**. Rewards miners who are confident when correct and uncertain when wrong (using Brier Score or Log Loss).
- $\text{Lat}$: **Latency Penalty**. $e^{t_{ij} - T_{max}}$.

### 3.2 Yuma Consensus & Weight Setting

Validators normalize scores to a weight vector $W$:
$$ w*{i} = \frac{e^{S_i / \tau}}{\sum*{k} e^{S_k / \tau}} $$
*(Using Softmax with temperature $\tau$ to control competition intensity)\*.

---

## 4. Adversarial Hardening (How We Win)

### üõ°Ô∏è Challenge 1: Memorization / Lookup

- **Attack**: Miners cache answers from previous epochs.
- **Defense**: **High-Entropy Generation**.
  - _Math_: "Calculate $A \times B$" where $A, B$ are random 10-digit primes.
  - _Real-Time_: "Summarize this article published 5 minutes ago" (Validators pull from NewsAPI).
  - _Code_: "Write a Python function to sort this random list [4, 1, 9...]".

### üõ°Ô∏è Challenge 2: Front-Running / Copying

- **Attack**: Fast miner sees a smart miner's answer in the mempool and copies it.
- **Defense**: **Commit-Reveal Scheme**.
  1.  Miner submits `Hash(Answer + Salt)`.
  2.  After window closes, Miner submits `Answer + Salt`.
  3.  Validator verifies hash matches.

### üõ°Ô∏è Challenge 3: Validator Collusion

- **Attack**: Validator shares Ground Truth with a specific miner.
- **Defense**: **Cross-Validation**.
  - Multiple validators score the same miner.
  - If Validator A's scores diverge significantly from the consensus media (Yuma Consensus), Validator A loses V-Trust and dividends.

---

## 5. Token Economics (The OpenArena Flywheel)

### 5.1 The Formal Value Loop ($V$)

Let $F$ be the fee paid by an enterprise (e.g., Anthropic) to prioritize a specific evaluation dataset $D_{target}$.

$$ F*{distribution} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{validators} + 0.2 \cdot F*{miners} $$

1.  **Burn ($40\%$)**: Permanently removed from supply, creating deflationary pressure on $\tau$.
2.  **Validator Reward ($40\%$)**: Distributed to validators proportional to their stake ($S_v$) and their **Curator Score** ($C_v$).
3.  **Miner Reward ($20\%$)**: Distributed to miners who solve $D_{target}$ with the highest **Generalization Score** ($G_m$).

### 5.2 The Enterprise Demand Flywheel

As enterprises pay $F$ to access the network:

1.  **Demand** for TAO increases (to pay fees).
2.  **Supply** of TAO decreases (via Burn).
3.  **Validator Yield** increases (via Dividend).
4.  **Miner Competition** increases (via Reward).

This creates a self-reinforcing loop where **Utility** drives **Security** and **Valuation**.

This ensures that **Enterprise Demand** directly correlates with **Miner Profitability** and **Token Scarcity**.

---

## 6. Security Analysis (Adversarial Robustness)

### 6.1 Attack: Pre-Computation (The "Lookup Table")

- **Vector**: Miner pre-calculates answers to known datasets to simulate intelligence.
- **Mitigation**: **Cryptographic Entropy Protocol**.
  - Let $H_b$ be the block hash at height $t$.
  - Let $K_v$ be the Validator's VRF key.
  - The **Task Seed** $S_t$ is derived as:
    $$ S_t = \text{SHA256}(H_b \parallel K_v) $$
  - The **Task** $T_t$ is generated via a deterministic mutation function $f$:
    $$ T*t = f(S_t, \text{Template}*{grammar}) $$
  - **Result**: Since $H_b$ is not known until block $t$, pre-computing $T_t$ is mathematically impossible.

### 6.2 Attack: Validator Laziness (Low Entropy)

- **Vector**: A Validator reuses old tasks to save compute, degrading the network's measurement quality.
- **Mitigation**: **Entropy Penalty ($E_v$)**.
  - We measure the Kullback-Leibler (KL) divergence between task distributions at time $t$ and $t-1$:
    $$ E*v = D*{KL}(P*t \parallel P*{t-1}) $$
  - If $E_v < \epsilon_{threshold}$ (statistically indistinguishable from previous epoch), the Validator's weight-setting power $W_v$ is slashed:
    $$ W*v^{new} = W_v^{old} \cdot (1 - \text{Penalty}*{lazy}) $$
- **Incentive**: **Difficulty Rating ($D_t$)**.
  - Validators are rewarded for generating tasks that separate miner performance.
  - If all miners score 100%, $D_t$ is low -> Validator reward reduced.
  - If no miner scores > 0%, $D_t$ is too high -> Validator reward reduced.
  - Optimal $D_t$ targets a Gaussian distribution of miner scores.

### 6.3 Attack: Front-Running (The "Copycat")

- **Mitigation**: **Commit-Reveal** (as defined in Section 4).
  - $t_0$: Miner submits $H = \text{SHA256}(Answer + Salt)$.
  - $t_1$: Reveal window opens.
  - Copycats only see hash $H$, preventing answer theft.

---

## 7. Go-To-Market & Integration (KaggleIngest)

We leverage **KaggleIngest** to visualize this war zone.

- **Leaderboard**: Real-time display of Miner Generalization Scores.
- **Museum**: Archive of "Hardest Tasks" (a valuable dataset).

---

## 6. Execution Roadmap (Round II Strategy)

### Phase 1: The "Stub" (Days 1-5)

- [ ] Implement `neurons/validator.py`: Basic task generation (Math/Logic).
- [ ] Implement `neurons/miner.py`: Basic OpenAI/Llama wrapper.
- [ ] Implement `commt-reveal` mechanism on-chain (using mock chain).

### Phase 2: The "Arena" (Days 6-12)

- [ ] Connect KaggleIngest frontend to Subnet stats.
- [ ] Deploy 5 Miner nodes (simulated) to show competition.
- [ ] Create visualization of "Score Drift" over time.



================================================
FILE: winning-strategy.md
================================================
# How to Win the Bittensor Subnet Ideathon: OpenArena Strategy

> [!IMPORTANT]
> **Pivot Alert**: We have shifted from "Green AI" to **OpenArena: The Decentralized Adversarial Evaluation Protocol**. This leverages your Kaggle background to solve AI's "Benchmark Saturation" crisis.

## üöÄ The Winning Concept: OpenArena

**Tagline**: _The World‚Äôs First Decentralized, Adversarial AI Evaluation Protocol._

### 1. The Problem Space

- **Benchmark Saturation**: GPT-4o effectively "memorizes" static datasets like GSM8K. We can no longer distinguish _intelligence_ from _retrieval_.
- **Data Contamination**: Public evaluation data leaks into training sets.
- **Trust**: Who validates the validator?

### 2. The Solution: Dynamic Adversarial Evaluation

OpenArena is a subnet where:

1.  **Validators** act as "Game Masters," generating _fresh_ tasks every epoch (math puzzles, real-time news summarization, code challenges).
2.  **Miners** act as "Solvers," competing to generalize to these unseen tasks instantly.
3.  **Incentives** reward **Generalization** (adaptive intelligence) and penalize **Memorization** (overfitting).

### 3. Mechanism Design (The "Heart")

Our core innovation is the **Generalization Score ($S$)**:

$$ S*i = \underbrace{\alpha \cdot \text{Accuracy}}*{\text{Correctness}} \times \underbrace{\beta \cdot \text{Calibration}}_{\text{Confidence}} - \underbrace{\gamma \cdot \text{Latency}}_{\text{Speed}} $$

- **Anti-Gaming**: We use a **Commit-Reveal** scheme to prevent miners from copying answers from the mempool.
- **Validator Consensus**: Yuma Consensus ensures no single validator controls the ground truth.

---

## üèóÔ∏è Technical Architecture

### High-Level Flow

```mermaid
graph TD
    Val[Validator: Task Generator] -->|Broadcast Synapse| Miner[Miner Pool]
    Miner -->|Commit Hash| Chain[Bittensor Chain]
    Miner -->|Reveal Solution| Val
    Val -->|Score & Set Weights| Chain
    Val -.->|Cross-Check| OtherVals[Other Validators]
```

---

## üìÑ Submission Components (Round I)

### 1. The Whitepaper (PDF)

- **Thesis**: "Proof of Intelligence is the ability to generalize, not memorize."
- **Math**: Detailed incentive function covering accuracy, calibration, and latency.
- **Adversarial Analysis**: How we prevent Sybil attacks and collusion.

### 2. The Pitch Deck (Business Case)

- **Market**: "The $10B Data Labeling & Evaluation Market."
- **Narrative**: OpenArena creates the "Gold Standard" for AI capability tracking.
- **Integration**: Leverage **KaggleIngest** as the frontend for real-time visualization.

### 3. The Video (5 Minutes)

- **Show**: Validator generating a math puzzle -> Miner solving it -> Score updating on-chain.
- **Tell**: "We are measuring _real_ intelligence, for the first time."

---

## üõ†Ô∏è Execution Strategy (Round II)

- **Phase 1**: Build a "Stub" subnet with simulated miners.
- **Phase 2**: Implement the Commit-Reveal mechanism on a local testnet.
- **Phase 3**: Connect KaggleIngest to show live "Generalization Leaderboards."



================================================
FILE: winning-strategy.md.resolved
================================================
# How to Win the Bittensor Subnet Ideathon: OpenArena Strategy

> [!IMPORTANT]
> **Pivot Alert**: We have shifted from "Green AI" to **OpenArena: The Decentralized Adversarial Evaluation Protocol**. This leverages your Kaggle background to solve AI's "Benchmark Saturation" crisis.

## üöÄ The Winning Concept: OpenArena

**Tagline**: *The World‚Äôs First Decentralized, Adversarial AI Evaluation Protocol.*

### 1. The Problem Space
- **Benchmark Saturation**: GPT-4o effectively "memorizes" static datasets like GSM8K. We can no longer distinguish *intelligence* from *retrieval*.
- **Data Contamination**: Public evaluation data leaks into training sets.
- **Trust**: Who validates the validator?

### 2. The Solution: Dynamic Adversarial Evaluation
OpenArena is a subnet where:
1.  **Validators** act as "Game Masters," generating *fresh* tasks every epoch (math puzzles, real-time news summarization, code challenges).
2.  **Miners** act as "Solvers," competing to generalize to these unseen tasks instantly.
3.  **Incentives** reward **Generalization** (adaptive intelligence) and penalize **Memorization** (overfitting).

### 3. Mechanism Design (The "Heart")
Our core innovation is the **Generalization Score ($S$)**:

$$ S_i = \underbrace{\alpha \cdot \text{Accuracy}}_{\text{Correctness}} \times \underbrace{\beta \cdot \text{Calibration}}_{\text{Confidence}} - \underbrace{\gamma \cdot \text{Latency}}_{\text{Speed}} $$

- **Anti-Gaming**: We use a **Commit-Reveal** scheme to prevent miners from copying answers from the mempool.
- **Validator Consensus**: Yuma Consensus ensures no single validator controls the ground truth.

---

## üèóÔ∏è Technical Architecture

### High-Level Flow
```mermaid
graph TD
    Val[Validator: Task Generator] -->|Broadcast Synapse| Miner[Miner Pool]
    Miner -->|Commit Hash| Chain[Bittensor Chain]
    Miner -->|Reveal Solution| Val
    Val -->|Score & Set Weights| Chain
    Val -.->|Cross-Check| OtherVals[Other Validators]
```

---

## üìÑ Submission Components (Round I)

### 1. The Whitepaper (PDF)
- **Thesis**: "Proof of Intelligence is the ability to generalize, not memorize."
- **Math**: Detailed incentive function covering accuracy, calibration, and latency.
- **Adversarial Analysis**: How we prevent Sybil attacks and collusion.

### 2. The Pitch Deck (Business Case)
- **Market**: "The $10B Data Labeling & Evaluation Market."
- **Narrative**: OpenArena creates the "Gold Standard" for AI capability tracking.
- **Integration**: Leverage **KaggleIngest** as the frontend for real-time visualization.

### 3. The Video (5 Minutes)
- **Show**: Validator generating a math puzzle -> Miner solving it -> Score updating on-chain.
- **Tell**: "We are measuring *real* intelligence, for the first time."

---

## üõ†Ô∏è Execution Strategy (Round II)
- **Phase 1**: Build a "Stub" subnet with simulated miners.
- **Phase 2**: Implement the Commit-Reveal mechanism on a local testnet.
- **Phase 3**: Connect KaggleIngest to show live "Generalization Leaderboards."



================================================
FILE: deliverables/pitch_deck.md
================================================
# OpenArena: The Pitch Deck

**Target Audience:** Bittensor Ideathon Judges & VC Investors
**Theme:** Neo-Brutalist, High Contrast, Urgent.

---

## Slide 1: The Hook

**Visual:** A split screen. Left: A robot reciting a dictionary (Static). Right: A robot navigating a shifting maze (Dynamic).
**Headline:** Static Benchmarks Are Dead.
**Sub-headline:** Introducing OpenArena: The First Decentralized Adversarial Evaluation Protocol.
**Speaker Notes:** "We are currently flying blind. We can no longer distinguish between a model that _remembers_ and a model that _thinks_. OpenArena is the solution."

---

## Slide 2: The Crisis (Market Problem)

**Visual:** A graph showing GSM8K scores hitting 95% saturation while "Real World Reliability" flatlines. Verified examples of "Data Contamination" (models completing test questions they shouldn't know).
**Headline:** The $10B Evaluation Gap.
**Core Stat:** "Goodhart's Law: When a measure becomes a target, it ceases to be a good measure."
**Speaker Notes:** "Every major lab is overfitting. MMLU is leaked. We need a test that changes every single day."

---

## Slide 3: The Solution (OpenArena)

**Visual:** The "Infinite Loop" of OpenArena.

1. **Validator** generates _fresh_ entropy (News, Math, Code).
2. **Miner** generalizes instantly.
3. **Score** assigned for adaptability.
   **Headline:** Proof of Generalization.
   **Value Prop:** "We don't test what you know. We test how fast you can learn."

---

## Slide 4: The Mechanism (Adversarial Hardening)

**Visual:** Deep dive into the **Commit-Reveal** architecture.

- Diagram showing `SHA256(Answer + Salt + Hotkey)`.
- A "Cheater" miner trying to copy-paste a hash and getting rejected.
  **Headline:** Uncheatable by Design.
  **Key Tech:**
- **Commit-Reveal:** Prevents front-running/weight-stealing.
- **Flash Challenges:** <200ms tasks to kill API wrappers.
- **Yuma Consensus:** Decentralized truth.

---

## Slide 5: The Business Model (The dTAO Flywheel)

**Visual:** A flywheel diagram.

1. **Demand:** Anthropic/OpenAI pay TAO to act as "Red Teamers".
2. **Action:** OpenArena Validators attack their models with adversarial prompts.
3. **Result:** Certified "Generalization Score".
4. **Value:** TAO burned/staked -> Subnet Value Increases.
   **Headline:** Evaluation-as-a-Service.
   **Speaker Notes:** "We turn 'Red Teaming' from a cost center into a decentralized commodity."

---

## Slide 6: The Unfair Advantage (KaggleIngest)

**Visual:** Screenshot of the **KaggleIngest** Dashboard with an "OpenArena" tab.

- Shows "Live Leaderboard" with familiar UI.
- "One-Click Submit" from Kaggle Notebooks.
  **Headline:** The Bridge from Web2 to Web3.
  **Key Point:** "We are not starting from zero. We are onboarding the 15M+ data scientists from Kaggle directly into the Bittensor ecosystem."

---

## Slide 7: The Roadmap (Execution)

**Visual:** Timeline.

- **Now:** Whitepaper & Protocol Design (Done).
- **Round II:** "Stub" Subnet (Simulated Miners).
- **Q3 2026:** Mainnet Launch + Kaggle Integration.
  **Headline:** From Idea to Standard.

---

## Slide 8: The Team

**Visual:** Headshots with "Kaggle Grandmaster" and "Blockchain Dev" badges.
**Headline:** Builders who know the Arena.
**Bio:** "We combine deep ML expertise with crypto-native mechanism design."

---

## Slide 9: The Ask

**Visual:** OpenArena Logomark.
**Headline:** Help Us Kill Static Benchmarks.
**Call to Action:** "Support OpenArena in the Ideathon. Let's build the Truth Machine."

---

## Slide 10: Appendix (Technical Specs)

**Visual:** The Scoring Function Equation.
$$ S = (Accuracy \times Novelty) - Latency $$
**Headline:** The Math of Intelligence.



================================================
FILE: deliverables/social_thread.md
================================================
# OpenArena X/Twitter Thread

**Post 1**
AI evaluation is broken. üìâ
Benchmarks like GSM8K and MMLU are saturated. Models are memorizing, not thinking.
We can no longer distinguish between a 100B param parrot and true intelligence.
It‚Äôs time to kill the static test set.
Introducing **OpenArena**: The Decentralized Adversarial Evaluation Protocol on @bittensor\_. üßµüëá

**Post 2**
Technical creativity > Memorization.
In OpenArena, Validators don't just grade tests‚Äîthey _generate_ them.
Fresh, high-entropy tasks every epoch.

- Synthetic Logic Puzzles
- Real-time News Summarization
- Code Generation for novel problems
  If your model can't generalize instantly, it scores zero. #ProofOfIntelligence

**Post 3**
üö´ The Cheating Problem.
Public leaderboards are plagued by "borderline" models that overfit to the test set.
OpenArena solves this with **Adversarial Hardening**:

1. **Dynamic Tasks**: Impossible to pre-train on.
2. **Commit-Reveal**: Cryptographically prevents front-running.
3. **Brier Scoring**: Ruthlessly penalizes "confident hallucinations."

**Post 4**
üöÄ The Unfair Advantage: **KaggleIngest**.
We aren't building an island. We're building a bridge.
OpenArena integrates exclusively with our **KaggleIngest** platform, allowing 15M+ Kaggle data scientists to deploy miners with ONE CLICK.
Web2 Talent ü§ù Web3 Incentives.

**Post 5**
The vision: **Evaluation-as-a-Service**.
Companies like Anthropic or xAI shouldn't grade their own homework.
In the future, they will pay the OpenArena subnet to "Red Team" their models against a global swarm of adversarial validators.
Trustless. Decentralized. Brutally honest.

**Post 6**
We are submitting OpenArena to the @bittensor\_ Ideathon.
Because the world needs a "Truth Machine" for AI Intelligence.
Read the Whitepaper & check the git: [Link]
Let the games begin. ‚öîÔ∏è
#Bittensor #AI #DeAI #MachineLearning #OpenSource



================================================
FILE: deliverables/video_script.md
================================================
# OpenArena: Video Explainer Script (5 Minutes)

**[0:00-0:45] The Hook: The Crisis of Trust**
_(Visual: Glitchy footage of "State of the Art" models failing simple logic puzzles.)_
"Every week, a new model claims 99% accuracy. But we all know the truth: They are memorizing the test. Benchmarks like GSM8K are static, leaked, and broken. We are measuring retrieval, not intelligence. And in a world of AI agents, that is dangerous."

**[0:45-1:30] The Solution: OpenArena**
_(Visual: Animated diagram of the OpenArena Loop. Tasks morphing shapes.)_
"Enter OpenArena. The world's first Decentralized Adversarial Evaluation Protocol. We don't use a fixed test set. Instead, our Validators act as 'Game Masters,' generating fresh, mathematically verified challenges every single epoch. Real-time news summarization. Synthetic logic puzzles. Code obfuscation."

**[1:30-2:30] The Mechanism: How It Works**
_(Visual: Code overlay of the `Synapse` protocol and Commit-Reveal flow.)_
"To win, a Miner must generalize. But they must also play fair. We've implemented a robust **Commit-Reveal Mechanism** directly on the Bittensor chain.

1. Miners solve the task.
2. They commit a cryptographic hash of their answer and a secret salt.
3. This prevents front-running. No one can steal your work.
4. Only after the window closes is the truth revealed."

**[2:30-3:30] The Business Case: dTAO Flywheel**
_(Visual: Spinning dTAO token and logos of AI Labs.)_
"This isn't just a game. It's a product. 'Evaluation-as-a-Service.' massive AI labs need independent verification. In OpenArena, they pay in TAO to stress-test their models against our network. This creates a sustainable demand loop for the subnet token."

**[3:30-4:30] The Secret Weapon: KaggleIngest**
_(Visual: Screen recording of the KaggleIngest dashboard showing OpenArena stats.)_
"And we have a secret weapon. We are integrating OpenArena into **KaggleIngest**. We are bringing the 15 million data scientists on Kaggle directly to Bittensor. One-click submission. Real-time leaderboards. A bridge between the best minds in data science and the best incentives in crypto."

**[4:30-5:00] The Close**
_(Visual: OpenArena Logo. "Join the Arena" text.)_
"Static benchmarks are history. Real intelligence adapts. Support OpenArena in the Ideathon. Let the games begin."



================================================
FILE: neurons/miner.py
================================================
import time
import typing
import bittensor as bt
from openarena.protocol import GeneralizationTask
from openarena.utils.crypto import hash_commitment, generate_salt
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class Miner:
    def __init__(self, config=None):
        self.config = config or self.get_config()
        self.wallet = bt.wallet(config=self.config)
        self.subtensor = bt.subtensor(config=self.config)
        self.metagraph = self.subtensor.metagraph(netuid=self.config.netuid)
        self.axon = bt.axon(wallet=self.wallet, config=self.config)

        # Local storage for commit-reveal (In-memory for MVP)
        # Key: query_hash, Value: (answer, salt)
        self.commitments: typing.Dict[str, typing.Tuple[str, str]] = {}

        # Load Model (Optimized for 4-bit/8-bit if available, here standard for compatibility)
        bt.logging.info("Loading LLM model...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # Using a lightweight model for demonstration/functionality; replaceable with larger models.
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.model = AutoModelForCausalLM.from_pretrained("gpt2").to(self.device)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        bt.logging.info(f"Miner initialized with wallet: {self.wallet} on device: {self.device}")

    def get_config(self):
        parser = bt.ArgumentParser()
        parser.add_argument('--netuid', type=int, default=1, help='The chain subnet uid.')
        bt.axon.add_args(parser)
        bt.subtensor.add_args(parser)
        bt.wallet.add_args(parser)
        return bt.config(parser)

    def forward(self, synapse: GeneralizationTask) -> GeneralizationTask:
        """
        The Miner's forward function.
        Handles both 'commit' and 'reveal' phases.
        """
        bt.logging.info(f"Received synapse from: {synapse.dendrite.hotkey} | Phase: {synapse.phase}")

        if synapse.phase == "commit":
            # 1. Generate Answer (Real LLM Inference)
            try:
                inputs = self.tokenizer(synapse.query, return_tensors="pt").to(self.device)
                outputs = self.model.generate(**inputs, max_new_tokens=50, pad_token_id=self.tokenizer.eos_token_id)
                answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            except Exception as e:
                bt.logging.error(f"Inference failed: {e}")
                answer = "Error during inference"

            # 2. Generate Salt
            salt = generate_salt()

            # 3. Store for later reveal
            # We use the query as the key. In production, use a more unique ID.
            self.commitments[synapse.query] = (answer, salt)

            # 4. Compute Commitment
            commitment = hash_commitment(answer, salt, self.wallet.hotkey.ss58_address)

            synapse.commitment = commitment
            bt.logging.info(f"Committed: {commitment}")

        elif synapse.phase == "reveal":
            # 1. Retrieve stored answer/salt
            if synapse.query in self.commitments:
                answer, salt = self.commitments[synapse.query]
                synapse.answer = answer
                synapse.salt = salt
                bt.logging.info(f"Revealed: {answer} (Salt: {salt})")

                # Cleanup (Optional: keep for a bit?)
                del self.commitments[synapse.query]
            else:
                bt.logging.error(f"No commitment found for query: {synapse.query}")

        return synapse

    def run(self):
        # Attach the forward function to the axon
        self.axon.attach(
            forward_fn=self.forward,
            blacklist_fn=self.blacklist,
            priority_fn=self.priority,
        )

        # Serve the axon
        bt.logging.info(f"Serving axon on port {self.config.axon.port}")
        self.axon.serve(netuid=self.config.netuid, subtensor=self.subtensor)

        # Start the axon
        bt.logging.info(f"Starting axon...")
        self.axon.start()

        # Keep the miner running
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            self.axon.stop()
            bt.logging.info("Miner stopped.")

    def blacklist(self, synapse: GeneralizationTask) -> typing.Tuple[bool, str]:
        # Implement blacklist logic (e.g., allow specific hotkeys)
        return False, "Allowed"

    def priority(self, synapse: GeneralizationTask) -> float:
        # Implement priority logic (e.g., stake-based)
        return 0.0

if __name__ == "__main__":
    miner = Miner()
    miner.run()



================================================
FILE: neurons/validator.py
================================================
import bittensor as bt
import time
import hashlib
from openarena.protocol import GeneralizationTask
from openarena.utils.crypto import verify_commitment

class Validator:
    def __init__(self, config=None):
        self.config = config or self.get_config()
        self.wallet = bt.wallet(config=self.config)
        self.subtensor = bt.subtensor(config=self.config)
        self.dendrite = bt.dendrite(wallet=self.wallet)
        self.metagraph = self.subtensor.metagraph(netuid=self.config.netuid)

        bt.logging.info(f"Validator initialized with wallet: {self.wallet}")

    def get_config(self):
        parser = bt.ArgumentParser()
        parser.add_argument('--netuid', type=int, default=1, help='The chain subnet uid.')
        bt.dendrite.add_args(parser)
        bt.subtensor.add_args(parser)
        bt.wallet.add_args(parser)
        return bt.config(parser)

    def run(self):
        bt.logging.info("Starting validator loop...")

        while True:
            try:
                # 1. Generate Task (Entropy Protocol)
                # VRF: Hash(BlockHeight + ValidatorKey + PrevBlockHash)
                block_height = self.subtensor.get_current_block()
                vrf_key = str(self.wallet.hotkey.ss58_address)
                # Fetch actual previous block hash for on-chain entropy
                prev_block_hash = self.subtensor.get_block_hash(block_height - 1)

                # Formal Entropy Seed Derivation
                entropy_seed = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()

                bt.logging.info(f"Generated Entropy Seed: {entropy_seed}")
                bt.logging.info(f"Derivation: SHA256({block_height} + {vrf_key[:8]}... + {prev_block_hash[:8]}...)")

                # In production, this seed drives the Task Generator.
                # For this stub, we use a simple string reversal, but the seed is ready.
                query = f"Task_{step}_{entropy_seed[:8]}: Reverse this string"
                bt.logging.info(f"\n--- Step {step}: {query} ---")

                # Filter miners (mock: select top 10 or random)
                # For this stub, we just query all available axons in the metagraph
                # But to avoid timeout on large nets, let's pick 2 for testing or assume local execution.
                # If running locally with miner, we need to know miner's IP/Port.
                # Standard pattern: Query metagraph.axons
                miner_axons = self.metagraph.axons
                if not miner_axons:
                    bt.logging.warning("No miners found in metagraph. Waiting...")
                    time.sleep(10)
                    continue

                # 2. Commit Phase
                bt.logging.info("Phase 1: COMMIT")
                responses_commit = self.dendrite.query(
                    miner_axons,
                    GeneralizationTask(query=query, phase="commit"),
                    deserialize=True,
                    timeout=5
                )

                # Store commitments
                # Key: miner_hotkey, Value: commitment_hash
                commitments = {}
                for axon, synapse in zip(miner_axons, responses_commit):
                    if synapse.commitment:
                        bt.logging.info(f"Miner {axon.hotkey} committed: {synapse.commitment}")
                        commitments[axon.hotkey] = synapse.commitment
                    else:
                        bt.logging.warning(f"Miner {axon.hotkey} failed to commit.")

                # 3. Wait (Reveal Window)
                bt.logging.info("Waiting for reveal window (simulated 2s)...")
                time.sleep(2)

                # 4. Reveal Phase
                bt.logging.info("Phase 2: REVEAL")
                responses_reveal = self.dendrite.query(
                    miner_axons,
                    GeneralizationTask(query=query, phase="reveal"),
                    deserialize=True,
                    timeout=5
                )

                # 5. Verify and Score
                scores = []
                for axon, synapse in zip(miner_axons, responses_reveal):
                    hotkey = axon.hotkey
                    if hotkey not in commitments:
                        bt.logging.info(f"Miner {hotkey} ignored (no commitment).")
                        continue

                    commitment = commitments[hotkey]
                    answer = synapse.answer
                    salt = synapse.salt

                    if not answer or not salt:
                        bt.logging.info(f"Miner {hotkey} failed to reveal.")
                        continue

                    # Verify Commitment
                    is_valid = verify_commitment(commitment, answer, salt, hotkey)
                    if is_valid:
                        bt.logging.success(f"Miner {hotkey} VERIFIED! Answer: {answer}")
                        # Score: Check if answer is correct (reverse string)
                        # Mock Logic:
                        expected = query[::-1]
                        if answer == expected:
                            bt.logging.success(f"Miner {hotkey} CORRECT!")
                        else:
                            bt.logging.info(f"Miner {hotkey} WRONG Answer.")
                    else:
                        bt.logging.error(f"Miner {hotkey} CHEATING attempt! Hash mismatch.")

                time.sleep(12) # Wait for roughly one block time

            except KeyboardInterrupt:
                break
            except Exception as e:
                bt.logging.error(f"Error in validator loop: {e}")
                time.sleep(5)

if __name__ == "__main__":
    validator = Validator()
    validator.run()



================================================
FILE: openarena/protocol.py
================================================
import bittensor as bt
from typing import Optional, List, Literal
import pydantic

class GeneralizationTask(bt.Synapse):
    """
    The GeneralizationTask Synapse.

    Validators send this to Miners with a 'query' and a 'phase'.
    Miners respond based on the phase:
    - 'commit': Return 'commitment' (hash).
    - 'reveal': Return 'answer' and 'salt'.
    """

    # The Challenge
    query: str = pydantic.Field(
        ...,
        title="Query",
        description="The task to be solved."
    )

    phase: Literal["commit", "reveal"] = pydantic.Field(
        "commit",
        title="Phase",
        description="The phase of the protocol: 'commit' or 'reveal'."
    )

    # The Commit Phase
    commitment: Optional[str] = pydantic.Field(
        None,
        title="Commitment",
        description="SHA256(answer + salt + miner_hotkey). Sent in 'commit' phase."
    )

    # The Reveal Phase
    answer: Optional[str] = pydantic.Field(
        None,
        title="Answer",
        description="The actual solution. Sent in 'reveal' phase."
    )

    salt: Optional[str] = pydantic.Field(
        None,
        title="Salt",
        description="A random salt string. Sent in 'reveal' phase."
    )

    required_hash_fields: List[str] = pydantic.Field(
        ["query", "phase"],
        title="Required Hash Fields",
        description="Fields required to compute the hash of this Synapse."
    )



================================================
FILE: openarena/utils/crypto.py
================================================
import hashlib
import secrets

def generate_salt(length: int = 16) -> str:
    """Generates a random hex salt."""
    return secrets.token_hex(length)

def hash_commitment(answer: str, salt: str, miner_hotkey: str) -> str:
    """
    Creates a SHA256 commitment hash.
    Format: SHA256(answer + salt + miner_hotkey)
    """
    data = f"{answer}{salt}{miner_hotkey}".encode("utf-8")
    return hashlib.sha256(data).hexdigest()

def verify_commitment(commitment: str, answer: str, salt: str, miner_hotkey: str) -> bool:
    """Verifies that the answer and salt match the commitment."""
    expected_hash = hash_commitment(answer, salt, miner_hotkey)
    return secrets.compare_digest(expected_hash, commitment)



================================================
FILE: submission/Incentive_Mechanism.md
================================================
# OpenArena: Incentive Mechanism Design

## 1. Core Philosophy: Proof of Generalization

Unlike traditional subnets that reward _weight availability_ or _loss on a fixed dataset_, OpenArena rewards **Generalization**.
We define Generalization ($G$) as the ability of a miner $i$ to minimize loss $\mathcal{L}$ on a distribution $D_t$ that is disjoint from all prior distributions $\{D_0, ..., D_{t-1}\}$.

$$ G*i(t) = \mathbb{E}*{x \sim D_t} [ S(M_i(x), y^*) ] $$

## 2. The Reward Function ($R$)

The reward for miner $i$ at epoch $t$ is calculated as an aggregate of their performance across $K$ tasks.

$$ R*i = \sigma \left( \sum*{k=1}^{K} w*k \cdot \left( \alpha \cdot \underbrace{\mathcal{A}(y*{ik}, y^\*_k)}_{\text{Accuracy}} + \beta \cdot \underbrace{\mathcal{C}(c*{ik}, y*{ik})}_{\text{Calibration}} - \gamma \cdot \underbrace{\mathcal{L}(l_{ik})}\_{\text{Latency}} \right) \right) $$

### 2.1 Component Definitions

#### Accuracy ($\mathcal{A}$)

For Generative Tasks (e.g., Summarization), we use a semantic similarity metric (BERTScore) or Levenshtein Distance ($Lev$).
$$ \mathcal{A}_{text} = 1 - \frac{Lev(y_{ik}, y^_*k)}{\max(|y*{ik}|, |y^_\_k|)} $$

For Logic/Math Tasks, we use a binary score:
$$ \mathcal{A}_{logic} = \mathbb{I}(y_{ik} == y^\*\_k) $$

#### Calibration ($\mathcal{C}$)

We incentivize miners to know their own uncertainty using the **Brier Score**.
Miners submit a confidence $c_{ik} \in [0, 1]$.
$$ \mathcal{C} = 1 - (c*{ik} - \mathcal{A}*{logic})^2 $$
_Rationale_: A miner that is 100% confident but wrong is penalized heavily. A miner that is 50% confident and wrong is penalized less.

#### Latency ($\mathcal{L}$)

Speed is critical for real-world utility. We apply an exponential decay penalty based on the time delta $\Delta t$ relative to the fastest correct submission $t_{min}$.
$$ \mathcal{L} = e^{\lambda (t*{ik} - t*{min})} - 1 $$

## 3. Consensus Mechanism (Yuma)

The final weight $W_i$ set on the Bittensor blockchain is a consensus of the normalized rewards from all validators $v \in V$.

$$ W*i = \frac{\sum*{v \in V} S*v \cdot R*{vi}}{\sum*{j \in M} \sum*{v \in V} S*v \cdot R*{vj}} $$

Where $S_v$ is the stake of validator $v$ (V-Trust).
Miners with the highest $W_i$ receive the largest emission of $TAO.

## 4. Sustainability: The Efficiency Multiplier ($\mathcal{E}$)

To ensure long-term sustainability and prevent the subnet from becoming just "who has the most H100s", we introduce an **Efficiency Multiplier**.
This favors miners who achieve high accuracy with lower latency (proxy for model efficiency) and consistent uptime.

$$ R\_{final} = R_i \times \mathcal{E}\_i $$

Where $\mathcal{E}$ boosts miners who consistently solve "Flash Challenges" (sub-200ms tasks) which are impossible for API wrappers to route in time.

## 5. Anti-Gaming & Adversarial Hardening

### 5.1 The Commit-Reveal Scheme (Anti-Front-Running)

To prevent "Copycat Mining" (listening to the mempool), we strictly enforce a two-phase process:

1. **Commit Phase**: Miner $i$ submits $H_i = \text{SHA256}(y_{ik} || \text{salt} || \text{hotkey}_i)$.
2. **Reveal Phase**: Miner $i$ submits $y_{ik}, \text{salt}$.
3. **Verification**: Validator checks $H_i' == H_i$. If mismatch, $R_i = 0$.

### 5.2 Flash Challenges (Anti-Wrapper)

Validators randomly inject "Flash Tasks" with a strict $T_{max} = 200ms$.

- **Goal**: Filter out miners who are just wrapping GPT-4/Claude via API (network latency > 200ms).
- **Penalty**: Failure to respond in time $\to$ Score penalty $\gamma$ increases.

### 5.3 High-Entropy Generation (Anti-Lookup)

Tasks are generated procedurally with random seeds, ensuring $P(Task_t \in \text{TrainingSet}) \approx 0$.

- _Math_: Random coefficients.
- _Logic_: Randomly generated rulesets.



================================================
FILE: submission/OpenArena_PitchDeck.md
================================================
# OpenArena: The Pitch Deck

**Target Audience:** Bittensor Ideathon Judges & VC Investors
**Theme:** Neo-Brutalist, High Contrast, Urgent.

---

## Slide 1: The Hook

**Visual:** A split screen. Left: A robot reciting a dictionary (Static) - _Monochrome, pixelated_. Right: A robot navigating a shifting maze (Dynamic) - _Vibrant Neon, smooth motion_.
**Headline:** Static Benchmarks Are Dead.
**Sub-headline:** Introducing OpenArena: The First Decentralized Adversarial Evaluation Protocol.
**Speaker Notes:** "We are currently flying blind. We can no longer distinguish between a model that _remembers_ and a model that _thinks_. OpenArena is the solution."

---

## Slide 2: The Crisis (Market Problem)

**Visual:** A rugged, "glitch-art" line graph. The "Benchmark Score" line soars to 99% (Green), while the "Real World Utility" line crashes to 0% (Red).
**Headline:** The $10B Evaluation Gap.
**Core Stat:** "Goodhart's Law: When a measure becomes a target, it ceases to be a good measure."
**Speaker Notes:** "Every major lab is overfitting. MMLU is leaked. We need a test that changes every single day."

---

## Slide 3: The Solution (OpenArena)

**Visual:** The "Infinite Loop" of OpenArena.

1. **Validator** generates _fresh_ entropy (News, Math, Code).
2. **Miner** generalizes instantly.
3. **Score** assigned for adaptability.
   **Headline:** Proof of Generalization.
   **Value Prop:** "We don't test what you know. We test how fast you can learn."

---

## Slide 4: The Mechanism (Adversarial Hardening)

**Visual:** Deep dive into the **Commit-Reveal** architecture.

- Diagram showing `SHA256(Answer + Salt + Hotkey)`.
- A "Cheater" miner trying to copy-paste a hash and getting rejected.
  **Headline:** Uncheatable by Design.
  **Key Tech:**
- **Commit-Reveal:** Prevents front-running/weight-stealing.
- **Flash Challenges:** <200ms tasks to kill API wrappers.
- **Yuma Consensus:** Decentralized truth.

---

## Slide 5: The Business Model (The dTAO Flywheel)

**Visual:** A flywheel diagram.

1. **Demand:** Anthropic/OpenAI pay TAO to act as "Red Teamers".
2. **Action:** OpenArena Validators attack their models with adversarial prompts.
3. **Result:** Certified "Generalization Score".
4. **Value:** TAO burned/staked -> Subnet Value Increases.
   **Headline:** Evaluation-as-a-Service.
   **Speaker Notes:** "We turn 'Red Teaming' from a cost center into a decentralized commodity."

---

## Slide 6: The Unfair Advantage (KaggleIngest)

**Visual:** High-fidelity screenshot of the **KaggleIngest** Dashboard. A bold "Connect to OpenArena" button pulsing in the corner. Background is a dark, data-dense grid.

- **Exclusive Portal**: "The Only Bridge for 15M+ Data Scientists."
- **Zero Friction**: "From Notebook to Miner in 30 seconds."
  **Headline:** The Bridge from Web2 to Web3.
  **Key Point:** "We are not starting from zero. We are onboarding the 15M+ data scientists from Kaggle directly into the Bittensor ecosystem via KaggleIngest."

---

## Slide 7: The Roadmap (Execution)

**Visual:** Timeline.

- **Now:** Whitepaper & Protocol Design (Done).
- **Round II:** "Stub" Subnet (Simulated Miners).
- **Q3 2026:** Mainnet Launch + Kaggle Integration.
  **Headline:** From Idea to Standard.

---

## Slide 8: The Team

**Visual:** Headshots with "Kaggle Grandmaster" and "Blockchain Dev" badges.
**Headline:** Builders who know the Arena.
**Bio:** "We combine deep ML expertise with crypto-native mechanism design."

---

## Slide 9: The Ask

**Visual:** OpenArena Logomark.
**Headline:** Help Us Kill Static Benchmarks.
**Call to Action:** "Support OpenArena in the Ideathon. Let's build the Truth Machine."

---

## Slide 10: Appendix (Technical Specs)

**Visual:** The Scoring Function Equation.
$$ S = (Accuracy \times Novelty) - Latency $$
**Headline:** The Math of Intelligence.



================================================
FILE: submission/OpenArena_SourceCode.tar.gz
================================================
[Binary file]


================================================
FILE: submission/OpenArena_VideoScript.md
================================================
# OpenArena: Explanation Video Script

**Target Duration:** 90-120 Seconds
**Tone:** Urgent, Technical, Visionary.
**Visual Style:** Fast-paced, Neo-Brutalist typography, glitch effects, code snippets overlay.

---

## 0:00 - 0:20: The Problem (The "Benchmark Saturation" Glitch)

**[Visual]**: Screen recording of a generic "Leaderboard" scrolling infinitely. The numbers "99.9%" start multiplying and covering the screen until it looks broken.
**[Audio]**: "We are in a crisis of measurement. GPT-4, Gemini, Claude‚Äîthey all ace the benchmarks. 90% on GSM8K. 95% on MMLU. But ask them to code a novel app, or reason through a new paradox, and they hallucinate."

**[Visual]**: Text slam: **GOODHART'S LAW**.
**[Audio]**: "When a measure becomes a target, it ceases to be a good measure. Static benchmarks are dead. They've been leaked, memorized, and gamed."

---

## 0:20 - 0:50: The Solution (Enter OpenArena)

**[Visual]**: A dark screen. A single prompt appears: `> INIT_OPEN_ARENA`.
**[Visual]**: A map of nodes (Validators and Miners) lighting up.
**[Audio]**: "Introducing OpenArena. The first decentralized, adversarial evaluation protocol on Bittensor."

**[Visual]**: Split screen.

- Left ("Old Way"): A teacher handling out the same test every year.
- Right ("OpenArena"): A sparring algorithm generating new moves every second.
  **[Audio]**: "In OpenArena, there is no static test set. Validators generate _fresh_, verified tasks every single epoch. Math puzzles. Code generation. Logic traps. Unseen. Un-memorizable."

---

## 0:50 - 1:10: How It Works (The Mechanics)

**[Visual]**: Diagram of the **Commit-Reveal** flow.

1. Info graphic: `Task -> Miner -> Hash(Answer)`.
2. A "Lock" icon appears.
3. Timer counts down.
4. `Reveal -> Score`.
   **[Audio]**: "Miners must solve these tasks instantly to prove _Generalization_, not memorization. We use a cryptographically secure Commit-Reveal scheme to prevent front-running, and a Brier-Score based calibration metric to punish hallucinations."

---

## 1:10 - 1:30: The Unfair Advantage (KaggleIngest)

**[Visual]**: The **KaggleIngest** Dashboard (Sleek, Dark Mode). Cursor clicks "Submit Model".
**[Audio]**: "But technology is nothing without distribution. We are leveraging **KaggleIngest** to onboard the 15 million data scientists from Kaggle directly into this arena."

**[Visual]**: Numbers ticking up: "Miners: 10... 100... 10,000".
**[Audio]**: "The world's best talent, competing to define the true 'Smartest Model'."

---

## 1:30 - 1:40: Call to Action

**[Visual]**: OpenArena Logo using the generated Neo-Brutalist assets.
**[Text]**: **THE PROOF OF INTELLIGENCE**.
**[Audio]**: "Static benchmarks are the past. Adversarial evaluation is the future. Join the Arena."

**[End Card]**: Github URL | Bittensor Ideathon Logo.



================================================
FILE: submission/OpenArena_Whitepaper.md
================================================
# OpenArena: The Decentralized Adversarial Evaluation Protocol

**"The Proof of Intelligence"**

> [!IMPORTANT]
> **Core Thesis**: Static benchmarks are dead. Intelligence is not the ability to memorize a fixed dataset; it is the ability to generalize to new, unseen distributions. OpenArena is a continuous, adversarial stress-test for AI models, turning evaluation into a verifiable digital commodity.

---

## 1. Introduction: The Crisis of Evaluation

Modern AI has a **Goodhart's Law** problem: "When a measure becomes a target, it ceases to be a good measure."

- **Contamination**: Public datasets (GSM8K, MMLU) leak into training data.
- **Saturation**: Top models score 90%+ on benchmarks but fail in production.
- **Trust**: Who validates the validator?

**OpenArena** solves this by creating a **Dynamic Adversarial Evaluation Game**.

- **Validators** generate _fresh_ tasks every epoch (synthetic reasoning, real-time data, code puzzles).
- **Miners** must solve these unseen tasks instantly.
- **Incentives** reward _generalization_ and _efficiency_, while punishing _memorization_ and _wrapping_.

### 1.1 Core Thesis: Proof of Intelligence

We define "Intelligence" not as knowledge retrieval, but as **Generalization Efficiency**:

> _The ability to solve novel, high-entropy tasks with minimum latency and compute._

This shift allows us to distinguish between a 100B parameter model that memorized the internet and a 7B parameter model that can actually _reason_.

---

## 2. Technical Architecture

### 2.1 The Flow of Intelligence

```mermaid
sequenceDiagram
    participant V as Validator (Game Master)
    participant M as Miner (Solver)
    participant C as Chain (Bittensor)

    Note over V, M: Epoch N Starts (Block 0-360)

    rect rgb(20, 20, 20)
        Note right of V: 1. Generate Dynamic Task<br/>(e.g. "Solve random math puzzle")
        V->>V: Hash(Task) + Encrypt(GroundTruth)
        V->>M: Broadcast Synapse (Task Only)
    end

    rect rgb(40, 40, 40)
        Note right of M: 2. Compute Solution<br/>(LLM Inference / Code Exec)
        M->>M: Hash(Answer + Salt)
        M->>C: Commit Hash (Prevents Front-running)
    end

    rect rgb(20, 20, 20)
        Note right of V: 3. Reveal Phase
        M->>V: Reveal Answer
        V->>V: Verify Hash matches Commit
        V->>V: Score(Answer, GroundTruth)
    end

    rect rgb(60, 20, 20)
        Note right of C: 4. Yuma Consensus
        V->>C: Set Weights (W_i)
        C->>M: Distribute TAO Rewards
    end
```

### 2.2 Component Roles

| Role          | Responsibility                                                                     | Incentive                                                                        |
| :------------ | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Miner**     | Solve arbitrary tasks (Text, Code, Math) with high accuracy and low latency.       | Maximizes Reward ($R$) by optimizing inference speed and model generalization.   |
| **Validator** | Generate high-entropy, non-repeatable tasks. Evaluate miner solutions objectively. | Maximizes Dividends ($D$) by attracting high-quality miners and staking support. |

---

## 3. Incentive Mechanism (The Math)

The core innovation is the **Generalization Score ($S$)**.

### 3.1 The Scoring Function

For a set of $N$ tasks in an epoch, a miner $i$'s score $S_i$ is:

$$ S*i = \underbrace{\alpha \cdot \frac{1}{N} \sum*{j=1}^{N} \text{Acc}(y\*{ij}, y^\*_{j})}*{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij}, \text{Acc}*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t_{ij})}\*{\text{Latency Penalty}} $$

Where:

- $\text{Acc}(y_{ij}, y^*_{j})$: Accuracy metric (0 or 1 for exact match, or Levenshtein/BLEU for text).
- $\text{Cal}$: **Calibration Score**. Rewards miners who are confident when correct and uncertain when wrong. We use the **Brier Score** decomposition:
  $$ \text{Brier} = \frac{1}{N} \sum\_{t=1}^{N} (f_t - o_t)^2 $$
  Where $f_t$ is the forecasted probability and $o_t$ is the outcome. This strictly penalizes "hallucinations" where a model claims high confidence but is wrong.
- $\text{Lat}$: **Latency Penalty**. $e^{t_{ij} - T_{max}}$.

### 3.2 Yuma Consensus & Weight Setting

Validators normalize scores to a weight vector $W$:
$$ w*{i} = \frac{e^{S_i / \tau}}{\sum*{k} e^{S_k / \tau}} $$
*(Using Softmax with temperature $\tau$ to control competition intensity)\*.

---

## 4. Adversarial Hardening (How We Win)

### üõ°Ô∏è Challenge 1: Memorization / Lookup

- **Attack**: Miners cache answers from previous epochs.
- **Defense**: **High-Entropy Generation**.
  - _Math_: "Calculate $A \times B$" where $A, B$ are random 10-digit primes.
  - _Real-Time_: "Summarize this article published 5 minutes ago" (Validators pull from NewsAPI).
  - _Code_: "Write a Python function to sort this random list [4, 1, 9...]".

### üõ°Ô∏è Challenge 2: Front-Running / Copying

- **Attack**: Fast miner sees a smart miner's answer in the mempool and copies it.
- **Defense**: **Commit-Reveal Scheme**.
  1.  Miner submits `Hash(Answer + Salt)`.
  2.  After window closes, Miner submits `Answer + Salt`.
  3.  Validator verifies hash matches.

### üõ°Ô∏è Challenge 3: Validator Collusion

- **Attack**: Validator shares Ground Truth with a specific miner.
- **Defense**: **Cross-Validation**.
  - Multiple validators score the same miner.
  - If Validator A's scores diverge significantly from the consensus media (Yuma Consensus), Validator A loses V-Trust and dividends.

---

## 5. Token Economics (The OpenArena Flywheel)

### 5.1 The Formal Value Loop ($V$)

Let $F$ be the fee paid by an enterprise (e.g., Anthropic) to prioritize a specific evaluation dataset $D_{target}$.

$$ F*{distribution} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{validators} + 0.2 \cdot F*{miners} $$

1.  **Burn ($40\%$)**: Permanently removed from supply, creating deflationary pressure on $\tau$.
2.  **Validator Reward ($40\%$)**: Distributed to validators proportional to their stake ($S_v$) and their **Curator Score** ($C_v$).
3.  **Miner Reward ($20\%$)**: Distributed to miners who solve $D_{target}$ with the highest **Generalization Score** ($G_m$).

This ensures that **Enterprise Demand** directly correlates with **Miner Profitability** and **Token Scarcity**.

---

## 6. Security Analysis (Adversarial Robustness)

### 6.1 Attack: Pre-Computation (The "Lookup Table")

- **Vector**: Miner pre-calculates answers to known datasets to simulate intelligence.
- **Mitigation**: **Cryptographic Entropy Protocol**.
  - Let $H_b$ be the block hash at height $t$.
  - Let $K_v$ be the Validator's VRF key.
  - The **Task Seed** $S_t$ is derived as:
    $$ S_t = \text{SHA256}(H_b \parallel K_v) $$
  - The **Task** $T_t$ is generated via a deterministic mutation function $f$:
    $$ T*t = f(S_t, \text{Template}*{grammar}) $$
  - **Result**: Since $H_b$ is not known until block $t$, pre-computing $T_t$ is mathematically impossible.

### 6.2 Attack: Validator Laziness (Low Entropy)

- **Vector**: A Validator reuses old tasks to save compute, degrading the network's measurement quality.
- **Mitigation**: **Entropy Penalty ($E_v$)**.
  - We measure the Kullback-Leibler (KL) divergence between task distributions at time $t$ and $t-1$:
    $$ E*v = D*{KL}(P*t \parallel P*{t-1}) $$
  - If $E_v < \epsilon_{threshold}$ (statistically indistinguishable from previous epoch), the Validator's weight-setting power $W_v$ is slashed:
    $$ W*v^{new} = W_v^{old} \cdot (1 - \text{Penalty}*{lazy}) $$

### 6.3 Attack: Front-Running (The "Copycat")

- **Mitigation**: **Commit-Reveal** (as defined in Section 4).
  - $t_0$: Miner submits $H = \text{SHA256}(Answer + Salt)$.
  - $t_1$: Reveal window opens.
  - Copycats only see hash $H$, preventing answer theft.

---

## 7. Go-To-Market & Integration (The KaggleIngest Advantage)

**OpenArena** is not just a protocol; it is a bridge. We leverage **KaggleIngest** to be the **exclusive portal** for on-boarding the 15M+ Kaggle data scientists into Bittensor.

### 7.1 The Cold Start Solution

Most subnets fail because they cannot attract talent. We solve this by meeting miners where they already are.

- **Direct Integration**: Kaggle Notebooks can submit to OpenArena via a single Python cell (`!pip install openarena-kaggle`).
- **Leaderboard Sync**: Real-time display of Miner Generalization Scores on a familiar, Web2-style dashboard.
- **The Museum**: We archive the "Hardest Tasks" generated by Validators, creating a valuable, ever-growing dataset of "adversarial examples" that researchers can download.

---

## 6. Execution Roadmap (Round II Strategy)

### Phase 1: The "Stub" (Days 1-5)

- [ ] Implement `neurons/validator.py`: Basic task generation (Math/Logic).
- [ ] Implement `neurons/miner.py`: Basic OpenAI/Llama wrapper.
- [ ] Implement `commt-reveal` mechanism on-chain (using mock chain).

### Phase 2: The "Arena" (Days 6-12)

- [ ] Connect KaggleIngest frontend to Subnet stats.
- [ ] Deploy 5 Miner nodes (simulated) to show competition.
- [ ] Create visualization of "Score Drift" over time.



================================================
FILE: submission/SUBMISSION.md
================================================
# OpenArena: The Proof of Intelligence Subnet

**Team**: OpenArena (Anand / KaggleIngest)
**Track**: Bittensor Ideathon

## Abstract

OpenArena solves the "Static Benchmark Crisis" in AI evaluation. By generating high-entropy, novel tasks every epoch and using a Commit-Reveal scheme to prevent gaming, we create a "Proof of Intelligence" that rewards generalization, not memorization.

## Core Deliverables

### 1. [Whitepaper](OpenArena_Whitepaper.md)

The comprehensive technical and economic architecture of the subnet.

- **Key Innovation**: "Generalization Efficiency" Score.
- **Security**: Commit-Reveal & Yuma Consensus.
- **Economics**: The "Alpha Demand Loop" for enterprise audits.

### 2. [Pitch Deck](OpenArena_PitchDeck.md)

A 10-slide visual overview of the $10B market opportunity and our go-to-market strategy via KaggleIngest.

### 3. [Video Script](OpenArena_VideoScript.md)

The narrative script for our submission video, demonstrating the "Live Leaderboard" and "Miner War Room."

### 4. [Incentive Mechanism](Incentive_Mechanism.md)

The rigorous mathematical definition of our scoring function: $S = \text{Accuracy} \times \text{Calibration} - \text{Latency}$.

## Proof of Implementation (Simulation)

We have successfully simulated the subnet lifecycle, including adversarial scenarios.

- **[Source Code](OpenArena_SourceCode.tar.gz)**: Full Python implementation of Miners, Validators, and Protocol.
- **Simulation Results**: `demo.py` demonstrates:
  - **Honest Miners**: Converge to high weights.
  - **Front-Runners**: Slashed to zero (proven via simulation logs).
  - **Lazy Miners**: Penalized for low accuracy.

## Go-To-Market

We leverage the existing **KaggleIngest** platform as a frontend visualization layer, creating an immediate, usable product for the Bittensor ecosystem.



================================================
FILE: tests/test_entropy.py
================================================
import hashlib
import sys

# Mocking the wallet part
class MockHotkey:
    @property
    def ss58_address(self):
        return "5F3sa2TJAUVfwsj5hbPxr3rV8g9pJjS6XjT99...MockKey"

class MockWallet:
    def __init__(self):
        self.hotkey = MockHotkey()

def test_entropy_generation():
    wallet = MockWallet()
    step = 123

    # 1. Generate Task (Entropy Protocol)
    # Simulated VRF: Hash(BlockHeight + ValidatorKey + PrevBlockHash)
    # In production, this uses on-chain randomness.
    block_height = step
    vrf_key = str(wallet.hotkey.ss58_address)
    # Simulating previous block hash for entropy chain
    prev_block_hash = hashlib.sha256(str(step - 1).encode()).hexdigest()

    # Formal Entropy Seed Derivation
    entropy_seed = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()

    print(f"Test Step: {step}")
    print(f"VRF Key: {vrf_key}")
    print(f"Prev Hash: {prev_block_hash}")
    print(f"Generated Entropy Seed: {entropy_seed}")

    # Verify deterministic property
    entropy_seed_2 = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()
    assert entropy_seed == entropy_seed_2, "Entropy seed must be deterministic!"
    print("SUCCESS: Entropy seed is deterministic and generated correctly.")

if __name__ == "__main__":
    test_entropy_generation()


