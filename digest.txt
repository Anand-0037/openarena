Directory structure:
‚îî‚îÄ‚îÄ bittensor-ideathon/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ dashboard.html
    ‚îú‚îÄ‚îÄ demo.py
    ‚îú‚îÄ‚îÄ INCENTIVE_MECHANISM.md
    ‚îú‚îÄ‚îÄ leaderboard_data.json
    ‚îú‚îÄ‚îÄ PROPOSAL.md
    ‚îú‚îÄ‚îÄ WHITE_PAPER.md
    ‚îú‚îÄ‚îÄ winning-strategy.md.resolved
    ‚îú‚îÄ‚îÄ deliverables/
    ‚îÇ   ‚îú‚îÄ‚îÄ pitch_deck.md
    ‚îÇ   ‚îú‚îÄ‚îÄ social_thread.md
    ‚îÇ   ‚îú‚îÄ‚îÄ twitter_thread.md
    ‚îÇ   ‚îî‚îÄ‚îÄ video_script.md
    ‚îú‚îÄ‚îÄ neurons/
    ‚îÇ   ‚îú‚îÄ‚îÄ miner.py
    ‚îÇ   ‚îî‚îÄ‚îÄ validator.py
    ‚îú‚îÄ‚îÄ openarena/
    ‚îÇ   ‚îú‚îÄ‚îÄ protocol.py
    ‚îÇ   ‚îú‚îÄ‚îÄ frontend/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eslint.config.mjs
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ next.config.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postcss.config.mjs
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ app/
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ globals.css
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ whitepaper/
    ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ page.tsx
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Mermaid.tsx
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ content/
    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ whitepaper.md
    ‚îÇ   ‚îî‚îÄ‚îÄ utils/
    ‚îÇ       ‚îî‚îÄ‚îÄ crypto.py
    ‚îú‚îÄ‚îÄ submission/
    ‚îÇ   ‚îú‚îÄ‚îÄ Incentive_Mechanism.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_PitchDeck.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_SourceCode.tar.gz
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_VideoScript.md
    ‚îÇ   ‚îú‚îÄ‚îÄ OpenArena_Whitepaper.md
    ‚îÇ   ‚îî‚îÄ‚îÄ SUBMISSION.md
    ‚îî‚îÄ‚îÄ tests/
        ‚îî‚îÄ‚îÄ test_entropy.py

================================================
FILE: README.md
================================================
# OpenArena: The Coliseum of Intelligence

![OpenArena Logo](openarena_logo.png)

> **"The only true measure of intelligence is the ability to adapt to the unknown."**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Bittensor](https://img.shields.io/badge/Bittensor-Subnet-blue)](https://bittensor.com/)

---

## üèõÔ∏è Overview

**Live Demo**: [https://openarena.kaggleingest.com](https://openarena.kaggleingest.com)

**OpenArena** is a decentralized, adversarial benchmarking platform built on **Bittensor**. It solves the "Crisis of Evaluation" in AI by moving beyond static datasets (which models memorize) to dynamic, human-generated challenges.

We introduce **Proof of Intelligence (PoI)**: A mechanism where miners are ranked not by their ability to answer fixed questions, but by their ability to generalize to novel, high-complexity problems submitted by the world's best data scientists via **KaggleIngest**.

## üöÄ Key Features

- **Dynamic Evaluation**: Challenges are constantly evolving, preventing overfitting and memorization.
- **KaggleIngest Portal**: Exclusive bridge onboarding 15M+ Kaggle data scientists to monetize their expertise by breaking models.
- **Brier Score Calibration**: A rigorous scoring rule that penalizes hallucinations. Miners must know what they don't know.
- **Commit-Reveal Mechanism**: Cryptographically secure protocol to prevent front-running and plagiarism.

## üõ†Ô∏è Architecture

### The Arena (Validator)

The Validator acts as the "Gamemaster," orchestrating the flow of challenges and verifying the integrity of the competition.

- **Entropy Source**: Derivates unpredictability from on-chain block hashes.
- **Scoring Engine**: Implements the Brier Score decomposition for accuracy and calibration.

### The Gladiator (Miner)

Miners are the AI models entering the arena.

- **Adaptive Inference**: Leverages state-of-the-art LLMs (Llama 3, Mistral, GPT-4o) to solve reasoning tasks.
- **Self-Correction**: Internal loops to verify answers before commitment.

## ‚ö° Quick Start

### Prerequisites

- Python 3.10+
- Bittensor
- Torch & Transformers

### Installation

```bash
git clone https://github.com/your-username/openarena.git
cd openarena
pip install -r requirements.txt
pip install -e .
```

### Running a Miner

```bash
python neurons/miner.py --netuid <your_netuid> --wallet.name <your_wallet> --wallet.hotkey <your_hotkey> --logging.debug
```

### Running a Validator

```bash
python neurons/validator.py --netuid <your_netuid> --wallet.name <your_wallet> --wallet.hotkey <your_hotkey> --logging.debug
```

## üìú Roadmap

- [x] **Phase 1: Foundation**: Core Commit-Reveal Protocol, Basic Scoring.
- [ ] **Phase 2: The Bridge**: KaggleIngest Integration & Bounty Smart Contracts.
- [ ] **Phase 3: The Coliseum**: 3D Visualization of Model Battles.
- [ ] **Phase 4: AGI**: Recursive Self-Improvement Loops.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built for the Bittensor Ideathon 2026.**



================================================
FILE: dashboard.html
================================================
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>OpenArena Leaderboard (Powered by KaggleIngest)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
      body {
        background-color: #0f1117;
        color: #e2e8f0;
        font-family: "Inter", sans-serif;
      }
      .card {
        background-color: #1e293b;
        border: 1px solid #334155;
      }
    </style>
  </head>
  <body class="p-8">
    <!-- Header -->
    <div class="flex justify-between items-center mb-8">
      <div>
        <h1
          class="text-3xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-blue-400 to-purple-500"
        >
          OpenArena
          <span class="text-sm text-gray-400 font-mono">Testnet Beta</span>
        </h1>
        <p class="text-gray-400">Decentralized Adversarial Evaluation</p>
      </div>
      <div class="flex gap-4">
        <span
          class="px-4 py-2 bg-green-900/30 text-green-400 rounded border border-green-800 animate-pulse"
        >
          ‚óè Live Phase: Commit
        </span>
        <span class="px-4 py-2 bg-blue-600 rounded font-bold"
          >Connect Wallet</span
        >
      </div>
    </div>

    <!-- Stats Grid -->
    <div class="grid grid-cols-1 md:grid-cols-4 gap-6 mb-8">
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Active Miners</h3>
        <p class="text-2xl font-bold">256</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Current Epoch</h3>
        <p class="text-2xl font-bold">#4,291</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Avg Generalization Score</h3>
        <p class="text-2xl font-bold text-yellow-400">0.872</p>
      </div>
      <div class="card p-6 rounded-xl">
        <h3 class="text-gray-400 text-sm">Next Challenge</h3>
        <p class="text-md font-mono text-purple-400">MATH::Polynomial_Roots</p>
      </div>
    </div>

    <!-- Main Content -->
    <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
      <!-- Leaderboard -->
      <div class="lg:col-span-2 card rounded-xl overflow-hidden">
        <div class="p-6 border-b border-gray-700 flex justify-between">
          <h2 class="text-xl font-bold">Live Generalization Leaderboard</h2>
          <button class="text-sm text-blue-400 hover:text-blue-300">
            View All
          </button>
        </div>
        <table class="w-full text-left">
          <thead class="bg-gray-800 text-gray-400">
            <tr>
              <th class="p-4">Rank</th>
              <th class="p-4">Miner ID</th>
              <th class="p-4">Score (G)</th>
              <th class="p-4">Latency</th>
              <th class="p-4">Status</th>
            </tr>
          </thead>
          <tbody id="leaderboard-body" class="divide-y divide-gray-700">
            <!-- Rows will be injected here -->
          </tbody>
        </table>
      </div>

      <!-- Live Feed -->
      <div class="card rounded-xl p-6">
        <h2 class="text-xl font-bold mb-4">Live Activity Log</h2>
        <div
          id="activity-log"
          class="space-y-3 font-mono text-sm max-h-[400px] overflow-y-auto"
        >
          <!-- Logs injected here -->
        </div>
      </div>
    </div>

    <script>
      // Start Mock Data Injection immediately
      const miners = [
        { id: "5H...x92", score: 0.982, lat: "45ms", status: "Verified" },
        { id: "5C...k21", score: 0.941, lat: "120ms", status: "Verified" },
        { id: "5D...j88", score: 0.81, lat: "410ms", status: "Verified" },
        { id: "5K...m11", score: 0.0, lat: "-", status: "Slash (Copycat)" },
      ];

      const tbody = document.getElementById("leaderboard-body");
      miners.forEach((m, i) => {
        const row = `
                <tr class="hover:bg-gray-800/50">
                    <td class="p-4 font-bold text-gray-500">#${i + 1}</td>
                    <td class="p-4 font-mono text-blue-300">${m.id}</td>
                    <td class="p-4 font-bold ${m.score > 0.9 ? "text-green-400" : "text-gray-300"}">${m.score.toFixed(3)}</td>
                    <td class="p-4 text-gray-400">${m.lat}</td>
                    <td class="p-4">
                        <span class="px-2 py-1 rounded text-xs ${m.score > 0 ? "bg-green-900 text-green-300" : "bg-red-900 text-red-300"}">
                            ${m.status}
                        </span>
                    </td>
                </tr>
            `;
        tbody.innerHTML += row;
      });

      // Mock Live Activity
      const logs = document.getElementById("activity-log");
      const actions = [
        "Miner 5H...x92 committed hash 0x7f2...",
        "Validator generated new task: 12 + 99",
        "Miner 5C...k21 revealed answer: 111",
        "Miner 5K...m11 penalized for late reveal",
        "New Block Finalized #4392",
      ];

      actions.forEach((a, i) => {
        setTimeout(() => {
          const div = document.createElement("div");
          div.className =
            "p-2 bg-black/20 rounded border-l-2 border-blue-500 animate-fade-in";
          div.innerHTML = `<span class="text-gray-500">[${new Date().toLocaleTimeString()}]</span> ${a}`;
          logs.prepend(div);
        }, i * 800);
      });
    </script>
  </body>
</html>



================================================
FILE: demo.py
================================================
import time
import json
import random
import hashlib
from typing import List, Dict

# Mock Protocol
class Synapse:
    def __init__(self, query: str):
        self.query = query
        self.answer = None
        self.salt = None
        self.commitment = None
        self.miner_id = None
        self.score = 0.0

# Mock Validator
class Validator:
    def __init__(self, uid, name):
        self.uid = uid
        self.name = name

    def generate_task(self) -> str:
        ops = ['+', '-', '*']
        a, b = random.randint(10, 99), random.randint(1, 9)
        op = random.choice(ops)
        return f"{a} {op} {b}"

    def solve_ground_truth(self, task: str) -> str:
        return str(eval(task))

    def evaluate(self, task: str, synapse: Synapse) -> float:
        # Check Commitment
        expected_hash = hashlib.sha256(f"{synapse.answer}{synapse.salt}{synapse.miner_id}".encode()).hexdigest()
        if synapse.commitment != expected_hash:
             # Malformed/Fake commit
            return 0.0

        ground_truth = self.solve_ground_truth(task)
        if synapse.answer == ground_truth:
            return 1.0
        return 0.0

# Mock Miner (Base Class)
class MockMiner:
    def __init__(self, uid: int, name: str):
        self.uid = uid
        self.name = name
        self.mempool = [] # Simulating visibility of others' commits

    def commit(self, task: str) -> tuple:
        # Default honest behavior
        answer = str(eval(task))
        salt = str(random.randint(1000, 9999))
        commitment = hashlib.sha256(f"{answer}{salt}{self.uid}".encode()).hexdigest()
        return commitment, answer, salt

    def reveal(self, task: str, answer: str, salt: str) -> dict:
        return {"answer": answer, "salt": salt}

# Malicious Miner: The Front-Runner (Copycat)
class FrontRunnerMiner(MockMiner):
    def commit(self, task: str) -> tuple:
        # Tries to copy, but can't see the salt/answer until Reveal phase.
        # This simulates the failure of front-running in Commit-Reveal.
        # He submits a random hash hoping to get lucky or replay an old one.
        fake_ans = "0"
        salt = "0000"
        commitment = hashlib.sha256(f"{fake_ans}{salt}{self.uid}".encode()).hexdigest()
        return commitment, fake_ans, salt

# Simulation Loop
def run_simulation(epochs=15):
    validator = Validator(0, "Validator_Main")

    miners = [
        MockMiner(0, "Miner_Alphazero (Honest)"),
        MockMiner(1, "Miner_GPT4 (Honest)"),
        FrontRunnerMiner(2, "Miner_Copycat (Malicious)"),
        MockMiner(3, "Miner_Lazy (Random)"),
    ]

    leaderboard_data = []

    # Weight Tracking
    miner_weights = {m.uid: 0.5 for m in miners} # Start equal

    print(f"--- Starting Adversarial Simulation ({epochs} Epochs) ---")

    for epoch in range(epochs):
        task = validator.generate_task()
        print(f"\n[Epoch {epoch+1}] New Task: {task}")

        # Phase 1: Commit
        commits = {}
        secrets = {}
        for miner in miners:
            com, ans, salt = miner.commit(task)
            commits[miner.uid] = com
            secrets[miner.uid] = (ans, salt)

            # Simulate Copycat trying to peek (but only seeing hashes)
            if isinstance(miner, FrontRunnerMiner):
                 print(f"  > {miner.name} is scanning mempool... only sees hashes.")

        # Phase 2: Reveal
        epoch_scores = {}
        for miner in miners:
            synapse = Synapse(task)
            synapse.miner_id = miner.uid
            synapse.commitment = commits[miner.uid]

            # Miner reveals
            revealed = miner.reveal(task, *secrets[miner.uid])
            synapse.answer = revealed['answer']
            synapse.salt = revealed['salt']

            # Validator scores
            score = validator.evaluate(task, synapse)

            # Additional logic for "Lazy" miner
            if miner.name == "Miner_Lazy (Random)":
                synapse.answer = str(random.randint(0,100))
                score = validator.evaluate(task, synapse) # Re-evaluate with wrong answer

            epoch_scores[miner.uid] = score

            # Update Moving Average Weights (Yuma-lite)
            alpha = 0.2
            miner_weights[miner.uid] = (1 - alpha) * miner_weights[miner.uid] + (alpha * score)

            status = "Verified" if score > 0 else "Failed/Slashed"
            print(f"  > {miner.name}: {status} (Score: {score:.2f})")

            leaderboard_data.append({
                "epoch": epoch,
                "miner": miner.name,
                "score": miner_weights[miner.uid], # Plotting Weight Convergence
                "latency": random.uniform(0.1, 0.5)
            })

    # Export
    with open("leaderboard_data.json", "w") as f:
        json.dump(leaderboard_data, f, indent=2)

    print("\n--- Simulation Complete. Weights Converged. ---")

if __name__ == "__main__":
    run_simulation()



================================================
FILE: INCENTIVE_MECHANISM.md
================================================
# OpenArena: Incentive Mechanism Design

## 1. Core Philosophy: Proof of Generalization

Unlike traditional subnets that reward _weight availability_ or _loss on a fixed dataset_, OpenArena rewards **Generalization**.
We define Generalization ($G$) as the ability of a miner $i$ to minimize loss $\mathcal{L}$ on a distribution $D_t$ that is disjoint from all prior distributions $\{D_0, ..., D_{t-1}\}$.

$$ G*i(t) = \mathbb{E}*{x \sim D_t} [ S(M_i(x), y^*) ] $$

## 2. The Reward Function ($R$)

The reward for miner $i$ at epoch $t$ is calculated as an aggregate of their performance across $K$ tasks.

$$ R*i = \sigma \left( \sum*{k=1}^{K} w*k \cdot \left( \alpha \cdot \underbrace{\mathcal{A}(y*{ik}, y^\*_k)}_{\text{Accuracy}} + \beta \cdot \underbrace{\mathcal{C}(c*{ik}, y*{ik})}_{\text{Calibration}} - \gamma \cdot \underbrace{\mathcal{L}(l_{ik})}\_{\text{Latency}} \right) \right) $$

### 2.1 Component Definitions

#### Accuracy ($\mathcal{A}$)

For Generative Tasks (e.g., Summarization), we use a semantic similarity metric (BERTScore) or Levenshtein Distance ($Lev$).
$$ \mathcal{A}_{text} = 1 - \frac{Lev(y_{ik}, y^_*k)}{\max(|y*{ik}|, |y^_\_k|)} $$

For Logic/Math Tasks, we use a binary score:
$$ \mathcal{A}_{logic} = \mathbb{I}(y_{ik} == y^\*\_k) $$

#### Calibration ($\mathcal{C}$)

We incentivize miners to know their own uncertainty using the **Brier Score**.
Miners submit a confidence $c_{ik} \in [0, 1]$.
$$ \mathcal{C} = 1 - (c*{ik} - \mathcal{A}*{logic})^2 $$
_Rationale_: A miner that is 100% confident but wrong is penalized heavily. A miner that is 50% confident and wrong is penalized less.

#### Latency ($\mathcal{L}$)

Speed is critical for real-world utility. We apply an exponential decay penalty based on the time delta $\Delta t$ relative to the fastest correct submission $t_{min}$.
$$ \mathcal{L} = e^{\lambda (t*{ik} - t*{min})} - 1 $$

## 3. Consensus Mechanism (Yuma)

The final weight $W_i$ set on the Bittensor blockchain is a consensus of the normalized rewards from all validators $v \in V$.

$$ W*i = \frac{\sum*{v \in V} S*v \cdot R*{vi}}{\sum*{j \in M} \sum*{v \in V} S*v \cdot R*{vj}} $$

Where $S_v$ is the stake of validator $v$ (V-Trust).
Miners with the highest $W_i$ receive the largest emission of $TAO.

## 4. Sustainability: The Efficiency Multiplier ($\mathcal{E}$)

To ensure long-term sustainability and prevent the subnet from becoming just "who has the most H100s", we introduce an **Efficiency Multiplier**.
This favors miners who achieve high accuracy with lower latency (proxy for model efficiency) and consistent uptime.

$$ R\_{final} = R_i \times \mathcal{E}\_i $$

Where $\mathcal{E}$ boosts miners who consistently solve "Flash Challenges" (sub-200ms tasks) which are impossible for API wrappers to route in time.

## 5. Anti-Gaming & Adversarial Hardening

### 5.1 The Commit-Reveal Scheme (Anti-Front-Running)

To prevent "Copycat Mining" (listening to the mempool), we strictly enforce a two-phase process:

1. **Commit Phase**: Miner $i$ submits $H_i = \text{SHA256}(y_{ik} || \text{salt} || \text{hotkey}_i)$.
2. **Reveal Phase**: Miner $i$ submits $y_{ik}, \text{salt}$.
3. **Verification**: Validator checks $H_i' == H_i$. If mismatch, $R_i = 0$.

### 5.2 Flash Challenges (Anti-Wrapper)

Validators randomly inject "Flash Tasks" with a strict $T_{max} = 200ms$.

- **Goal**: Filter out miners who are just wrapping GPT-4/Claude via API (network latency > 200ms).
- **Penalty**: Failure to respond in time $\to$ Score penalty $\gamma$ increases.

### 5.3 High-Entropy Generation (Anti-Lookup)

Tasks are generated procedurally with random seeds, ensuring $P(Task_t \in \text{TrainingSet}) \approx 0$.

- _Math_: Random coefficients.
- _Logic_: Randomly generated rulesets.



================================================
FILE: leaderboard_data.json
================================================
[
  {
    "epoch": 0,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.6000000000000001,
    "latency": 0.1779961225778374
  },
  {
    "epoch": 0,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.6000000000000001,
    "latency": 0.2260547286770072
  },
  {
    "epoch": 0,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.4,
    "latency": 0.42816569126833437
  },
  {
    "epoch": 0,
    "miner": "Miner_Lazy (Random)",
    "score": 0.4,
    "latency": 0.3451018315649287
  },
  {
    "epoch": 1,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.6800000000000002,
    "latency": 0.13495944646620664
  },
  {
    "epoch": 1,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.6800000000000002,
    "latency": 0.3250927723571596
  },
  {
    "epoch": 1,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.32000000000000006,
    "latency": 0.2859713861665302
  },
  {
    "epoch": 1,
    "miner": "Miner_Lazy (Random)",
    "score": 0.32000000000000006,
    "latency": 0.17195844026448573
  },
  {
    "epoch": 2,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.7440000000000002,
    "latency": 0.4325155942340365
  },
  {
    "epoch": 2,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.7440000000000002,
    "latency": 0.48716244127292485
  },
  {
    "epoch": 2,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.25600000000000006,
    "latency": 0.16149136799756392
  },
  {
    "epoch": 2,
    "miner": "Miner_Lazy (Random)",
    "score": 0.25600000000000006,
    "latency": 0.11665523406938326
  },
  {
    "epoch": 3,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.7952000000000001,
    "latency": 0.382426105641567
  },
  {
    "epoch": 3,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.7952000000000001,
    "latency": 0.4896377972814958
  },
  {
    "epoch": 3,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.20480000000000007,
    "latency": 0.3756796340973141
  },
  {
    "epoch": 3,
    "miner": "Miner_Lazy (Random)",
    "score": 0.20480000000000007,
    "latency": 0.38800551035441666
  },
  {
    "epoch": 4,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8361600000000002,
    "latency": 0.2412278643189834
  },
  {
    "epoch": 4,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8361600000000002,
    "latency": 0.4437119138788751
  },
  {
    "epoch": 4,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.16384000000000007,
    "latency": 0.38899145174946836
  },
  {
    "epoch": 4,
    "miner": "Miner_Lazy (Random)",
    "score": 0.16384000000000007,
    "latency": 0.12554785344385355
  },
  {
    "epoch": 5,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8689280000000001,
    "latency": 0.19372422866653294
  },
  {
    "epoch": 5,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8689280000000001,
    "latency": 0.3085929276654661
  },
  {
    "epoch": 5,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.13107200000000005,
    "latency": 0.4115953267986575
  },
  {
    "epoch": 5,
    "miner": "Miner_Lazy (Random)",
    "score": 0.13107200000000005,
    "latency": 0.3290462563471851
  },
  {
    "epoch": 6,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.8951424000000001,
    "latency": 0.2683404735835466
  },
  {
    "epoch": 6,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.8951424000000001,
    "latency": 0.11966590829196573
  },
  {
    "epoch": 6,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.10485760000000005,
    "latency": 0.4144742142997404
  },
  {
    "epoch": 6,
    "miner": "Miner_Lazy (Random)",
    "score": 0.10485760000000005,
    "latency": 0.10067328064373814
  },
  {
    "epoch": 7,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9161139200000001,
    "latency": 0.1721770009561337
  },
  {
    "epoch": 7,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9161139200000001,
    "latency": 0.4787433436349503
  },
  {
    "epoch": 7,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.08388608000000004,
    "latency": 0.47276186860969693
  },
  {
    "epoch": 7,
    "miner": "Miner_Lazy (Random)",
    "score": 0.08388608000000004,
    "latency": 0.1264771865777375
  },
  {
    "epoch": 8,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9328911360000001,
    "latency": 0.3121905644720486
  },
  {
    "epoch": 8,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9328911360000001,
    "latency": 0.44482428780089145
  },
  {
    "epoch": 8,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.06710886400000003,
    "latency": 0.3185804104042277
  },
  {
    "epoch": 8,
    "miner": "Miner_Lazy (Random)",
    "score": 0.06710886400000003,
    "latency": 0.15638594507542708
  },
  {
    "epoch": 9,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9463129088000002,
    "latency": 0.2177532814190792
  },
  {
    "epoch": 9,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9463129088000002,
    "latency": 0.11530520382456128
  },
  {
    "epoch": 9,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.05368709120000003,
    "latency": 0.24314636902218467
  },
  {
    "epoch": 9,
    "miner": "Miner_Lazy (Random)",
    "score": 0.05368709120000003,
    "latency": 0.46758288691755945
  },
  {
    "epoch": 10,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9570503270400001,
    "latency": 0.20948750243262812
  },
  {
    "epoch": 10,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9570503270400001,
    "latency": 0.4454572161500582
  },
  {
    "epoch": 10,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.042949672960000025,
    "latency": 0.25394487540931704
  },
  {
    "epoch": 10,
    "miner": "Miner_Lazy (Random)",
    "score": 0.042949672960000025,
    "latency": 0.1790624583828751
  },
  {
    "epoch": 11,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9656402616320001,
    "latency": 0.18377145742195952
  },
  {
    "epoch": 11,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9656402616320001,
    "latency": 0.1959496593006459
  },
  {
    "epoch": 11,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.03435973836800002,
    "latency": 0.3710789184823503
  },
  {
    "epoch": 11,
    "miner": "Miner_Lazy (Random)",
    "score": 0.03435973836800002,
    "latency": 0.45599955391476843
  },
  {
    "epoch": 12,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9725122093056002,
    "latency": 0.35154369664265284
  },
  {
    "epoch": 12,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9725122093056002,
    "latency": 0.3065066657940856
  },
  {
    "epoch": 12,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.027487790694400018,
    "latency": 0.3003858490497625
  },
  {
    "epoch": 12,
    "miner": "Miner_Lazy (Random)",
    "score": 0.027487790694400018,
    "latency": 0.3788329503721992
  },
  {
    "epoch": 13,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9780097674444803,
    "latency": 0.2206317988769877
  },
  {
    "epoch": 13,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9780097674444803,
    "latency": 0.22989815868243246
  },
  {
    "epoch": 13,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.021990232555520017,
    "latency": 0.12718786633265966
  },
  {
    "epoch": 13,
    "miner": "Miner_Lazy (Random)",
    "score": 0.021990232555520017,
    "latency": 0.4671292434024441
  },
  {
    "epoch": 14,
    "miner": "Miner_Alphazero (Honest)",
    "score": 0.9824078139555843,
    "latency": 0.2403259208236556
  },
  {
    "epoch": 14,
    "miner": "Miner_GPT4 (Honest)",
    "score": 0.9824078139555843,
    "latency": 0.19853007674303635
  },
  {
    "epoch": 14,
    "miner": "Miner_Copycat (Malicious)",
    "score": 0.017592186044416015,
    "latency": 0.4391603429901778
  },
  {
    "epoch": 14,
    "miner": "Miner_Lazy (Random)",
    "score": 0.017592186044416015,
    "latency": 0.1323444746860719
  }
]


================================================
FILE: PROPOSAL.md
================================================
# OpenArena (Subnet XX) - Subnet Design Proposal

## 1. Introduction: The Vision for Contamination-Free AI Evaluation

Modern AI has a massive **Goodhart's Law** problem: static benchmarks like GSM8K and MMLU are heavily contaminated because they leak into public training data. As frontier models get smarter, it becomes impossible to tell the difference between genuine reasoning intelligence and simple data retrieval. Who validates the validators?

OpenArena solves this crisis by introducing the world's first decentralized, dynamic, adversarial AI evaluation protocol on Bittensor. We evaluate AI models on freshly generated, unseen problems in real-time, utilizing **LiveBench**‚Äîthe only continuously updated, objective ground-truth benchmark that actively prevents contamination. By creating a competitive ecosystem where validators act as adversarial task generators and miners act as solvers, OpenArena establishes the gold standard for "Proof of Intelligence."

## 2. Incentive & Mechanism Design

**Emission and Reward Logic**
Yuma consensus with Generalization Score S = (Accuracy √ó Calibration) - Latency. Top miners get proportional emissions (no winner-takes-all, but strong top-heavy).

**Incentive Alignment**

- Miners: Real money for real generalization.
- Validators: Dividends for generating high-entropy LiveBench tasks.

**Mechanisms to Discourage Adversarial Behavior**

- No hard-coding (LiveBench private questions make it impossible).
- Commit-Reveal (cryptographic, already implemented).
- Flash challenges (<200ms).
- Brier score penalizes confident hallucinations.

**Proof of Intelligence**
Miners must generalize to brand-new LiveBench questions that have never been public. Objective ground truth, no LLM judges. This is the purest Proof of Intelligence on Bittensor.

**High-level algorithm**

```mermaid
sequenceDiagram
    participant Validator as Validator (LiveBench)
    participant Subnet as Bittensor Subnet
    participant Miner as Miner (Solver)

    Validator->>Validator: 1. Pull private LiveBench task
    Validator->>Subnet: 2. Broadcast Task & Timeout
    Subnet->>Miner: 3. Distribute Task

    Miner->>Miner: 4. Inference (Compute Solution)
    Miner->>Subnet: 5. COMMIT Hash(Solution + Secret)
    Note over Subnet: Reveal Window Opens
    Miner->>Subnet: 6. REVEAL (Solution + Secret)

    Subnet->>Validator: 7. Forward Plaintext
    Validator->>Validator: 8. Verify Hash & Score (Brier)
    Validator->>Subnet: 9. Set Weights
```

## 3. Miner Design

**Miner Tasks**
Solve fresh LiveBench tasks in a cryptographic commit ‚Üí reveal flow.

**Expected Input ‚Üí Output Format**
See `openarena/protocol.py` ‚Äî GeneralizationTask synapse.

**Performance Dimensions**
Accuracy (objective ground truth), Calibration (Brier score for confidence), Latency (tie-breaker for efficient architectures).

## 4. Validator Design

**Scoring and Evaluation Methodology**
Validators pull from the **private delayed** LiveBench-2026-01-08 questions ‚Äî mathematically impossible to contaminate. They broadcast the task, manage the commit-reveal windows, and score the revealed plaintext against the objective ground truth.

**Evaluation Cadence**
Every epoch (block-based).

**Validator Incentive Alignment**
Entropy penalty for reusing old questions + cross-validation against other validators to ensure consensus integrity.

## 5. Business Logic & Market Rationale

**The Problem**
Static benchmarks are dead due to data contamination and rapid saturation by frontier models. A true measure of intelligence requires dynamic, zero-shot evaluation.

**Competing Solutions**
LiveBench itself (centralized, no continuous incentive loop), other evaluation subnets (lack strong economic incentives for miners, no LiveBench integration), Kaggle Game Arena (new but not decentralized on Bittensor).

**Why Bittensor?**
Decentralized incentives align perfectly with Red-Teaming. TAO payments provide the ultimate financial motivation to break and improve models continuously.

**Path to Long-Term Adoption**
Enterprises (Anthropic, xAI, OpenAI) will inevitably pay TAO for massive private evaluation rounds to stress-test their models before deployment.

## 6. Go-To-Market Strategy

**Initial Target Users**
15M Kaggle data scientists via one-click `!pip install openarena-kaggle`.

**Distribution**
KaggleIngest dashboard + Twitter bot that actively "roasts" saturated models using real-time OpenArena data.

**Bootstrapping**

- First 100 miners get bonus TAO from the team wallet to jumpstart the network.
- $5k Kaggle bounty for the top OpenArena miners in month 1 to attract top-tier talent.



================================================
FILE: WHITE_PAPER.md
================================================
# OpenArena: The Decentralized Adversarial Evaluation Protocol

**"The Proof of Intelligence"**

> [!IMPORTANT]
> **Core Thesis**: Static benchmarks are dead. Intelligence is not the ability to memorize a fixed dataset; it is the ability to generalize to new, unseen distributions. OpenArena is a continuous, adversarial stress-test for AI models, turning evaluation into a verifiable digital commodity.

---

## 1. Introduction: The Crisis of Evaluation

Modern AI has a **Goodhart's Law** problem: "When a measure becomes a target, it ceases to be a good measure."

- **Contamination**: Public datasets (GSM8K, MMLU) leak into training data.
- **Saturation**: Top models score 90%+ on benchmarks but fail in production.
- **Trust**: Who validates the validator?

**OpenArena** solves this by creating a **Dynamic Adversarial Evaluation Game**.

- **Validators** generate _fresh_ tasks every epoch (synthetic reasoning, real-time data, code puzzles).
- **Miners** must solve these unseen tasks instantly.
- **Incentives** reward _generalization_ and _efficiency_, while punishing _memorization_ and _wrapping_.

### 1.1 Core Thesis: Proof of Intelligence

We define "Intelligence" not as knowledge retrieval, but as **Generalization Efficiency**:

> _The ability to solve novel, high-entropy tasks with minimum latency and compute._

This shift allows us to distinguish between a 100B parameter model that memorized the internet and a 7B parameter model that can actually _reason_.

---

## 2. Technical Architecture

### 2.1 The Flow of Intelligence

```mermaid
sequenceDiagram
    participant V as Validator (Game Master)
    participant M as Miner (Solver)
    participant C as Chain (Bittensor)

    Note over V, M: Epoch N Starts (Block 0-360)

    rect rgb(20, 20, 20)
        Note right of V: 1. Generate Dynamic Task<br/>(e.g. "Solve random math puzzle")
        V->>V: Hash(Task) + Encrypt(GroundTruth)
        V->>M: Broadcast Synapse (Task Only)
    end

    rect rgb(40, 40, 40)
        Note right of M: 2. Compute Solution<br/>(LLM Inference / Code Exec)
        M->>M: Hash(Answer + Salt)
        M->>C: Commit Hash (Prevents Front-running)
    end

    rect rgb(20, 20, 20)
        Note right of V: 3. Reveal Phase
        M->>V: Reveal Answer
        V->>V: Verify Hash matches Commit
        V->>V: Score(Answer, GroundTruth)
    end

    rect rgb(60, 20, 20)
        Note right of C: 4. Yuma Consensus
        V->>C: Set Weights (W_i)
        C->>M: Distribute TAO Rewards
    end
```

### 2.2 Component Roles

| Role          | Responsibility                                                                     | Incentive                                                                        |
| :------------ | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Miner**     | Solve arbitrary tasks (Text, Code, Math) with high accuracy and low latency.       | Maximizes Reward ($R$) by optimizing inference speed and model generalization.   |
| **Validator** | Generate high-entropy, non-repeatable tasks. Evaluate miner solutions objectively. | Maximizes Dividends ($D$) by attracting high-quality miners and staking support. |

---

## 3. Incentive Mechanism (The Math)

The core innovation is the **Generalization Score ($S$)**.

### 3.1 The Scoring Function

For a set of $N$ tasks in an epoch, a miner $i$'s score $S_i$ is:

$$ S*i = \underbrace{\alpha \cdot \frac{1}{N} \sum*{j=1}^{N} \text{Acc}(y\*{ij}, y^\*_{j})}*{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij}, \text{Acc}*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t_{ij})}\*{\text{Latency Penalty}} $$

Where:

- $\text{Acc}(y_{ij}, y^*_{j})$: Accuracy metric (0 or 1 for exact match, or Levenshtein/BLEU for text).
- $\text{Cal}$: **Calibration Score**. Rewards miners who are confident when correct and uncertain when wrong (using Brier Score or Log Loss).
- $\text{Lat}$: **Latency Penalty**. $e^{t_{ij} - T_{max}}$.

### 3.2 Yuma Consensus & Weight Setting

Validators normalize scores to a weight vector $W$:
$$ w*{i} = \frac{e^{S_i / \tau}}{\sum*{k} e^{S_k / \tau}} $$
*(Using Softmax with temperature $\tau$ to control competition intensity)\*.

---

## 4. Adversarial Hardening (How We Win)

### üõ°Ô∏è Challenge 1: Memorization / Lookup

- **Attack**: Miners cache answers from previous epochs.
- **Defense**: **High-Entropy Generation**.
  - _Math_: "Calculate $A \times B$" where $A, B$ are random 10-digit primes.
  - _Real-Time_: "Summarize this article published 5 minutes ago" (Validators pull from NewsAPI).
  - _Code_: "Write a Python function to sort this random list [4, 1, 9...]".

### üõ°Ô∏è Challenge 2: Front-Running / Copying

- **Attack**: Fast miner sees a smart miner's answer in the mempool and copies it.
- **Defense**: **Commit-Reveal Scheme**.
  1.  Miner submits `Hash(Answer + Salt)`.
  2.  After window closes, Miner submits `Answer + Salt`.
  3.  Validator verifies hash matches.

### üõ°Ô∏è Challenge 3: Validator Collusion

- **Attack**: Validator shares Ground Truth with a specific miner.
- **Defense**: **Cross-Validation**.
  - Multiple validators score the same miner.
  - If Validator A's scores diverge significantly from the consensus media (Yuma Consensus), Validator A loses V-Trust and dividends.

---

## 5. Token Economics (The OpenArena Flywheel)

### 5.1 The Formal Value Loop ($V$)

Let $F$ be the fee paid by an enterprise (e.g., Anthropic) to prioritize a specific evaluation dataset $D_{target}$.

$$ F*{distribution} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{validators} + 0.2 \cdot F*{miners} $$

1.  **Burn ($40\%$)**: Permanently removed from supply, creating deflationary pressure on $\tau$.
2.  **Validator Reward ($40\%$)**: Distributed to validators proportional to their stake ($S_v$) and their **Curator Score** ($C_v$).
3.  **Miner Reward ($20\%$)**: Distributed to miners who solve $D_{target}$ with the highest **Generalization Score** ($G_m$).

### 5.2 The Enterprise Demand Flywheel

As enterprises pay $F$ to access the network:

1.  **Demand** for TAO increases (to pay fees).
2.  **Supply** of TAO decreases (via Burn).
3.  **Validator Yield** increases (via Dividend).
4.  **Miner Competition** increases (via Reward).

This creates a self-reinforcing loop where **Utility** drives **Security** and **Valuation**.

This ensures that **Enterprise Demand** directly correlates with **Miner Profitability** and **Token Scarcity**.

---

## 6. Security Analysis (Adversarial Robustness)

### 6.1 Attack: Pre-Computation (The "Lookup Table")

- **Vector**: Miner pre-calculates answers to known datasets to simulate intelligence.
- **Mitigation**: **Cryptographic Entropy Protocol**.
  - Let $H_b$ be the block hash at height $t$.
  - Let $K_v$ be the Validator's VRF key.
  - The **Task Seed** $S_t$ is derived as:
    $$ S_t = \text{SHA256}(H_b \parallel K_v) $$
  - The **Task** $T_t$ is generated via a deterministic mutation function $f$:
    $$ T*t = f(S_t, \text{Template}*{grammar}) $$
  - **Result**: Since $H_b$ is not known until block $t$, pre-computing $T_t$ is mathematically impossible.

### 6.2 Attack: Validator Laziness (Low Entropy)

- **Vector**: A Validator reuses old tasks to save compute, degrading the network's measurement quality.
- **Mitigation**: **Entropy Penalty ($E_v$)**.
  - We measure the Kullback-Leibler (KL) divergence between task distributions at time $t$ and $t-1$:
    $$ E*v = D*{KL}(P*t \parallel P*{t-1}) $$
  - If $E_v < \epsilon_{threshold}$ (statistically indistinguishable from previous epoch), the Validator's weight-setting power $W_v$ is slashed:
    $$ W*v^{new} = W_v^{old} \cdot (1 - \text{Penalty}*{lazy}) $$
- **Incentive**: **Difficulty Rating ($D_t$)**.
  - Validators are rewarded for generating tasks that separate miner performance.
  - If all miners score 100%, $D_t$ is low -> Validator reward reduced.
  - If no miner scores > 0%, $D_t$ is too high -> Validator reward reduced.
  - Optimal $D_t$ targets a Gaussian distribution of miner scores.

### 6.3 Attack: Front-Running (The "Copycat")

- **Mitigation**: **Commit-Reveal** (as defined in Section 4).
  - $t_0$: Miner submits $H = \text{SHA256}(Answer + Salt)$.
  - $t_1$: Reveal window opens.
  - Copycats only see hash $H$, preventing answer theft.

---

## 7. Go-To-Market & Integration (KaggleIngest)

We leverage **KaggleIngest** to visualize this war zone.

- **Leaderboard**: Real-time display of Miner Generalization Scores.
- **Museum**: Archive of "Hardest Tasks" (a valuable dataset).

---

## 6. Execution Roadmap (Round II Strategy)

### Phase 1: The "Stub" (Days 1-5)

- [ ] Implement `neurons/validator.py`: Basic task generation (Math/Logic).
- [ ] Implement `neurons/miner.py`: Basic OpenAI/Llama wrapper.
- [ ] Implement `commt-reveal` mechanism on-chain (using mock chain).

### Phase 2: The "Arena" (Days 6-12)

- [ ] Connect KaggleIngest frontend to Subnet stats.
- [ ] Deploy 5 Miner nodes (simulated) to show competition.
- [ ] Create visualization of "Score Drift" over time.



================================================
FILE: winning-strategy.md.resolved
================================================
# How to Win the Bittensor Subnet Ideathon: OpenArena Strategy

> [!IMPORTANT]
> **Pivot Alert**: We have shifted from "Green AI" to **OpenArena: The Decentralized Adversarial Evaluation Protocol**. This leverages your Kaggle background to solve AI's "Benchmark Saturation" crisis.

## üöÄ The Winning Concept: OpenArena

**Tagline**: *The World‚Äôs First Decentralized, Adversarial AI Evaluation Protocol.*

### 1. The Problem Space
- **Benchmark Saturation**: GPT-4o effectively "memorizes" static datasets like GSM8K. We can no longer distinguish *intelligence* from *retrieval*.
- **Data Contamination**: Public evaluation data leaks into training sets.
- **Trust**: Who validates the validator?

### 2. The Solution: Dynamic Adversarial Evaluation
OpenArena is a subnet where:
1.  **Validators** act as "Game Masters," generating *fresh* tasks every epoch (math puzzles, real-time news summarization, code challenges).
2.  **Miners** act as "Solvers," competing to generalize to these unseen tasks instantly.
3.  **Incentives** reward **Generalization** (adaptive intelligence) and penalize **Memorization** (overfitting).

### 3. Mechanism Design (The "Heart")
Our core innovation is the **Generalization Score ($S$)**:

$$ S_i = \underbrace{\alpha \cdot \text{Accuracy}}_{\text{Correctness}} \times \underbrace{\beta \cdot \text{Calibration}}_{\text{Confidence}} - \underbrace{\gamma \cdot \text{Latency}}_{\text{Speed}} $$

- **Anti-Gaming**: We use a **Commit-Reveal** scheme to prevent miners from copying answers from the mempool.
- **Validator Consensus**: Yuma Consensus ensures no single validator controls the ground truth.

---

## üèóÔ∏è Technical Architecture

### High-Level Flow
```mermaid
graph TD
    Val[Validator: Task Generator] -->|Broadcast Synapse| Miner[Miner Pool]
    Miner -->|Commit Hash| Chain[Bittensor Chain]
    Miner -->|Reveal Solution| Val
    Val -->|Score & Set Weights| Chain
    Val -.->|Cross-Check| OtherVals[Other Validators]
```

---

## üìÑ Submission Components (Round I)

### 1. The Whitepaper (PDF)
- **Thesis**: "Proof of Intelligence is the ability to generalize, not memorize."
- **Math**: Detailed incentive function covering accuracy, calibration, and latency.
- **Adversarial Analysis**: How we prevent Sybil attacks and collusion.

### 2. The Pitch Deck (Business Case)
- **Market**: "The $10B Data Labeling & Evaluation Market."
- **Narrative**: OpenArena creates the "Gold Standard" for AI capability tracking.
- **Integration**: Leverage **KaggleIngest** as the frontend for real-time visualization.

### 3. The Video (5 Minutes)
- **Show**: Validator generating a math puzzle -> Miner solving it -> Score updating on-chain.
- **Tell**: "We are measuring *real* intelligence, for the first time."

---

## üõ†Ô∏è Execution Strategy (Round II)
- **Phase 1**: Build a "Stub" subnet with simulated miners.
- **Phase 2**: Implement the Commit-Reveal mechanism on a local testnet.
- **Phase 3**: Connect KaggleIngest to show live "Generalization Leaderboards."



================================================
FILE: deliverables/pitch_deck.md
================================================
# OpenArena Pitch Deck (10 Slides)

## Slide 1: Title Slide

**Headline:** OpenArena (Subnet)
**Sub-headline:** The Decentralized Adversarial AI Evaluation Protocol
**Visual:** OpenArena Neo-Brutalist Logo, Bittensor Logo
**Presenter:** [Your Name / Team Name]
**Tagline:** Moving beyond benchmark saturation to measure real intelligence.

---

## Slide 2: The Problem Context

**Headline:** AI Benchmarks are Saturated
**Bullet Points:**

- **Memorization vs. Generalization:** Frontier models easily "solve" static datasets (like GSM8K or MMLU), making it impossible to tell if they are thinking or just retrieving.
- **Data Contamination:** Public evaluation data inevitably leaks into training sets. A model scoring 99% today might just have the test key.
- **The Bottleneck:** We cannot improve AI capability unless we have a reliable, un-gameable, continuous way to measure it.

---

## Slide 3: The OpenArena Solution

**Headline:** Dynamic Adversarial Evaluation
**Bullet Points:**

- **Dynamic Puzzles:** Evaluating AI on freshly generated, unseen tasks, rather than static datasets.
- **Decentralized Verification:** A competitive subnet where validators act as adversarial task-generators.
- **True Measurement:** We replace static benchmarking with real-time, zero-shot Generalization scoring.

---

## Slide 4: Subnet Overview & Value Prop

**Headline:** What OpenArena Does & Why It Matters
**Text:** OpenArena is a Bittensor subnet where AI models compete in real-time to solve novel, non-static challenges.
**Who is it for?**

- **AI Builders/Researchers:** Needing reliable, contamination-free capability tracking.
- **Enterprises:** Requiring absolute proof of a model's logical reasoning before deployment.
  **Why it matters:** It establishes the gold standard for "Proof of Intelligence."

---

## Slide 5: Core Mechanism & Roles

**Headline:** The OpenArena Ecosystem
**Visual:** High-level Architect Flow (Mermaid Diagram style)
**Roles:**

- **Validators (Game Masters):** Draw from the **LiveBench** dataset‚Äîa continuously updating stream of verifiable, objective ground-truth questions. Because LiveBench regularly releases fresh, uncontaminated tasks across coding, math, and data analysis, Validators guarantee miners are evaluated on novel problems.
- **Miners (Solvers):** Operate LLMs and proprietary reasoning agents competing to solve the Validators' tasks first.
  **Task Flow:** Assign ‚Üí Commit Solution ‚Üí Reveal Solution ‚Üí Validate ‚Üí Score & Reward.

---

## Slide 6: Incentives & Scoring Formula

**Headline:** The Generalization Score
**Text:** We incentivize intelligence, not just speed or rote recall.
**The Formula:**
`Score = (Accuracy √ó Œ±) √ó (Calibration √ó Œ≤) - (Latency √ó Œ≥)`

- **Accuracy (Correctness):** Did the model get the objectively correct answer? (Highest weight).
- **Calibration (Confidence):** Did the model properly assess its own certainty?
- **Latency (Speed):** A tiebreaker designed to reward efficient reasoning architectures.
  _Miners who generalize best across diverse, unseen tasks capture the majority of TAO emissions._

---

## Slide 7: Quality Control & Anti-Cheating

**Headline:** Bulletproof Adversarial Robustness
**Bullet Points:**

- **Cryptographic Commit-Reveal Scheme:** Miners must submit a hash of their answer before revealing the plaintext. This mathematically prevents "lazy" miners from copying answers from the mempool.
- **Procedural Freshness:** Tasks are guaranteed unique per epoch. Overfitting is impossible.
- **Yuma Consensus:** Validator scores are aggregated and cross-checked on-chain, preventing rogue validators from favoring specific miners.
- **Spam Sybil Defenses:** Strict timeouts and response format requirements auto-reject low-effort or automated noise.

---

## Slide 8: "Proof of Intelligence"

**Headline:** Why This is True Proof of Intelligence
**Text:** OpenArena doesn't just measure compute ("Proof of Work") or stake ("Proof of Stake").
**The Argument:**

- By evaluating miners against the **LiveBench** dataset‚Äîwhere tasks are verified, updated continuously, and kept out of public training data‚Äîminers cannot rely on pre-trained memory.
- Success here strictly correlates with advanced reasoning, logical deduction, and zero-shot capability.
- The _effort_ is the computational reasoning required; the _intelligence_ is the generalization.

---

## Slide 9: Go-To-Market & Ecosystem Integration

**Headline:** KaggleIngest as the Ultimate Frontend
**Bullet Points:**

- **The Integration:** We are integrating OpenArena with **KaggleIngest**, providing a massive, existing user base of data scientists and ML engineers a window into the Bittensor ecosystem.
- **Real-Time Leaderboard:** A beautiful, neo-brutalist public dashboard tracking the smartest open-source models dynamically.
- **Flywheel Effect:** Better evaluation brings better models -> better models bring enterprise demand -> enterprise demand drives TAO utility.

---

## Slide 10: Implementation Plan (Round II Hackathon)

**Headline:** Milestones to Mainnet
**Bullet Points:**

- **Phase 1: The "Stub" Subnet (Testnet MVP):** Implement the core Validator-Miner loop with fundamental math/logic generation.
- **Phase 2: Commit-Reveal Integration:** Deploy the cryptographic anti-collusion layer on the testnet.
- **Phase 3: Dashboard Release:** Launch the live KaggleIngest-integrated frontend connecting directly to the Testnet subnet state.
  **Wrap up:** "Let's build the arena where true AGI is forged."



================================================
FILE: deliverables/social_thread.md
================================================
# OpenArena X/Twitter Thread

**Post 1**
AI evaluation is broken. üìâ
Benchmarks like GSM8K and MMLU are saturated. Models are memorizing, not thinking.
We can no longer distinguish between a 100B param parrot and true intelligence.
It‚Äôs time to kill the static test set.
Introducing **OpenArena**: The Decentralized Adversarial Evaluation Protocol on @bittensor\_. üßµüëá

**Post 2**
Technical creativity > Memorization.
In OpenArena, Validators don't just grade tests‚Äîthey _generate_ them.
Fresh, high-entropy tasks every epoch.

- Synthetic Logic Puzzles
- Real-time News Summarization
- Code Generation for novel problems
  If your model can't generalize instantly, it scores zero. #ProofOfIntelligence

**Post 3**
üö´ The Cheating Problem.
Public leaderboards are plagued by "borderline" models that overfit to the test set.
OpenArena solves this with **Adversarial Hardening**:

1. **Dynamic Tasks**: Impossible to pre-train on.
2. **Commit-Reveal**: Cryptographically prevents front-running.
3. **Brier Scoring**: Ruthlessly penalizes "confident hallucinations."

**Post 4**
üöÄ The Unfair Advantage: **KaggleIngest**.
We aren't building an island. We're building a bridge.
OpenArena integrates exclusively with our **KaggleIngest** platform, allowing 15M+ Kaggle data scientists to deploy miners with ONE CLICK.
Web2 Talent ü§ù Web3 Incentives.

**Post 5**
The vision: **Evaluation-as-a-Service**.
Companies like Anthropic or xAI shouldn't grade their own homework.
In the future, they will pay the OpenArena subnet to "Red Team" their models against a global swarm of adversarial validators.
Trustless. Decentralized. Brutally honest.

**Post 6**
We are submitting OpenArena to the @bittensor\_ Ideathon.
Because the world needs a "Truth Machine" for AI Intelligence.
Read the Whitepaper & check the git: [Link]
Let the games begin. ‚öîÔ∏è
#Bittensor #AI #DeAI #MachineLearning #OpenSource



================================================
FILE: deliverables/twitter_thread.md
================================================
# OpenArena Twitter/X Announcement Thread (Subnet Ideathon Submission)

**Tweet 1:**
1/ ü§ñ AI benchmarks are saturated. When a model scores 99% on MMLU or GSM8K, is it actually reasoning, or is it just memorizing the test set? We can no longer tell the difference.
Today we are introducing OpenArena: The decentralized, adversarial AI evaluation protocol on @bittensor\_. üßµüëá

**Tweet 2:**
2/ ‚öîÔ∏è OpenArena replaces static benchmarks with dynamic, real-time adversarial evaluation.
Validators act as Game Masters, generating complex, unseen tasks every epoch.
Miners act as Solvers, competing to verify their zero-shot reasoning capabilities instantly.
No training. No memorization. Just intelligence.

**Tweet 3:**
3/ üß† The "Proof of Intelligence"
To earn $TAO emissions, miners must maximize their Generalization Score:
Score = (Accuracy √ó Œ±) √ó (Calibration √ó Œ≤) - (Latency √ó Œ≥)
If a model overfits, it fails. If a miner scrapes the mempool, they are caught by our Commit-Reveal scheme.

**Tweet 4:**
4/ üöÄ We are building the gold standard for capability tracking.
By integrating OpenArena with the massive KaggleIngest data science community, we are bridging cutting-edge, trustless AI evaluation directly to the enterprise edge.
Read the whitepaper: [Link to your deployed site/whitepaper]

**Tweet 5:**
5/ üèÜ We are thrilled to submit OpenArena for Round I of the Bittensor Subnet Ideathon!
A huge thank you to the @opentensor team and the community.
The journey to true AGI starts in the arena.
#Bittensor #AI #MachineLearning #Hackathon



================================================
FILE: deliverables/video_script.md
================================================
# OpenArena Ideathon Video Script (3-7 Minutes)

**Format:** Screen Recording + Voice-Over (Showing the OpenArena Next.js Website, Whitepaper, and KaggleIngest Integration)
**Tone:** Confident, visionary, clear, and focused on the technical architecture.

---

### [0:00 - 0:45] 1. The Hook & Subnet Overview

_(Visual: Start on the OpenArena Homepage hero section showing the automated terminal animation.)_

**Voice-Over:**
"Hello everyone, and welcome to OpenArena: Subnet X.
Right now, the AI industry is facing a massive problem: Benchmark Saturation. As frontier models get smarter, they memorize static datasets like MMLU or HumanEval. It is becoming impossible to tell the difference between genuine reasoning intelligence, and simple data retrieval. Who validates the validators?

OpenArena solves this. We are the world's first decentralized, dynamic, adversarial AI evaluation protocol on Bittensor. Instead of static benchmarks, we evaluate AI on freshly generated, unseen problems in real-time."

### [0:45 - 1:45] 2. Core Mechanism & Roles

_(Visual: Scroll down to the "Mechanism Design" section on the homepage and hover over the interactive Mermaid architecture diagram.)_

**Voice-Over:**
"Here is how our subnet operates. The core mechanism involves two distinct roles: Validators and Miners.

Validators act as 'Game Masters'. In every epoch, they pull from **LiveBench**‚Äîa continuously updating benchmark of verifiable, objective ground-truth questions. Because LiveBench regularly releases fresh, harder tasks across coding, math, and data analysis‚Äîwhile keeping newer questions completely private from public datasets‚ÄîValidators guarantee zero data contamination.

Miners act as 'Solvers'. They run their proprietary language models and autonomous agents, competing to solve these verifiable tasks before anyone else."

### [1:45 - 2:45] 3. Incentives, Scoring & Reward Allocation

_(Visual: Switch tabs to the Whitepaper page, specifically highlighting the "Generalization Score" Math formula that renders on the page.)_

**Voice-Over:**
"To ensure we are rewarding the right behavior, we engineered a specific incentive mechanism built around the 'Generalization Score'.

This score is a function of Accuracy, Calibration, and Latency.
First and foremost, accuracy is king. Did the miner produce the objectively correct solution?
Second, calibration: did the miner correctly assess its own confidence level?
And finally, latency serves as a tiebreaker for efficiency.

The most capable models‚Äîthe ones that generalize the best to completely unseen problems‚Äîcapture the highest scores and earn the TAO emissions. This creates an evolutionary pressure for miners to build genuinely smarter reasoning architectures, not just faster retrieval systems."

### [2:45 - 3:45] 4. Quality Control & Anti-Cheating

_(Visual: Scroll down in the whitepaper to the 'Security and Anti-Gaming' section, highlighting the Commit-Reveal bullet points.)_

**Voice-Over:**
"To make this 'Proof of Intelligence' mathematically secure, we implement strict anti-cheating mechanisms.

The biggest vulnerability in peer-to-peer solving is copying. To prevent 'lazy' miners from simply scraping the mempool for correct answers published by smarter competitors, we use a cryptographic Commit-Reveal scheme. Miners must commit a hash of their solution before revealing the plaintext payload.

Coupled with strict timeouts and procedural generation that prevents overfitting, OpenArena ensures that only genuine computational effort and actual intelligence are rewarded."

### [3:45 - 4:45] 5. Go-To-Market & The Implementation Plan

_(Visual: Switch to a preview of the 10-page Pitch Deck or the deployed KaggleIngest integration showcasing a live leaderboard mockup.)_

**Voice-Over:**
"What makes OpenArena a viable business? We are integrating this protocol directly with our existing platform, KaggleIngest. By exposing the real-time OpenArena 'Generalization Leaderboard' to thousands of data scientists, we create immediate, overwhelming demand for this zero-shot evaluation standard.

Our Implementation Plan for Round II is straightforward:
In Phase 1, we will launch our 'Stub' Subnet on the Testnet with simulated miners.
In Phase 2, we will harden the Commit-Reveal mechanism across the network.
In Phase 3, we will connect the live decentralized consensus directly to our KaggleIngest frontend dashboard.

Thank you for watching. OpenArena isn't just a benchmark‚Äîit's the crucible where true intelligence is proven. We look forward to seeing you on the testnet."



================================================
FILE: neurons/miner.py
================================================
import time
import typing
import bittensor as bt
from openarena.protocol import GeneralizationTask
from openarena.utils.crypto import hash_commitment, generate_salt
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class Miner:
    def __init__(self, config=None):
        self.config = config or self.get_config()
        self.wallet = bt.wallet(config=self.config)
        self.subtensor = bt.subtensor(config=self.config)
        self.metagraph = self.subtensor.metagraph(netuid=self.config.netuid)
        self.axon = bt.axon(wallet=self.wallet, config=self.config)

        # Local storage for commit-reveal (In-memory for MVP)
        # Key: query_hash, Value: (answer, salt)
        self.commitments: typing.Dict[str, typing.Tuple[str, str]] = {}

        # Load Model (Optimized for 4-bit/8-bit if available, here standard for compatibility)
        bt.logging.info("Loading LLM model...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # Using a lightweight model for demonstration/functionality; replaceable with larger models.
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.model = AutoModelForCausalLM.from_pretrained("gpt2").to(self.device)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        bt.logging.info(f"Miner initialized with wallet: {self.wallet} on device: {self.device}")

    def get_config(self):
        parser = bt.ArgumentParser()
        parser.add_argument('--netuid', type=int, default=1, help='The chain subnet uid.')
        bt.axon.add_args(parser)
        bt.subtensor.add_args(parser)
        bt.wallet.add_args(parser)
        return bt.config(parser)

    def forward(self, synapse: GeneralizationTask) -> GeneralizationTask:
        """
        The Miner's forward function.
        Handles both 'commit' and 'reveal' phases.
        """
        bt.logging.info(f"Received synapse from: {synapse.dendrite.hotkey} | Phase: {synapse.phase}")

        if synapse.phase == "commit":
            # 1. Generate Answer (Real LLM Inference)
            try:
                inputs = self.tokenizer(synapse.query, return_tensors="pt").to(self.device)
                outputs = self.model.generate(**inputs, max_new_tokens=50, pad_token_id=self.tokenizer.eos_token_id)
                answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            except Exception as e:
                bt.logging.error(f"Inference failed: {e}")
                answer = "Error during inference"

            # 2. Generate Salt
            salt = generate_salt()

            # 3. Store for later reveal
            # We use the query as the key. In production, use a more unique ID.
            self.commitments[synapse.query] = (answer, salt)

            # 4. Compute Commitment
            commitment = hash_commitment(answer, salt, self.wallet.hotkey.ss58_address)

            synapse.commitment = commitment
            bt.logging.info(f"Committed: {commitment}")

        elif synapse.phase == "reveal":
            # 1. Retrieve stored answer/salt
            if synapse.query in self.commitments:
                answer, salt = self.commitments[synapse.query]
                synapse.answer = answer
                synapse.salt = salt
                bt.logging.info(f"Revealed: {answer} (Salt: {salt})")

                # Cleanup (Optional: keep for a bit?)
                del self.commitments[synapse.query]
            else:
                bt.logging.error(f"No commitment found for query: {synapse.query}")

        return synapse

    def run(self):
        # Attach the forward function to the axon
        self.axon.attach(
            forward_fn=self.forward,
            blacklist_fn=self.blacklist,
            priority_fn=self.priority,
        )

        # Serve the axon
        bt.logging.info(f"Serving axon on port {self.config.axon.port}")
        self.axon.serve(netuid=self.config.netuid, subtensor=self.subtensor)

        # Start the axon
        bt.logging.info(f"Starting axon...")
        self.axon.start()

        # Keep the miner running
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            self.axon.stop()
            bt.logging.info("Miner stopped.")

    def blacklist(self, synapse: GeneralizationTask) -> typing.Tuple[bool, str]:
        # Implement blacklist logic (e.g., allow specific hotkeys)
        return False, "Allowed"

    def priority(self, synapse: GeneralizationTask) -> float:
        # Implement priority logic (e.g., stake-based)
        return 0.0

if __name__ == "__main__":
    miner = Miner()
    miner.run()



================================================
FILE: neurons/validator.py
================================================
import bittensor as bt
import time
import hashlib
from openarena.protocol import GeneralizationTask
from openarena.utils.crypto import verify_commitment

class Validator:
    def __init__(self, config=None):
        self.config = config or self.get_config()
        self.wallet = bt.wallet(config=self.config)
        self.subtensor = bt.subtensor(config=self.config)
        self.dendrite = bt.dendrite(wallet=self.wallet)
        self.metagraph = self.subtensor.metagraph(netuid=self.config.netuid)

        bt.logging.info(f"Validator initialized with wallet: {self.wallet}")

    def get_config(self):
        parser = bt.ArgumentParser()
        parser.add_argument('--netuid', type=int, default=1, help='The chain subnet uid.')
        bt.dendrite.add_args(parser)
        bt.subtensor.add_args(parser)
        bt.wallet.add_args(parser)
        return bt.config(parser)

    def run(self):
        bt.logging.info("Starting validator loop...")

        while True:
            try:
                # 1. Generate Task (Entropy Protocol)
                # VRF: Hash(BlockHeight + ValidatorKey + PrevBlockHash)
                block_height = self.subtensor.get_current_block()
                vrf_key = str(self.wallet.hotkey.ss58_address)
                # Fetch actual previous block hash for on-chain entropy
                prev_block_hash = self.subtensor.get_block_hash(block_height - 1)

                # Formal Entropy Seed Derivation
                entropy_seed = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()

                bt.logging.info(f"Generated Entropy Seed: {entropy_seed}")
                bt.logging.info(f"Derivation: SHA256({block_height} + {vrf_key[:8]}... + {prev_block_hash[:8]}...)")

                # In production, this seed drives the Task Generator.
                # For this stub, we use a simple string reversal, but the seed is ready.
                query = f"Task_{step}_{entropy_seed[:8]}: Reverse this string"
                bt.logging.info(f"\n--- Step {step}: {query} ---")

                # Filter miners (mock: select top 10 or random)
                # For this stub, we just query all available axons in the metagraph
                # But to avoid timeout on large nets, let's pick 2 for testing or assume local execution.
                # If running locally with miner, we need to know miner's IP/Port.
                # Standard pattern: Query metagraph.axons
                miner_axons = self.metagraph.axons
                if not miner_axons:
                    bt.logging.warning("No miners found in metagraph. Waiting...")
                    time.sleep(10)
                    continue

                # 2. Commit Phase
                bt.logging.info("Phase 1: COMMIT")
                responses_commit = self.dendrite.query(
                    miner_axons,
                    GeneralizationTask(query=query, phase="commit"),
                    deserialize=True,
                    timeout=5
                )

                # Store commitments
                # Key: miner_hotkey, Value: commitment_hash
                commitments = {}
                for axon, synapse in zip(miner_axons, responses_commit):
                    if synapse.commitment:
                        bt.logging.info(f"Miner {axon.hotkey} committed: {synapse.commitment}")
                        commitments[axon.hotkey] = synapse.commitment
                    else:
                        bt.logging.warning(f"Miner {axon.hotkey} failed to commit.")

                # 3. Wait (Reveal Window)
                bt.logging.info("Waiting for reveal window (simulated 2s)...")
                time.sleep(2)

                # 4. Reveal Phase
                bt.logging.info("Phase 2: REVEAL")
                responses_reveal = self.dendrite.query(
                    miner_axons,
                    GeneralizationTask(query=query, phase="reveal"),
                    deserialize=True,
                    timeout=5
                )

                # 5. Verify and Score
                scores = []
                for axon, synapse in zip(miner_axons, responses_reveal):
                    hotkey = axon.hotkey
                    if hotkey not in commitments:
                        bt.logging.info(f"Miner {hotkey} ignored (no commitment).")
                        continue

                    commitment = commitments[hotkey]
                    answer = synapse.answer
                    salt = synapse.salt

                    if not answer or not salt:
                        bt.logging.info(f"Miner {hotkey} failed to reveal.")
                        continue

                    # Verify Commitment
                    is_valid = verify_commitment(commitment, answer, salt, hotkey)
                    if is_valid:
                        bt.logging.success(f"Miner {hotkey} VERIFIED! Answer: {answer}")
                        # Score: Check if answer is correct (reverse string)
                        # Mock Logic:
                        expected = query[::-1]
                        if answer == expected:
                            bt.logging.success(f"Miner {hotkey} CORRECT!")
                        else:
                            bt.logging.info(f"Miner {hotkey} WRONG Answer.")
                    else:
                        bt.logging.error(f"Miner {hotkey} CHEATING attempt! Hash mismatch.")

                time.sleep(12) # Wait for roughly one block time

            except KeyboardInterrupt:
                break
            except Exception as e:
                bt.logging.error(f"Error in validator loop: {e}")
                time.sleep(5)

if __name__ == "__main__":
    validator = Validator()
    validator.run()



================================================
FILE: openarena/protocol.py
================================================
import bittensor as bt
from typing import Optional, List, Literal
import pydantic

class GeneralizationTask(bt.Synapse):
    """
    The GeneralizationTask Synapse.

    Validators send this to Miners with a 'query' and a 'phase'.
    Miners respond based on the phase:
    - 'commit': Return 'commitment' (hash).
    - 'reveal': Return 'answer' and 'salt'.
    """

    # The Challenge
    query: str = pydantic.Field(
        ...,
        title="Query",
        description="The task to be solved."
    )

    phase: Literal["commit", "reveal"] = pydantic.Field(
        "commit",
        title="Phase",
        description="The phase of the protocol: 'commit' or 'reveal'."
    )

    # The Commit Phase
    commitment: Optional[str] = pydantic.Field(
        None,
        title="Commitment",
        description="SHA256(answer + salt + miner_hotkey). Sent in 'commit' phase."
    )

    # The Reveal Phase
    answer: Optional[str] = pydantic.Field(
        None,
        title="Answer",
        description="The actual solution. Sent in 'reveal' phase."
    )

    salt: Optional[str] = pydantic.Field(
        None,
        title="Salt",
        description="A random salt string. Sent in 'reveal' phase."
    )

    required_hash_fields: List[str] = pydantic.Field(
        ["query", "phase"],
        title="Required Hash Fields",
        description="Fields required to compute the hash of this Synapse."
    )



================================================
FILE: openarena/frontend/README.md
================================================
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.



================================================
FILE: openarena/frontend/eslint.config.mjs
================================================
import { defineConfig, globalIgnores } from "eslint/config";
import nextVitals from "eslint-config-next/core-web-vitals";
import nextTs from "eslint-config-next/typescript";

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    ".next/**",
    "out/**",
    "build/**",
    "next-env.d.ts",
  ]),
]);

export default eslintConfig;



================================================
FILE: openarena/frontend/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;



================================================
FILE: openarena/frontend/package.json
================================================
{
  "name": "frontend",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint"
  },
  "dependencies": {
    "clsx": "^2.1.1",
    "framer-motion": "^12.34.3",
    "katex": "^0.16.33",
    "lucide-react": "^0.575.0",
    "mermaid": "^11.12.3",
    "next": "16.1.6",
    "react": "19.2.3",
    "react-dom": "19.2.3",
    "react-markdown": "^10.1.0",
    "rehype-katex": "^7.0.1",
    "rehype-raw": "^7.0.0",
    "remark-gfm": "^4.0.1",
    "remark-math": "^6.0.0",
    "tailwind-merge": "^3.5.0"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "16.1.6",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}



================================================
FILE: openarena/frontend/postcss.config.mjs
================================================
const config = {
  plugins: {
    "@tailwindcss/postcss": {},
  },
};

export default config;



================================================
FILE: openarena/frontend/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts",
    "**/*.mts"
  ],
  "exclude": ["node_modules"]
}



================================================
FILE: openarena/frontend/src/app/globals.css
================================================
@import "tailwindcss";

@theme {
  --color-brand-yellow: #fde047;
  --color-brand-red: #ef4444;
  --color-brand-blue: #3b82f6;
  --color-brand-green: #22c55e;
  --color-brand-light: #fbfbf8;
  --color-brand-dark: #121212;
}

@layer base {
  :root {
    --background: #fbfbf8;
    --foreground: #121212;
  }

  body {
    background-color: var(--background);
    color: var(--foreground);
    background-image: radial-gradient(#d1d5db 1px, transparent 1px);
    background-size: 20px 20px;
    margin: 0;
    padding: 0;
  }
}

@layer utilities {
  .brutal-border {
    border: 3px solid #121212;
  }

  .brutal-shadow {
    box-shadow: 6px 6px 0px #121212;
  }

  .brutal-shadow-hover {
    transition: all 0.15s ease-in-out;
  }

  .brutal-shadow-hover:hover {
    transform: translate(-3px, -3px);
    box-shadow: 9px 9px 0px #121212;
  }

  .brutal-shadow-hover:active {
    transform: translate(4px, 4px);
    box-shadow: 2px 2px 0px #121212;
  }

  .brutal-card {
    background-color: white;
    border: 3px solid #121212;
    box-shadow: 6px 6px 0px #121212;
    border-radius: 0;
  }
}



================================================
FILE: openarena/frontend/src/app/layout.tsx
================================================
import type { Metadata } from 'next';
import { Inter, Space_Mono } from 'next/font/google';
import './globals.css';

const inter = Inter({ subsets: ['latin'], variable: '--font-inter' });
const spaceMono = Space_Mono({
  weight: ['400', '700'],
  subsets: ['latin'],
  variable: '--font-space-mono'
});

export const metadata: Metadata = {
  title: 'OpenArena | The Trust Machine for AI',
  description: 'The First Decentralized Adversarial Evaluation Protocol on Bittensor.',
};

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <body className={`${inter.variable} ${spaceMono.variable} font-sans antialiased text-brand-dark min-h-screen`}>
        {children}
      </body>
    </html>
  );
}



================================================
FILE: openarena/frontend/src/app/page.tsx
================================================
'use client';

import React, { useState, useEffect } from 'react';
import { Play, Terminal, Database, ShieldAlert, Cpu, CheckCircle2, Users, Flame } from 'lucide-react';
import { motion, AnimatePresence } from 'framer-motion';
import Link from 'next/link';
import Mermaid from '../components/Mermaid';

export default function Home() {
  const [activeTab, setActiveTab] = useState('live');
  const [logs, setLogs] = useState<string[]>([]);
  const [isSimulating, setIsSimulating] = useState(true);

  // Simulation effect for the hero terminal
  useEffect(() => {
    if (!isSimulating) return;

    const initialLogs = [
      "// Epoch 4829 - Subnet 99 (OpenArena)",
      "> Awaiting Validator Task Broadcast...",
    ];

    setLogs(initialLogs);

    const sequence = [
      { text: "> [TASK RECEIVED] Source: LiveBench API | Category: Reasoning", delay: 1500, color: "text-brand-yellow" },
      { text: "> Committing solution hash to chain...", delay: 2500, color: "" },
      { text: "> Hash: 0x8f2a9...d31e", delay: 3000, color: "" },
      { text: "// Wait for Reveal Window...", delay: 4500, color: "text-gray-500" },
      { text: "> Revealing plain-text solution...", delay: 5500, color: "" },
      { text: "> [SUCCESS] Validator confirmed structure.", delay: 6500, color: "text-brand-green" },
    ];

    const timeouts: NodeJS.Timeout[] = [];

    sequence.forEach((step) => {
      const timeout = setTimeout(() => {
        setLogs(prev => [...prev, `<span class="${step.color}">${step.text}</span>`]);
      }, step.delay);
      timeouts.push(timeout);
    });

    const resetTimeout = setTimeout(() => {
      setLogs([]);
      setIsSimulating(false);
      setTimeout(() => setIsSimulating(true), 100);
    }, 9000);

    timeouts.push(resetTimeout);

    return () => timeouts.forEach(clearTimeout);
  }, [isSimulating]);

  const architectureDiagram = `
    graph TD
      classDef val fill:#fde047,stroke:#121212,stroke-width:4px,color:#121212,font-weight:bold
      classDef miner fill:#3b82f6,stroke:#121212,stroke-width:4px,color:#fff,font-weight:bold
      classDef chain fill:#ef4444,stroke:#121212,stroke-width:4px,color:#fff,font-weight:bold

      Val[Validator: Task Generator]:::val -->|Broadcast Synapse| Miner[Miner Pool]:::miner
      Miner -->|Commit Hash| Chain[Bittensor Chain]:::chain
      Miner -->|Reveal Solution| Val
      Val -->|Score & Set Weights| Chain
      Val -.->|Cross-Check| OtherVals[Other Validators]:::val
  `;

  return (
    <div className="min-h-screen pb-24 font-sans selection:bg-brand-yellow selection:text-black">
      {/* Navigation */}
      <nav className="border-b-4 border-black bg-white sticky top-0 z-50">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="flex justify-between h-16 items-center">
            <div className="flex items-center space-x-2">
              <div className="w-8 h-8 bg-brand-red border-2 border-black flex items-center justify-center brutal-shadow-hover">
                <Flame size={20} className="text-white" />
              </div>
              <span className="font-bold text-xl tracking-tight uppercase">OpenArena</span>
            </div>
            <div className="flex space-x-4">
              <Link href="https://kaggleingest.com" target="_blank" className="hidden md:flex font-bold px-4 py-2 border-2 border-black bg-brand-yellow brutal-shadow-hover hover:-translate-y-1 items-center justify-center">
                KaggleIngest Portal
              </Link>
              <Link href="/whitepaper" className="font-bold px-4 py-2 border-2 border-black bg-black text-white brutal-shadow-hover hover:-translate-y-1 flex items-center justify-center">
                Read Whitepaper
              </Link>
            </div>
          </div>
        </div>
      </nav>

      <main className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 mt-12 md:mt-24">
        {/* Hero Section */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-12 items-center mb-24">
          <motion.div
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            transition={{ duration: 0.5 }}
          >
            <div className="inline-block px-4 py-2 bg-brand-yellow border-4 border-black font-bold mb-6 brutal-shadow rotate-1 transform">
              #1 BITTENSOR IDEATHON SUBMISSION
            </div>
            <div className="mb-6 border-4 border-black p-4 bg-brand-blue text-white brutal-shadow transform -rotate-1">
               <p className="font-bold text-lg md:text-xl uppercase">OpenArena ‚Äî Live Bittensor Ideathon Submission</p>
               <p className="font-medium mt-2 text-md">Powered by <span className="font-black bg-brand-yellow text-black px-1 py-0.5">LiveBench-2026-01-08</span> (private delayed questions) + KaggleIngest</p>
            </div>
            <h1 className="text-6xl md:text-8xl font-black leading-none mb-6 tracking-tighter uppercase relative">
              The Truth <br />
              <span className="text-white text-shadow-solid relative inline-block">
                Machine
                <span className="absolute inset-0 text-white text-shadow-[4px_4px_0_#121212] -z-10">Machine</span>
              </span> <br />
              For AI.
            </h1>
            <p className="text-xl md:text-2xl font-medium mb-8 border-l-8 border-brand-red pl-6 py-2 bg-white brutual-border p-4 shadow-[4px_4px_0_#121212]">
              Static benchmarks are dead. Models memorize the test set. We built the first decentralized adversarial evaluation protocol to score <span className="font-bold underline decoration-brand-blue decoration-4">generalization</span>, not memorization.
            </p>
            <div className="flex flex-col sm:flex-row space-y-4 sm:space-y-0 sm:space-x-4">
              <Link href="#network" className="flex items-center justify-center space-x-2 font-black text-lg px-8 py-4 border-4 border-black bg-brand-blue text-white brutal-shadow brutal-shadow-hover">
                <Terminal size={24} />
                <span>View Live Network</span>
              </Link>
              <Link href="https://youtube.com" target="_blank" className="flex items-center justify-center space-x-2 font-black text-lg px-8 py-4 border-4 border-black bg-white brutal-shadow brutal-shadow-hover">
                <Play size={24} />
                <span>Watch Demo</span>
              </Link>
              <Link href="#network" className="flex items-center justify-center space-x-2 font-black text-lg px-8 py-4 border-4 border-black bg-brand-yellow text-black brutal-shadow brutal-shadow-hover">
                <Cpu size={24} />
                <span>Submit your miner</span>
              </Link>
            </div>
          </motion.div>

          {/* Hero Graphic / Code Window */}
          <motion.div
            initial={{ opacity: 0, scale: 0.95 }}
            animate={{ opacity: 1, scale: 1 }}
            transition={{ duration: 0.5, delay: 0.2 }}
            className="border-4 border-black bg-[#1E1E1E] brutal-shadow overflow-hidden flex flex-col h-[500px]"
          >
            <div className="bg-black text-white px-4 py-2 flex items-center justify-between border-b-4 border-black">
              <div className="flex space-x-2">
                <div className="w-3 h-3 rounded-full bg-brand-red border border-white"></div>
                <div className="w-3 h-3 rounded-full bg-brand-yellow border border-white"></div>
                <div className="w-3 h-3 rounded-full bg-brand-green border border-white"></div>
              </div>
              <span className="font-mono text-sm font-bold text-gray-400">mining_loop.rs</span>
            </div>
            <div className="p-6 font-mono text-sm md:text-md text-green-400 overflow-y-auto flex-1 h-full">
              <AnimatePresence>
                {logs.map((log, i) => (
                  <motion.p
                    key={i}
                    initial={{ opacity: 0, x: -10 }}
                    animate={{ opacity: 1, x: 0 }}
                    className="mb-2"
                    dangerouslySetInnerHTML={{ __html: log }}
                  />
                ))}
              </AnimatePresence>
              <p className="text-white animate-pulse mt-2">_</p>
            </div>
          </motion.div>
        </div>

        {/* The Protocol Section */}
        <div className="mb-24">
          <h2 className="text-4xl md:text-6xl font-black uppercase mb-12 text-center border-b-8 border-black pb-4 inline-block mx-auto">
            How The Protocol Works
          </h2>
          <div className="grid grid-cols-1 md:grid-cols-3 gap-8">
            <div className="bg-white p-8 border-4 border-black brutal-shadow brutal-shadow-hover">
              <div className="w-16 h-16 bg-brand-yellow border-4 border-black flex items-center justify-center mb-6 brutal-shadow transform -rotate-3">
                <Database size={32} />
              </div>
              <h3 className="text-2xl font-black uppercase mb-4">1. Livebench Task Generation</h3>
              <p className="text-lg font-medium">Validators act as &quot;Game Masters,&quot; pulling verified, contamination-free tasks from the <strong>LiveBench</strong> dataset every epoch. No static datasets. No memorization.</p>
            </div>
            <div className="bg-white p-8 border-4 border-black brutal-shadow brutal-shadow-hover">
              <div className="w-16 h-16 bg-brand-blue text-white border-4 border-black flex items-center justify-center mb-6 brutal-shadow transform rotate-3">
                <Cpu size={32} />
              </div>
              <h3 className="text-2xl font-black uppercase mb-4">2. Miner Inference Loop</h3>
              <p className="text-lg font-medium">Miners receive the prompt and must instantly generalize. We utilize a cryptographic Commit-Reveal scheme to prevent front-running.</p>
            </div>
            <div className="bg-white p-8 border-4 border-black brutal-shadow brutal-shadow-hover">
              <div className="w-16 h-16 bg-brand-green text-black border-4 border-black flex items-center justify-center mb-6 brutal-shadow">
                <ShieldAlert size={32} />
              </div>
              <h3 className="text-2xl font-black uppercase mb-4">3. Brier Scoring Consensus</h3>
              <p className="text-lg font-medium">Validators grade solutions using strict Brier Scores that penalize &quot;hallucination&quot; and heavily reward well-calibrated confidence and correctness.</p>
            </div>
          </div>
        </div>

        {/* Architecture Section */}
        <div className="mb-24 bg-white border-4 border-black brutal-shadow p-8">
          <h2 className="text-3xl md:text-4xl font-black uppercase mb-8 text-center bg-brand-yellow inline-block px-4 py-2 border-4 border-black transform -rotate-1 brutal-shadow">
            System Architecture
          </h2>
          <div className="w-full overflow-x-auto">
            <Mermaid chart={architectureDiagram} />
          </div>
        </div>

        {/* Live Dashboard/Leaderboard Section */}
        <div id="network" className="mb-24 scroll-mt-24">
          <div className="flex flex-col md:flex-row justify-between items-end mb-8">
            <div>
              <h2 className="text-4xl md:text-5xl font-black uppercase mb-2">Live Miner Leaderboard</h2>
              <p className="text-xl font-bold bg-black text-white px-3 py-1 inline-block">Epoch 4829 ‚Ä¢ Generalization Score (S)</p>
            </div>
            <div className="flex space-x-2 mt-4 md:mt-0">
              <button
                onClick={() => setActiveTab('live')}
                className={`px-6 py-2 font-bold border-4 border-black brutal-shadow-hover ${activeTab === 'live' ? 'bg-brand-red text-white' : 'bg-white'}`}
              >
                Top Miners
              </button>
              <button
                onClick={() => setActiveTab('validators')}
                className={`px-6 py-2 font-bold border-4 border-black brutal-shadow-hover ${activeTab === 'validators' ? 'bg-brand-yellow text-black' : 'bg-white'}`}
              >
                Validators
              </button>
            </div>
          </div>

          <div className="bg-white border-4 border-black brutal-shadow overflow-hidden">
            <div className="overflow-x-auto">
              <table className="w-full text-left border-collapse min-w-[800px]">
                <thead>
                  <tr className="bg-black text-white text-lg font-bold uppercase border-b-4 border-black">
                    <th className="p-4 border-r-4 border-black">Rank</th>
                    <th className="p-4 border-r-4 border-black">Miner UID / Coldkey</th>
                    <th className="p-4 border-r-4 border-black bg-brand-blue text-white">Generalization (S)</th>
                    <th className="p-4 border-r-4 border-black">Accuracy</th>
                    <th className="p-4">Calibration</th>
                  </tr>
                </thead>
                <tbody className="font-mono text-lg font-medium">
                  {[
                    { rank: 1, uid: "4091", key: "5HeR...x9P", score: "0.942", acc: "96.4%", cal: "0.02 Brier", bg: "bg-brand-yellow" },
                    { rank: 2, uid: "882", key: "5Ca1...yZ2", score: "0.915", acc: "94.1%", cal: "0.05 Brier", bg: "bg-white" },
                    { rank: 3, uid: "1104", key: "5Ff9...kK1", score: "0.889", acc: "90.2%", cal: "0.08 Brier", bg: "bg-white" },
                    { rank: 4, uid: "77", key: "5Jj2...pQ8", score: "0.851", acc: "88.7%", cal: "0.11 Brier", bg: "bg-gray-100" },
                    { rank: 5, uid: "305", key: "5Oo4...rR5", score: "0.812", acc: "85.0%", cal: "0.15 Brier", bg: "bg-gray-100" },
                  ].map((row, i) => (
                    <tr key={i} className={`border-b-4 border-black hover:bg-gray-200 transition-colors ${row.bg}`}>
                      <td className="p-4 border-r-4 border-black font-black text-xl text-center">{row.rank}</td>
                      <td className="p-4 border-r-4 border-black flex items-center space-x-3">
                        <div className="w-8 h-8 rounded-full border-2 border-black bg-gradient-to-tr from-brand-red to-brand-yellow"></div>
                        <span>UID: {row.uid} <span className="text-sm text-gray-500">[{row.key}]</span></span>
                      </td>
                      <td className="p-4 border-r-4 border-black text-brand-blue font-black">{row.score}</td>
                      <td className="p-4 border-r-4 border-black text-green-600 font-bold">{row.acc}</td>
                      <td className="p-4 text-brand-red font-bold">{row.cal}</td>
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
          </div>
        </div>

        {/* KaggleIngest Promo */}
        <div className="bg-brand-red text-white border-4 border-black brutal-shadow p-8 md:p-12 mb-24 relative overflow-hidden">
          <div className="absolute top-0 right-0 p-8 opacity-20 transform rotate-12">
            <Users size={120} />
          </div>
          <div className="relative z-10 max-w-2xl">
            <h2 className="text-4xl md:text-5xl font-black uppercase mb-6">The Unfair Advantage:<br/>KaggleIngest</h2>
            <p className="text-xl font-medium mb-8">
              Most subnets fail because they lack skilled miners. We solve this by bridging the 15M+ data scientists from Kaggle directly into the OpenArena ecosystem.
            </p>
            <div className="bg-black p-4 inline-flex items-center space-x-4 border-4 border-white brutal-shadow transform -rotate-2">
              <span className="font-mono text-brand-yellow font-bold text-lg">{`!pip install openarena-kaggle`}</span>
              <CheckCircle2 className="text-brand-green" />
            </div>
            <p className="mt-6 text-lg font-bold underline decoration-brand-yellow decoration-4 underline-offset-4">Cold Start Solved. Instant liquidity of intelligence.</p>
          </div>
        </div>

      </main>
    </div>
  );
}



================================================
FILE: openarena/frontend/src/app/whitepaper/page.tsx
================================================
import fs from 'fs';
import path from 'path';
import Link from 'next/link';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkMath from 'remark-math';
import rehypeKatex from 'rehype-katex';
import rehypeRaw from 'rehype-raw';
import { ArrowLeft } from 'lucide-react';
import Mermaid from '../../components/Mermaid';
import 'katex/dist/katex.min.css';

export default function WhitepaperPage() {
  const filePath = path.join(process.cwd(), 'src/content/whitepaper.md');
  const fileContent = fs.readFileSync(filePath, 'utf8');

  return (
    <div className="min-h-screen pb-24 font-sans selection:bg-brand-yellow selection:text-black">
      {/* Navigation */}
      <nav className="border-b-4 border-black bg-white sticky top-0 z-50">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="flex justify-between h-16 items-center">
            <Link href="/" className="flex items-center space-x-2 brutal-shadow-hover hover:-translate-x-1">
              <div className="w-8 h-8 bg-brand-red border-2 border-black flex items-center justify-center">
                <ArrowLeft size={20} className="text-white" />
              </div>
              <span className="font-bold text-xl tracking-tight uppercase">Back to Arena</span>
            </Link>
          </div>
        </div>
      </nav>

      <main className="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 mt-12 md:mt-16">
        <div className="bg-white border-4 border-black brutal-shadow p-8 md:p-12 prose prose-lg prose-black max-w-none">
          <ReactMarkdown
            remarkPlugins={[remarkGfm, remarkMath]}
            rehypePlugins={[rehypeRaw, rehypeKatex]}
            components={{
              code({ inline, className, children, ...props }: React.DetailedHTMLProps<React.HTMLAttributes<HTMLElement>, HTMLElement> & { inline?: boolean }) {
                const match = /language-(\w+)/.exec(className || '');
                if (!inline && match && match[1] === 'mermaid') {
                  return <Mermaid chart={String(children).replace(/\n$/, '')} />;
                }
                return !inline ? (
                  <pre className="bg-black text-green-400 p-4 font-mono text-sm overflow-x-auto brutal-shadow">
                    <code className={className} {...props}>
                      {children}
                    </code>
                  </pre>
                ) : (
                  <code className="bg-gray-200 px-1 py-0.5 rounded text-sm text-red-600" {...props}>
                    {children}
                  </code>
                );
              }
            }}
          >
            {fileContent}
          </ReactMarkdown>
        </div>

        <div className="mt-12 text-center">
          <Link href="/" className="inline-flex items-center justify-center space-x-2 font-black text-lg px-8 py-4 border-4 border-black bg-brand-yellow text-black brutal-shadow brutal-shadow-hover">
            <span>Return to Home</span>
          </Link>
        </div>
      </main>
    </div>
  );
}



================================================
FILE: openarena/frontend/src/components/Mermaid.tsx
================================================
'use client';

import React, { useEffect, useState, useRef } from 'react';
import mermaid from 'mermaid';

export default function Mermaid({ chart }: { chart: string }) {
  const svgId = React.useId().replace(/:/g, '');
  const [svg, setSvg] = useState<string>('');
  const containerRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    mermaid.initialize({
      startOnLoad: false,
      theme: 'base',
      themeVariables: {
        fontFamily: 'var(--font-space-mono)',
        primaryColor: '#fde047',
        primaryBorderColor: '#121212',
        primaryTextColor: '#121212',
        secondaryColor: '#ef4444',
        tertiaryColor: '#3b82f6',
        lineColor: '#121212',
      },
      flowchart: {
        htmlLabels: true,
        curve: 'stepAfter',
      }
    });

    const renderChart = async () => {
      if (containerRef.current) {
        try {
          const { svg } = await mermaid.render(svgId, chart);
          setSvg(svg);
        } catch (error) {
          console.error('Mermaid rendering failed', error);
        }
      }
    };

    renderChart();
  }, [chart, svgId]);

  return (
    <div
      ref={containerRef}
      className="mermaid-container flex justify-center w-full"
      dangerouslySetInnerHTML={{ __html: svg }}
    />
  );
}



================================================
FILE: openarena/frontend/src/content/whitepaper.md
================================================
# OpenArena: The Decentralized Adversarial Evaluation Protocol

**"The Proof of Intelligence"**

> [!IMPORTANT]
> **Core Thesis**: Static benchmarks are dead. Intelligence is not the ability to memorize a fixed dataset; it is the ability to generalize to new, unseen distributions. OpenArena is a continuous, adversarial stress-test for AI models, turning evaluation into a verifiable digital commodity.

---

## 1. Introduction: The Crisis of Evaluation

Modern AI has a **Goodhart's Law** problem: "When a measure becomes a target, it ceases to be a good measure."

- **Contamination**: Public datasets (GSM8K, MMLU) leak into training data.
- **Saturation**: Top models score 90%+ on benchmarks but fail in production.
- **Trust**: Who validates the validator?

**OpenArena** solves this by creating a **Dynamic Adversarial Evaluation Game**.

- **Validators** draw from **LiveBench** every epoch (a continuously updated stream of verifiable, objective ground-truth questions across math, coding, and reasoning).
- **Miners** must solve these unseen tasks instantly.
- **Incentives** reward _generalization_ and _efficiency_, while punishing _memorization_ and _wrapping_.

### 1.1 Core Thesis: Proof of Intelligence

We define "Intelligence" not as knowledge retrieval, but as **Generalization Efficiency**:

> _The ability to solve novel, high-entropy tasks with minimum latency and compute._

This shift allows us to distinguish between a 100B parameter model that memorized the internet and a 7B parameter model that can actually _reason_.

---

## 2. Technical Architecture

### 2.1 The Flow of Intelligence

```mermaid
sequenceDiagram
    participant V as Validator (Game Master)
    participant M as Miner (Solver)
    participant C as Chain (Bittensor)

    Note over V, M: Epoch N Starts (Block 0-360)

    rect rgb(20, 20, 20)
        Note right of V: 1. Generate Dynamic Task<br/>(e.g. "Solve random math puzzle")
        V->>V: Hash(Task) + Encrypt(GroundTruth)
        V->>M: Broadcast Synapse (Task Only)
    end

    rect rgb(40, 40, 40)
        Note right of M: 2. Compute Solution<br/>(LLM Inference / Code Exec)
        M->>M: Hash(Answer + Salt)
        M->>C: Commit Hash (Prevents Front-running)
    end

    rect rgb(20, 20, 20)
        Note right of V: 3. Reveal Phase
        M->>V: Reveal Answer
        V->>V: Verify Hash matches Commit
        V->>V: Score(Answer, GroundTruth)
    end

    rect rgb(60, 20, 20)
        Note right of C: 4. Yuma Consensus
        V->>C: Set Weights (W_i)
        C->>M: Distribute TAO Rewards
    end
```

### 2.2 Component Roles

| Role          | Responsibility                                                                     | Incentive                                                                        |
| :------------ | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Miner**     | Solve arbitrary tasks (Text, Code, Math) with high accuracy and low latency.       | Maximizes Reward ($R$) by optimizing inference speed and model generalization.   |
| **Validator** | Generate high-entropy, non-repeatable tasks. Evaluate miner solutions objectively. | Maximizes Dividends ($D$) by attracting high-quality miners and staking support. |

---

## 3. Incentive Mechanism (The Math)

The core innovation is the **Generalization Score ($S$)**.

### 3.1 The Scoring Function

For a set of $N$ tasks in an epoch, a miner $i$'s score $S_i$ is:

$$ S*i = \underbrace{\alpha \cdot \frac{1}{N} \sum*{j=1}^{N} \text{Acc}(y\*{ij}, y^\*_{j})}*{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij}, \text{Acc}*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t_{ij})}\*{\text{Latency Penalty}} $$

Where:

- $\text{Acc}(y_{ij}, y^*_{j})$: Accuracy metric (0 or 1 for exact match, or Levenshtein/BLEU for text).
- $\text{Cal}$: **Calibration Score**. Rewards miners who are confident when correct and uncertain when wrong (using Brier Score or Log Loss).
- $\text{Lat}$: **Latency Penalty**. $e^{t_{ij} - T_{max}}$.

### 3.2 Yuma Consensus & Weight Setting

Validators normalize scores to a weight vector $W$:
$$ w*{i} = \frac{e^{S_i / \tau}}{\sum*{k} e^{S_k / \tau}} $$
*(Using Softmax with temperature $\tau$ to control competition intensity)\*.

---

## 4. Adversarial Hardening (How We Win)

### üõ°Ô∏è Challenge 1: Memorization / Lookup

- **Attack**: Miners cache answers from previous epochs.
- **Defense**: **LiveBench Data Pipeline**.
  - _Continuous Updates_: LiveBench releases new questions regularly.
  - _Contamination Free_: New questions are delayed from public release ensuring models cannot pre-train on them.
  - _Objective Truth_: Each question has verifiable ground-truth answers (math, code, data analysis) eliminating subjective LLM Judges.

### üõ°Ô∏è Challenge 2: Front-Running / Copying

- **Attack**: Fast miner sees a smart miner's answer in the mempool and copies it.
- **Defense**: **Commit-Reveal Scheme**.
  1.  Miner submits `Hash(Answer + Salt)`.
  2.  After window closes, Miner submits `Answer + Salt`.
  3.  Validator verifies hash matches.

### üõ°Ô∏è Challenge 3: Validator Collusion

- **Attack**: Validator shares Ground Truth with a specific miner.
- **Defense**: **Cross-Validation**.
  - Multiple validators score the same miner.
  - If Validator A's scores diverge significantly from the consensus media (Yuma Consensus), Validator A loses V-Trust and dividends.

---

## 5. Token Economics (The OpenArena Flywheel)

### 5.1 The Formal Value Loop ($V$)

Let $F$ be the fee paid by an enterprise (e.g., Anthropic) to prioritize a specific evaluation dataset $D_{target}$.

$$ F*{distribution} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{validators} + 0.2 \cdot F*{miners} $$

1.  **Burn ($40\%$)**: Permanently removed from supply, creating deflationary pressure on $\tau$.
2.  **Validator Reward ($40\%$)**: Distributed to validators proportional to their stake ($S_v$) and their **Curator Score** ($C_v$).
3.  **Miner Reward ($20\%$)**: Distributed to miners who solve $D_{target}$ with the highest **Generalization Score** ($G_m$).

### 5.2 The Enterprise Demand Flywheel

As enterprises pay $F$ to access the network:

1.  **Demand** for TAO increases (to pay fees).
2.  **Supply** of TAO decreases (via Burn).
3.  **Validator Yield** increases (via Dividend).
4.  **Miner Competition** increases (via Reward).

This creates a self-reinforcing loop where **Utility** drives **Security** and **Valuation**.

This ensures that **Enterprise Demand** directly correlates with **Miner Profitability** and **Token Scarcity**.

---

## 6. Security Analysis (Adversarial Robustness)

### 6.1 Attack: Pre-Computation (The "Lookup Table")

- **Vector**: Miner pre-calculates answers to known datasets to simulate intelligence.
- **Mitigation**: **Private LiveBench Release Schedule**.
  - LiveBench limits potential contamination by releasing new questions regularly.
  - To further reduce contamination, LiveBench delays publicly releasing the questions from the most-recent updates.
  - Validators pull from the _private_ LiveBench API tier, guaranteeing that the questions evaluated in the subnet are fundamentally un-indexed by any public model training pipeline.

### 6.2 Attack: Validator Laziness (Low Entropy)

- **Vector**: A Validator reuses old tasks to save compute, degrading the network's measurement quality.
- **Mitigation**: **Entropy Penalty ($E_v$)**.
  - We measure the Kullback-Leibler (KL) divergence between task distributions at time $t$ and $t-1$:
    $$ E*v = D*{KL}(P*t \parallel P*{t-1}) $$
  - If $E_v < \epsilon_{threshold}$ (statistically indistinguishable from previous epoch), the Validator's weight-setting power $W_v$ is slashed:
    $$ W*v^{new} = W_v^{old} \cdot (1 - \text{Penalty}*{lazy}) $$
- **Incentive**: **Difficulty Rating ($D_t$)**.
  - Validators are rewarded for generating tasks that separate miner performance.
  - If all miners score 100%, $D_t$ is low -> Validator reward reduced.
  - If no miner scores > 0%, $D_t$ is too high -> Validator reward reduced.
  - Optimal $D_t$ targets a Gaussian distribution of miner scores.

### 6.3 Attack: Front-Running (The "Copycat")

- **Mitigation**: **Commit-Reveal** (as defined in Section 4).
  - $t_0$: Miner submits $H = \text{SHA256}(Answer + Salt)$.
  - $t_1$: Reveal window opens.
  - Copycats only see hash $H$, preventing answer theft.

---

## 7. Go-To-Market & Integration (KaggleIngest)

We leverage **KaggleIngest** to visualize this war zone.

- **Leaderboard**: Real-time display of Miner Generalization Scores.
- **Museum**: Archive of "Hardest Tasks" (a valuable dataset).

---

## 6. Execution Roadmap (Round II Strategy)

### Phase 1: The "Stub" (Days 1-5)

- [ ] Implement `neurons/validator.py`: Basic task generation (Math/Logic).
- [ ] Implement `neurons/miner.py`: Basic OpenAI/Llama wrapper.
- [ ] Implement `commt-reveal` mechanism on-chain (using mock chain).

### Phase 2: The "Arena" (Days 6-12)

- [ ] Connect KaggleIngest frontend to Subnet stats.
- [ ] Deploy 5 Miner nodes (simulated) to show competition.
- [ ] Create visualization of "Score Drift" over time.



================================================
FILE: openarena/utils/crypto.py
================================================
import hashlib
import secrets

def generate_salt(length: int = 16) -> str:
    """Generates a random hex salt."""
    return secrets.token_hex(length)

def hash_commitment(answer: str, salt: str, miner_hotkey: str) -> str:
    """
    Creates a SHA256 commitment hash.
    Format: SHA256(answer + salt + miner_hotkey)
    """
    data = f"{answer}{salt}{miner_hotkey}".encode("utf-8")
    return hashlib.sha256(data).hexdigest()

def verify_commitment(commitment: str, answer: str, salt: str, miner_hotkey: str) -> bool:
    """Verifies that the answer and salt match the commitment."""
    expected_hash = hash_commitment(answer, salt, miner_hotkey)
    return secrets.compare_digest(expected_hash, commitment)



================================================
FILE: submission/Incentive_Mechanism.md
================================================
# OpenArena: Incentive Mechanism Design

## 1. Core Philosophy: Proof of Generalization

Unlike traditional subnets that reward _weight availability_ or _loss on a fixed dataset_, OpenArena rewards **Generalization**.
We define Generalization ($G$) as the ability of a miner $i$ to minimize loss $\mathcal{L}$ on a distribution $D_t$ that is disjoint from all prior distributions $\{D_0, ..., D_{t-1}\}$.

$$ G*i(t) = \mathbb{E}*{x \sim D_t} [ S(M_i(x), y^*) ] $$

## 2. The Reward Function ($R$)

The reward for miner $i$ at epoch $t$ is calculated as an aggregate of their performance across $K$ tasks.

$$ R*i = \sigma \left( \sum*{k=1}^{K} w*k \cdot \left( \alpha \cdot \underbrace{\mathcal{A}(y*{ik}, y^\*_k)}_{\text{Accuracy}} + \beta \cdot \underbrace{\mathcal{C}(c*{ik}, y*{ik})}_{\text{Calibration}} - \gamma \cdot \underbrace{\mathcal{L}(l_{ik})}\_{\text{Latency}} \right) \right) $$

### 2.1 Component Definitions

#### Accuracy ($\mathcal{A}$)

For Generative Tasks (e.g., Summarization), we use a semantic similarity metric (BERTScore) or Levenshtein Distance ($Lev$).
$$ \mathcal{A}_{text} = 1 - \frac{Lev(y_{ik}, y^_*k)}{\max(|y*{ik}|, |y^_\_k|)} $$

For Logic/Math Tasks, we use a binary score:
$$ \mathcal{A}_{logic} = \mathbb{I}(y_{ik} == y^\*\_k) $$

#### Calibration ($\mathcal{C}$)

We incentivize miners to know their own uncertainty using the **Brier Score**.
Miners submit a confidence $c_{ik} \in [0, 1]$.
$$ \mathcal{C} = 1 - (c*{ik} - \mathcal{A}*{logic})^2 $$
_Rationale_: A miner that is 100% confident but wrong is penalized heavily. A miner that is 50% confident and wrong is penalized less.

#### Latency ($\mathcal{L}$)

Speed is critical for real-world utility. We apply an exponential decay penalty based on the time delta $\Delta t$ relative to the fastest correct submission $t_{min}$.
$$ \mathcal{L} = e^{\lambda (t*{ik} - t*{min})} - 1 $$

## 3. Consensus Mechanism (Yuma)

The final weight $W_i$ set on the Bittensor blockchain is a consensus of the normalized rewards from all validators $v \in V$.

$$ W*i = \frac{\sum*{v \in V} S*v \cdot R*{vi}}{\sum*{j \in M} \sum*{v \in V} S*v \cdot R*{vj}} $$

Where $S_v$ is the stake of validator $v$ (V-Trust).
Miners with the highest $W_i$ receive the largest emission of $TAO.

## 4. Sustainability: The Efficiency Multiplier ($\mathcal{E}$)

To ensure long-term sustainability and prevent the subnet from becoming just "who has the most H100s", we introduce an **Efficiency Multiplier**.
This favors miners who achieve high accuracy with lower latency (proxy for model efficiency) and consistent uptime.

$$ R\_{final} = R_i \times \mathcal{E}\_i $$

Where $\mathcal{E}$ boosts miners who consistently solve "Flash Challenges" (sub-200ms tasks) which are impossible for API wrappers to route in time.

## 5. Anti-Gaming & Adversarial Hardening

### 5.1 The Commit-Reveal Scheme (Anti-Front-Running)

To prevent "Copycat Mining" (listening to the mempool), we strictly enforce a two-phase process:

1. **Commit Phase**: Miner $i$ submits $H_i = \text{SHA256}(y_{ik} || \text{salt} || \text{hotkey}_i)$.
2. **Reveal Phase**: Miner $i$ submits $y_{ik}, \text{salt}$.
3. **Verification**: Validator checks $H_i' == H_i$. If mismatch, $R_i = 0$.

### 5.2 Flash Challenges (Anti-Wrapper)

Validators randomly inject "Flash Tasks" with a strict $T_{max} = 200ms$.

- **Goal**: Filter out miners who are just wrapping GPT-4/Claude via API (network latency > 200ms).
- **Penalty**: Failure to respond in time $\to$ Score penalty $\gamma$ increases.

### 5.3 High-Entropy Generation (Anti-Lookup)

Tasks are generated procedurally with random seeds, ensuring $P(Task_t \in \text{TrainingSet}) \approx 0$.

- _Math_: Random coefficients.
- _Logic_: Randomly generated rulesets.



================================================
FILE: submission/OpenArena_PitchDeck.md
================================================
# OpenArena: The Pitch Deck

**Target Audience:** Bittensor Ideathon Judges & VC Investors
**Theme:** Neo-Brutalist, High Contrast, Urgent.

---

## Slide 1: The Hook

**Visual:** A split screen. Left: A robot reciting a dictionary (Static) - _Monochrome, pixelated_. Right: A robot navigating a shifting maze (Dynamic) - _Vibrant Neon, smooth motion_.
**Headline:** Static Benchmarks Are Dead.
**Sub-headline:** Introducing OpenArena: The First Decentralized Adversarial Evaluation Protocol.
**Speaker Notes:** "We are currently flying blind. We can no longer distinguish between a model that _remembers_ and a model that _thinks_. OpenArena is the solution."

---

## Slide 2: The Crisis (Market Problem)

**Visual:** A rugged, "glitch-art" line graph. The "Benchmark Score" line soars to 99% (Green), while the "Real World Utility" line crashes to 0% (Red).
**Headline:** The $10B Evaluation Gap.
**Core Stat:** "Goodhart's Law: When a measure becomes a target, it ceases to be a good measure."
**Speaker Notes:** "Every major lab is overfitting. MMLU is leaked. We need a test that changes every single day."

---

## Slide 3: The Solution (OpenArena)

**Visual:** The "Infinite Loop" of OpenArena.

1. **Validator** generates _fresh_ entropy (News, Math, Code).
2. **Miner** generalizes instantly.
3. **Score** assigned for adaptability.
   **Headline:** Proof of Generalization.
   **Value Prop:** "We don't test what you know. We test how fast you can learn."

---

## Slide 4: The Mechanism (Adversarial Hardening)

**Visual:** Deep dive into the **Commit-Reveal** architecture.

- Diagram showing `SHA256(Answer + Salt + Hotkey)`.
- A "Cheater" miner trying to copy-paste a hash and getting rejected.
  **Headline:** Uncheatable by Design.
  **Key Tech:**
- **Commit-Reveal:** Prevents front-running/weight-stealing.
- **Flash Challenges:** <200ms tasks to kill API wrappers.
- **Yuma Consensus:** Decentralized truth.

---

## Slide 5: The Business Model (The dTAO Flywheel)

**Visual:** A flywheel diagram.

1. **Demand:** Anthropic/OpenAI pay TAO to act as "Red Teamers".
2. **Action:** OpenArena Validators attack their models with adversarial prompts.
3. **Result:** Certified "Generalization Score".
4. **Value:** TAO burned/staked -> Subnet Value Increases.
   **Headline:** Evaluation-as-a-Service.
   **Speaker Notes:** "We turn 'Red Teaming' from a cost center into a decentralized commodity."

---

## Slide 6: The Unfair Advantage (KaggleIngest)

**Visual:** High-fidelity screenshot of the **KaggleIngest** Dashboard. A bold "Connect to OpenArena" button pulsing in the corner. Background is a dark, data-dense grid.

- **Exclusive Portal**: "The Only Bridge for 15M+ Data Scientists."
- **Zero Friction**: "From Notebook to Miner in 30 seconds."
  **Headline:** The Bridge from Web2 to Web3.
  **Key Point:** "We are not starting from zero. We are onboarding the 15M+ data scientists from Kaggle directly into the Bittensor ecosystem via KaggleIngest."

---

## Slide 7: The Roadmap (Execution)

**Visual:** Timeline.

- **Now:** Whitepaper & Protocol Design (Done).
- **Round II:** "Stub" Subnet (Simulated Miners).
- **Q3 2026:** Mainnet Launch + Kaggle Integration.
  **Headline:** From Idea to Standard.

---

## Slide 8: The Team

**Visual:** Headshots with "Kaggle Grandmaster" and "Blockchain Dev" badges.
**Headline:** Builders who know the Arena.
**Bio:** "We combine deep ML expertise with crypto-native mechanism design."

---

## Slide 9: The Ask

**Visual:** OpenArena Logomark.
**Headline:** Help Us Kill Static Benchmarks.
**Call to Action:** "Support OpenArena in the Ideathon. Let's build the Truth Machine."

---

## Slide 10: Appendix (Technical Specs)

**Visual:** The Scoring Function Equation.
$$ S = (Accuracy \times Novelty) - Latency $$
**Headline:** The Math of Intelligence.



================================================
FILE: submission/OpenArena_SourceCode.tar.gz
================================================
[Binary file]


================================================
FILE: submission/OpenArena_VideoScript.md
================================================
# OpenArena: Explanation Video Script

**Target Duration:** 90-120 Seconds
**Tone:** Urgent, Technical, Visionary.
**Visual Style:** Fast-paced, Neo-Brutalist typography, glitch effects, code snippets overlay.

---

## 0:00 - 0:20: The Problem (The "Benchmark Saturation" Glitch)

**[Visual]**: Screen recording of a generic "Leaderboard" scrolling infinitely. The numbers "99.9%" start multiplying and covering the screen until it looks broken.
**[Audio]**: "We are in a crisis of measurement. GPT-4, Gemini, Claude‚Äîthey all ace the benchmarks. 90% on GSM8K. 95% on MMLU. But ask them to code a novel app, or reason through a new paradox, and they hallucinate."

**[Visual]**: Text slam: **GOODHART'S LAW**.
**[Audio]**: "When a measure becomes a target, it ceases to be a good measure. Static benchmarks are dead. They've been leaked, memorized, and gamed."

---

## 0:20 - 0:50: The Solution (Enter OpenArena)

**[Visual]**: A dark screen. A single prompt appears: `> INIT_OPEN_ARENA`.
**[Visual]**: A map of nodes (Validators and Miners) lighting up.
**[Audio]**: "Introducing OpenArena. The first decentralized, adversarial evaluation protocol on Bittensor."

**[Visual]**: Split screen.

- Left ("Old Way"): A teacher handling out the same test every year.
- Right ("OpenArena"): A sparring algorithm generating new moves every second.
  **[Audio]**: "In OpenArena, there is no static test set. Validators generate _fresh_, verified tasks every single epoch. Math puzzles. Code generation. Logic traps. Unseen. Un-memorizable."

---

## 0:50 - 1:10: How It Works (The Mechanics)

**[Visual]**: Diagram of the **Commit-Reveal** flow.

1. Info graphic: `Task -> Miner -> Hash(Answer)`.
2. A "Lock" icon appears.
3. Timer counts down.
4. `Reveal -> Score`.
   **[Audio]**: "Miners must solve these tasks instantly to prove _Generalization_, not memorization. We use a cryptographically secure Commit-Reveal scheme to prevent front-running, and a Brier-Score based calibration metric to punish hallucinations."

---

## 1:10 - 1:30: The Unfair Advantage (KaggleIngest)

**[Visual]**: The **KaggleIngest** Dashboard (Sleek, Dark Mode). Cursor clicks "Submit Model".
**[Audio]**: "But technology is nothing without distribution. We are leveraging **KaggleIngest** to onboard the 15 million data scientists from Kaggle directly into this arena."

**[Visual]**: Numbers ticking up: "Miners: 10... 100... 10,000".
**[Audio]**: "The world's best talent, competing to define the true 'Smartest Model'."

---

## 1:30 - 1:40: Call to Action

**[Visual]**: OpenArena Logo using the generated Neo-Brutalist assets.
**[Text]**: **THE PROOF OF INTELLIGENCE**.
**[Audio]**: "Static benchmarks are the past. Adversarial evaluation is the future. Join the Arena."

**[End Card]**: Github URL | Bittensor Ideathon Logo.



================================================
FILE: submission/OpenArena_Whitepaper.md
================================================
# OpenArena: The Decentralized Adversarial Evaluation Protocol

**"The Proof of Intelligence"**

> [!IMPORTANT]
> **Core Thesis**: Static benchmarks are dead. Intelligence is not the ability to memorize a fixed dataset; it is the ability to generalize to new, unseen distributions. OpenArena is a continuous, adversarial stress-test for AI models, turning evaluation into a verifiable digital commodity.

---

## 1. Introduction: The Crisis of Evaluation

Modern AI has a **Goodhart's Law** problem: "When a measure becomes a target, it ceases to be a good measure."

- **Contamination**: Public datasets (GSM8K, MMLU) leak into training data.
- **Saturation**: Top models score 90%+ on benchmarks but fail in production.
- **Trust**: Who validates the validator?

**OpenArena** solves this by creating a **Dynamic Adversarial Evaluation Game**.

- **Validators** generate _fresh_ tasks every epoch (synthetic reasoning, real-time data, code puzzles).
- **Miners** must solve these unseen tasks instantly.
- **Incentives** reward _generalization_ and _efficiency_, while punishing _memorization_ and _wrapping_.

### 1.1 Core Thesis: Proof of Intelligence

We define "Intelligence" not as knowledge retrieval, but as **Generalization Efficiency**:

> _The ability to solve novel, high-entropy tasks with minimum latency and compute._

This shift allows us to distinguish between a 100B parameter model that memorized the internet and a 7B parameter model that can actually _reason_.

---

## 2. Technical Architecture

### 2.1 The Flow of Intelligence

```mermaid
sequenceDiagram
    participant V as Validator (Game Master)
    participant M as Miner (Solver)
    participant C as Chain (Bittensor)

    Note over V, M: Epoch N Starts (Block 0-360)

    rect rgb(20, 20, 20)
        Note right of V: 1. Generate Dynamic Task<br/>(e.g. "Solve random math puzzle")
        V->>V: Hash(Task) + Encrypt(GroundTruth)
        V->>M: Broadcast Synapse (Task Only)
    end

    rect rgb(40, 40, 40)
        Note right of M: 2. Compute Solution<br/>(LLM Inference / Code Exec)
        M->>M: Hash(Answer + Salt)
        M->>C: Commit Hash (Prevents Front-running)
    end

    rect rgb(20, 20, 20)
        Note right of V: 3. Reveal Phase
        M->>V: Reveal Answer
        V->>V: Verify Hash matches Commit
        V->>V: Score(Answer, GroundTruth)
    end

    rect rgb(60, 20, 20)
        Note right of C: 4. Yuma Consensus
        V->>C: Set Weights (W_i)
        C->>M: Distribute TAO Rewards
    end
```

### 2.2 Component Roles

| Role          | Responsibility                                                                     | Incentive                                                                        |
| :------------ | :--------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Miner**     | Solve arbitrary tasks (Text, Code, Math) with high accuracy and low latency.       | Maximizes Reward ($R$) by optimizing inference speed and model generalization.   |
| **Validator** | Generate high-entropy, non-repeatable tasks. Evaluate miner solutions objectively. | Maximizes Dividends ($D$) by attracting high-quality miners and staking support. |

---

## 3. Incentive Mechanism (The Math)

The core innovation is the **Generalization Score ($S$)**.

### 3.1 The Scoring Function

For a set of $N$ tasks in an epoch, a miner $i$'s score $S_i$ is:

$$ S*i = \underbrace{\alpha \cdot \frac{1}{N} \sum*{j=1}^{N} \text{Acc}(y\*{ij}, y^\*_{j})}*{\text{Accuracy}} \times \underbrace{\beta \cdot \text{Cal}(c*{ij}, \text{Acc}*{ij})}*{\text{Calibration}} - \underbrace{\gamma \cdot \text{Lat}(t_{ij})}\*{\text{Latency Penalty}} $$

Where:

- $\text{Acc}(y_{ij}, y^*_{j})$: Accuracy metric (0 or 1 for exact match, or Levenshtein/BLEU for text).
- $\text{Cal}$: **Calibration Score**. Rewards miners who are confident when correct and uncertain when wrong. We use the **Brier Score** decomposition:
  $$ \text{Brier} = \frac{1}{N} \sum\_{t=1}^{N} (f_t - o_t)^2 $$
  Where $f_t$ is the forecasted probability and $o_t$ is the outcome. This strictly penalizes "hallucinations" where a model claims high confidence but is wrong.
- $\text{Lat}$: **Latency Penalty**. $e^{t_{ij} - T_{max}}$.

### 3.2 Yuma Consensus & Weight Setting

Validators normalize scores to a weight vector $W$:
$$ w*{i} = \frac{e^{S_i / \tau}}{\sum*{k} e^{S_k / \tau}} $$
*(Using Softmax with temperature $\tau$ to control competition intensity)\*.

---

## 4. Adversarial Hardening (How We Win)

### üõ°Ô∏è Challenge 1: Memorization / Lookup

- **Attack**: Miners cache answers from previous epochs.
- **Defense**: **High-Entropy Generation**.
  - _Math_: "Calculate $A \times B$" where $A, B$ are random 10-digit primes.
  - _Real-Time_: "Summarize this article published 5 minutes ago" (Validators pull from NewsAPI).
  - _Code_: "Write a Python function to sort this random list [4, 1, 9...]".

### üõ°Ô∏è Challenge 2: Front-Running / Copying

- **Attack**: Fast miner sees a smart miner's answer in the mempool and copies it.
- **Defense**: **Commit-Reveal Scheme**.
  1.  Miner submits `Hash(Answer + Salt)`.
  2.  After window closes, Miner submits `Answer + Salt`.
  3.  Validator verifies hash matches.

### üõ°Ô∏è Challenge 3: Validator Collusion

- **Attack**: Validator shares Ground Truth with a specific miner.
- **Defense**: **Cross-Validation**.
  - Multiple validators score the same miner.
  - If Validator A's scores diverge significantly from the consensus media (Yuma Consensus), Validator A loses V-Trust and dividends.

---

## 5. Token Economics (The OpenArena Flywheel)

### 5.1 The Formal Value Loop ($V$)

Let $F$ be the fee paid by an enterprise (e.g., Anthropic) to prioritize a specific evaluation dataset $D_{target}$.

$$ F*{distribution} = 0.4 \cdot F*{burn} + 0.4 \cdot F*{validators} + 0.2 \cdot F*{miners} $$

1.  **Burn ($40\%$)**: Permanently removed from supply, creating deflationary pressure on $\tau$.
2.  **Validator Reward ($40\%$)**: Distributed to validators proportional to their stake ($S_v$) and their **Curator Score** ($C_v$).
3.  **Miner Reward ($20\%$)**: Distributed to miners who solve $D_{target}$ with the highest **Generalization Score** ($G_m$).

This ensures that **Enterprise Demand** directly correlates with **Miner Profitability** and **Token Scarcity**.

---

## 6. Security Analysis (Adversarial Robustness)

### 6.1 Attack: Pre-Computation (The "Lookup Table")

- **Vector**: Miner pre-calculates answers to known datasets to simulate intelligence.
- **Mitigation**: **Cryptographic Entropy Protocol**.
  - Let $H_b$ be the block hash at height $t$.
  - Let $K_v$ be the Validator's VRF key.
  - The **Task Seed** $S_t$ is derived as:
    $$ S_t = \text{SHA256}(H_b \parallel K_v) $$
  - The **Task** $T_t$ is generated via a deterministic mutation function $f$:
    $$ T*t = f(S_t, \text{Template}*{grammar}) $$
  - **Result**: Since $H_b$ is not known until block $t$, pre-computing $T_t$ is mathematically impossible.

### 6.2 Attack: Validator Laziness (Low Entropy)

- **Vector**: A Validator reuses old tasks to save compute, degrading the network's measurement quality.
- **Mitigation**: **Entropy Penalty ($E_v$)**.
  - We measure the Kullback-Leibler (KL) divergence between task distributions at time $t$ and $t-1$:
    $$ E*v = D*{KL}(P*t \parallel P*{t-1}) $$
  - If $E_v < \epsilon_{threshold}$ (statistically indistinguishable from previous epoch), the Validator's weight-setting power $W_v$ is slashed:
    $$ W*v^{new} = W_v^{old} \cdot (1 - \text{Penalty}*{lazy}) $$

### 6.3 Attack: Front-Running (The "Copycat")

- **Mitigation**: **Commit-Reveal** (as defined in Section 4).
  - $t_0$: Miner submits $H = \text{SHA256}(Answer + Salt)$.
  - $t_1$: Reveal window opens.
  - Copycats only see hash $H$, preventing answer theft.

---

## 7. Go-To-Market & Integration (The KaggleIngest Advantage)

**OpenArena** is not just a protocol; it is a bridge. We leverage **KaggleIngest** to be the **exclusive portal** for on-boarding the 15M+ Kaggle data scientists into Bittensor.

### 7.1 The Cold Start Solution

Most subnets fail because they cannot attract talent. We solve this by meeting miners where they already are.

- **Direct Integration**: Kaggle Notebooks can submit to OpenArena via a single Python cell (`!pip install openarena-kaggle`).
- **Leaderboard Sync**: Real-time display of Miner Generalization Scores on a familiar, Web2-style dashboard.
- **The Museum**: We archive the "Hardest Tasks" generated by Validators, creating a valuable, ever-growing dataset of "adversarial examples" that researchers can download.

---

## 6. Execution Roadmap (Round II Strategy)

### Phase 1: The "Stub" (Days 1-5)

- [ ] Implement `neurons/validator.py`: Basic task generation (Math/Logic).
- [ ] Implement `neurons/miner.py`: Basic OpenAI/Llama wrapper.
- [ ] Implement `commt-reveal` mechanism on-chain (using mock chain).

### Phase 2: The "Arena" (Days 6-12)

- [ ] Connect KaggleIngest frontend to Subnet stats.
- [ ] Deploy 5 Miner nodes (simulated) to show competition.
- [ ] Create visualization of "Score Drift" over time.



================================================
FILE: submission/SUBMISSION.md
================================================
# OpenArena: The Proof of Intelligence Subnet

**Team**: OpenArena (Anand / KaggleIngest)
**Track**: Bittensor Ideathon

## Abstract

OpenArena solves the "Static Benchmark Crisis" in AI evaluation. By generating high-entropy, novel tasks every epoch and using a Commit-Reveal scheme to prevent gaming, we create a "Proof of Intelligence" that rewards generalization, not memorization.

## Core Deliverables

### 1. [Whitepaper](OpenArena_Whitepaper.md)

The comprehensive technical and economic architecture of the subnet.

- **Key Innovation**: "Generalization Efficiency" Score.
- **Security**: Commit-Reveal & Yuma Consensus.
- **Economics**: The "Alpha Demand Loop" for enterprise audits.

### 2. [Pitch Deck](OpenArena_PitchDeck.md)

A 10-slide visual overview of the $10B market opportunity and our go-to-market strategy via KaggleIngest.

### 3. [Video Script](OpenArena_VideoScript.md)

The narrative script for our submission video, demonstrating the "Live Leaderboard" and "Miner War Room."

### 4. [Incentive Mechanism](Incentive_Mechanism.md)

The rigorous mathematical definition of our scoring function: $S = \text{Accuracy} \times \text{Calibration} - \text{Latency}$.

## Proof of Implementation (Simulation)

We have successfully simulated the subnet lifecycle, including adversarial scenarios.

- **[Source Code](OpenArena_SourceCode.tar.gz)**: Full Python implementation of Miners, Validators, and Protocol.
- **Simulation Results**: `demo.py` demonstrates:
  - **Honest Miners**: Converge to high weights.
  - **Front-Runners**: Slashed to zero (proven via simulation logs).
  - **Lazy Miners**: Penalized for low accuracy.

## Go-To-Market

We leverage the existing **KaggleIngest** platform as a frontend visualization layer, creating an immediate, usable product for the Bittensor ecosystem.



================================================
FILE: tests/test_entropy.py
================================================
import hashlib
import sys

# Mocking the wallet part
class MockHotkey:
    @property
    def ss58_address(self):
        return "5F3sa2TJAUVfwsj5hbPxr3rV8g9pJjS6XjT99...MockKey"

class MockWallet:
    def __init__(self):
        self.hotkey = MockHotkey()

def test_entropy_generation():
    wallet = MockWallet()
    step = 123

    # 1. Generate Task (Entropy Protocol)
    # Simulated VRF: Hash(BlockHeight + ValidatorKey + PrevBlockHash)
    # In production, this uses on-chain randomness.
    block_height = step
    vrf_key = str(wallet.hotkey.ss58_address)
    # Simulating previous block hash for entropy chain
    prev_block_hash = hashlib.sha256(str(step - 1).encode()).hexdigest()

    # Formal Entropy Seed Derivation
    entropy_seed = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()

    print(f"Test Step: {step}")
    print(f"VRF Key: {vrf_key}")
    print(f"Prev Hash: {prev_block_hash}")
    print(f"Generated Entropy Seed: {entropy_seed}")

    # Verify deterministic property
    entropy_seed_2 = hashlib.sha256(f"{block_height}{vrf_key}{prev_block_hash}".encode()).hexdigest()
    assert entropy_seed == entropy_seed_2, "Entropy seed must be deterministic!"
    print("SUCCESS: Entropy seed is deterministic and generated correctly.")

if __name__ == "__main__":
    test_entropy_generation()


