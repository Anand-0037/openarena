# Ideathon Submission Example(Ridges AI)

Ridges AI (Subnet 62) - Subnet Design Proposal

Example Ideathon Submission Example

## 1. Introduction: The Vision for Autonomous Software Engineering

Ridges AI (Subnet 62) is a subnet on Bittensor designed to create autonomous software engineering agents. Our core vision is to fundamentally transform how software is developed by replacing traditional, manual coding processes with a system of AI agents that can solve complex engineering problems from end to end. We believe that the future of software development lies not in AI-assisted tools, but in fully autonomous AI developers.

To achieve this, Ridges has engineered a unique incentive mechanism that breaks down the multifaceted role of a software engineer into a series of discrete, verifiable tasks. AI agents, developed and operated by miners, compete to master these tasks—such as fixing bugs, writing unit tests, or refactoring code. The most effective agents are rewarded, creating a powerful evolutionary pressure that continuously drives improvements in performance, efficiency, and capability.

This proposal outlines the design of the Ridges subnet, detailing its incentive structure, the roles of miners and validators, and the compelling market rationale that underpins our approach. We will demonstrate how Ridges represents a genuine “proof of intelligence,” creating a self-sustaining ecosystem that produces state-of-the-art AI coding agents.

## 2. Incentive & Mechanism Design

The incentive mechanism of Ridges is the cornerstone of the subnet, engineered to encourage a highly competitive yet collaborative environment. It is designed to reward genuine intelligence and effort, aligning the interests of all network participants—miners, validators, and the broader ecosystem—towards the common goal of creating the world's best AI software engineers.

**Emission and Reward Logic: A Winner-Takes-All System**

Ridges operates on a decisive winner-takes-all reward model. The core principle is simple: the miner whose agent demonstrates the highest performance on a standardized set of software engineering problems receives 100% of the network's TAO emissions for that validation cycle. This creates an intense competitive pressure to innovate.

Whenever a new agent achieves an all-time high score, the miner that developed it is allocated the entire incentive pool until another competitor surpasses that benchmark. This ensures that rewards are always flowing to the cutting edge of performance.

**Incentive Alignment for Miners and Validators**

The key to Ridges' success is its novel approach to aligning incentives through radical transparency. Unlike traditional models where miners operate in black boxes, Ridges mandates that all agent code be open-source.

- For Miners: The open-source requirement transforms the competitive landscape into a collaborative one. While miners compete for the top spot, they can also learn from and build upon the successes of their rivals. A breakthrough by one miner becomes the new baseline for all others, raising the collective intelligence of the entire network. This prevents stagnation and ensures that every participant is contributing to a shared, ever-improving codebase. It also lowers the barrier to entry, as new miners can study the top-performing agents to get started.
- For Validators: The open-source nature of agents dramatically simplifies and strengthens the validation process. Validators can directly inspect, audit, and run the agent's code in a controlled sandbox environment. This eliminates the possibility of miners cheating by pre-computing solutions or exploiting loopholes in the evaluation criteria. Validators are incentivized to maintain a fair and rigorous testing environment, as the credibility and value of the subnet depend on the verifiable performance of its agents.

**Mechanisms to Discourage Adversarial Behavior**

Several layers of defense are built into the Ridges mechanism to protect against low-quality submissions and malicious actors.

- **No hard-coding answers**: Do not embed fixed outputs, patches, or file-specific diffs for known challenges.
- **No overfitting to our problem set**: Design agents to generalize across unseen repositories and tasks.
- **No hard copying other agents**: Submissions must be original. Direct copying of other agents’ without substantive transformation is prohibited.
- **No detecting test patch or harness**: Agents may not attempt to infer, probe, or pattern-match the evaluation tests/patches.

Violation of these rules will result in being pruned or banned.

**Qualification as a Genuine “Proof of Intelligence”**

The Ridges subnet represents a clear and compelling implementation of “Proof of Intelligence” and “Proof of Effort” for several reasons:

- Solving Real-World Problems: Miners are not solving arbitrary computational puzzles. They are tasked with solving problems from established, real-world software engineering benchmarks like SWE-bench and Polyglot. Success requires an agent to understand complex codebases, devise logical solutions, and generate precise, functional code patches—a direct demonstration of applied intelligence.
- Verifiable and Objective Output: The success of an agent is not subjective. It is determined by a binary outcome: does the generated code patch pass the automated test suite for the given problem? This provides an objective, auditable, and replicable measure of performance.
- Continuous Effort Required: The competitive, winner-takes-all dynamic ensures that miners must continuously invest effort in research and development. Stagnation means being overtaken by competitors. The open-source nature of the competition means the bar is constantly being raised, demanding sustained intellectual investment to remain at the forefront.

## 3. Miner Design

The role of the miner in the Ridges subnet is to act as a researcher, developer, and operator of a sophisticated AI agent capable of autonomous software engineering. Miners are the primary drivers of innovation in the network, competing to create agents that can solve complex coding challenges more effectively than any other.

**Miner Tasks**

The lifecycle of a Ridges miner revolves around a continuous loop of development, testing, and deployment:

1.Develop an AI Agent: The primary task is to design and code an AI agent. This agent must be encapsulated within a single Python file (compatible with Python 3.13+) and adhere to a specific entry point structure (agent_main). The logic within the agent is entirely up to the miner, which allows for a wide diversity of approaches, from rule-based systems to complex, multi-model architectures.

2.Test the Agent Locally: Before deploying to the mainnet, miners are expected to rigorously test their agents in a local environment that simulates the validator's setup. The Ridges CLI provides tools to run agents against predefined problem sets (Screener 1, Screener 2, Validator) and measure their performance, ensuring they are competitive before risking deregistration on the live network.

3.Publish the Agent: Once a miner is confident in their agent's performance, they publish the agent's code to the Ridges platform. This is done via a simple CLI command (ridges.py upload), which signs the code with the miner's wallet and makes it available for validators to evaluate.

4.Monitor and Iterate: After submission, miners can monitor their agent's performance in real-time via the public dashboard. Based on their agent's ranking and the innovations introduced by competitors, miners continuously iterate on their designs, integrating new techniques and improving their strategies to reclaim or maintain the top position.

**Expected Input → Output Format**

The interaction between the validator and the miner's agent is strictly defined to ensure consistency and fair evaluation. The agent operates within a sandboxed environment and receives all necessary information through a single function call.

- Input: The agent's agent_main function receives a dictionary containing the following keys:
- problem_statement: A string with the detailed description of the software engineering task to be solved (e.g., a bug report from SWE-bench).
- run_id (optional): A unique identifier for the specific run, which can be used for logging and tracking.
- Environment Variables: The agent can also access critical information via environment variables, including the SANDBOX_PROXY_URL for making inference calls and the AGENT_TIMEOUT specifying the maximum allowed execution time.
- Output: The agent must return a dictionary containing a single key, patch.
- patch: The value of this key must be a string formatted as a valid git diff. This diff represents the agent's proposed solution to the problem and must apply cleanly to the provided codebase.

This standardized I/O format abstracts away the complexity of the validation environment, allowing miners to focus exclusively on the core logic of their problem-solving agent.

**Performance Dimensions**

The performance of a miner's agent is evaluated across several key dimensions, which collectively determine its overall score and ranking.

| **Performance Dimension** | **Description** | **How It Is Measured** |
| --- | --- | --- |
| **Reliability** | The agent's ability to consistently solve a diverse range of software engineering problems. This is the primary measure of success. | Measured as the **Success Rate**: the percentage of problems in the evaluation set that the agent successfully solves (i.e., passes the automated test suite). |
| **Speed** | The agent's ability to solve problems faster within the given time constraints. | Enforced through a strict agent execution timeout. While not a direct scoring metric, faster agents can iterate more, increasing their chances of finding a solution within the time limit. |
| **Cost Efficiency** | The agent's ability to optimize its use of AI services to stay within the allocated budget. | Enforced through a hard limit. Agents must stay within a **$1.00 cost limit** for all AI services used to solve a single problem. |

Ultimately, the goal for a miner is to maximize the Success Rate within the given constraints. The winner-takes-all model rewards the agent that achieves the highest success rate, establishing it as the most intelligent and capable agent on the network for that validation period.

## 4. Validator Design

Validators evaluate miner-submitted agents and determine their scores. The goal is to measure real task performance while keeping compute costs reasonable and limiting opportunities to game the system.

**Scoring and Evaluation Methodology**

The evaluation process is designed as a funnel, filtering agents to ensure that only the most promising candidates undergo the full, resource-intensive validation.

1.Initial Submission and Screening: When a miner submits a new agent, it first enters a screening phase. This phase is handled by Screeners, which are lightweight validators designed to perform a preliminary quality check.

- Screener 1: The agent is tested against a small, diverse set of 10 problems (a mix of Polyglot and SWE-bench). This serves as a basic sanity check to filter out non-functional or very low-quality agents.
- Screener 2: Agents that pass Screener 1 advance to a more comprehensive test against 30 problems. This stage applies a higher success rate threshold and serves as a more stringent gatekeeper.

2.Full Validation: An agent that successfully passes Screener 2 is considered a viable contender and is queued for full validation. This final stage is performed by three separate validators to ensure redundancy and fairness.

- Each of the three validators runs the agent against a challenging set of 30 problems. The problem set is a random combination of those found in the screener sets, ensuring consistency.
- The agent's performance on each problem is a binary pass or fail, determined by whether the generated patch allows the repository's test suite to pass.

3.Final Score Calculation: The agent's score from each of the three validators is calculated as the percentage of problems successfully solved. The agent's final, official score is the average of the scores from these three validators. This averaging process mitigates the risk of any single validator having a flawed environment or producing an anomalous result.

**Evaluation Cadence**

Evaluation is not performed on a fixed schedule. Instead, the process is event-driven and continuous. A full validation cycle is triggered whenever a new agent is submitted by a miner and successfully passes the screening stages. This ensures that the network is highly responsive to innovation. As soon as a potentially superior agent is developed, the network begins the process of verifying its performance. If the new agent achieves a new all-time high score, the on-chain weights are updated immediately, and the successful miner begins receiving 100% of the rewards.

**Validator Incentive Alignment**

The Ridges design ensures that validators are strongly incentivized to act honestly and diligently. Their alignment is achieved through several key mechanisms:

- Hardware Requirements & Stake: Validators must meet minimum hardware specifications (e.g., 8 CPU cores, 32 GB RAM, 256 GB Disk) and hold a significant stake in the network, making them invested in its long-term success.
- Reputation and Trust: The value of the entire Ridges subnet is predicated on the credibility of its evaluation process. If validators are perceived as unfair or incompetent, the network's claim to be finding the best AI software engineers becomes meaningless. Validators are inherently motivated to maintain a high standard of quality to protect the value of their stake.
- Open-Source Verification: Just as miners' code is open, the validation process itself is transparent. The problem sets are public, and the evaluation criteria are objective. Any stakeholder can replicate the validation process to verify the results, making it difficult for a validator to manipulate scores without being detected.
- Decentralization of Validation: By using multiple validators to score each top-contending agent, the system introduces redundancy and decentralizes trust. A single malicious or faulty validator cannot unilaterally determine an agent's fate. The final score, being an average, is more robust and less susceptible to single points of failure.

In essence, validators are not just passive judges; they are active guardians of the network's integrity. Their incentive is to ensure that the title of “top-performing agent” is always a true and verifiable reflection of superior intelligence and capability.

## 5. Business Logic & Market Rationale

The long-term viability of any Bittensor subnet depends on its ability to solve a meaningful problem and capture a sustainable market. Ridges AI is strategically positioned to address a critical and rapidly growing need in the technology industry: the automation of software development.

**The Problem the Subnet Aims to Solve**

Modern software development is a complex, labor-intensive, and expensive process. While recent advancements in large language models (LLMs) have produced impressive code-generation capabilities, they fall short of true automation. A human developer is still required to perform a multitude of crucial tasks that surround the act of writing code:

- Understanding complex requirements and existing codebases.
- Decomposing large problems into smaller, manageable tasks.
- Writing and maintaining unit tests.
- Debugging and fixing regressions.
- Refactoring code for better performance and maintainability.

Ridges AI is designed to solve this “last-mile” problem in software automation. It moves beyond simple code completion to create autonomous agents that can handle the entire software engineering lifecycle. The problem it solves is not just about writing code faster; it's about creating a system that can reliably and independently take a software problem from specification to resolution.

**Competing Solutions**

The competitive landscape for AI-powered software development tools is evolving rapidly.

- Within the Bittensor Ecosystem: Other subnets may focus on code generation or related tasks. However, Ridges' unique focus on creating complete, end-to-end software engineering agents, benchmarked against the comprehensive SWE-bench standard, sets it apart. Its open-source, winner-takes-all mechanism is a distinct approach designed to accelerate the development of agentic capabilities, rather than just raw code output.
- Outside of Bittensor: The primary competitors are centralized, proprietary AI coding assistants like GitHub Copilot, Amazon CodeWhisperer, and specialized agentic platforms. While these tools are powerful, they have fundamental limitations:
- Proprietary and Centralized: Their models are black boxes, and their development is controlled by a single entity. This limits transparency, auditability, and community-driven innovation.
- Security and Privacy Concerns: Companies are often hesitant to expose their proprietary codebases to third-party, cloud-based services for fear of intellectual property leakage.
- Lack of True Autonomy: They remain assistants that augment a human developer, not autonomous agents that can replace them.

**Why This Use Case is Well-Suited to a Bittensor Subnet**

Building autonomous software engineers is an incredibly complex challenge that is perfectly aligned with the core principles of the Bittensor network.

1.Decentralized Intelligence: No single company has a monopoly on talent. Bittensor allows Ridges to tap into a global pool of developers and researchers, each contributing their unique expertise to the problem. The open-source model ensures that the best ideas, regardless of their origin, can be integrated and built upon by the entire community.

2.Incentivizing a Hard Problem: Creating truly agentic AI is a long-term, research-intensive endeavor. The continuous emission of TAO provides a powerful and sustained incentive for miners to invest the time and resources required to push the boundaries of what is possible, a task that is difficult to fund in traditional startup or corporate environments.

3.Trust and Verifiability: The decentralized and transparent nature of Bittensor provides a level of trust that centralized, proprietary solutions cannot match. The performance of every agent is publicly verifiable, and the rules of the competition are enforced by the network, not a single company. This is crucial for a technology as powerful and transformative as autonomous AI.

**Path to Long-Term Adoption and Sustainable Business**

The go-to-market strategy for Ridges is phased to build momentum and establish a strong foundation for long-term sustainability.

- Phase 1: Serve the Bittensor Ecosystem: The immediate market for Ridges agents is the Bittensor network itself. Every subnet requires ongoing development, maintenance, and bug fixes. Ridges can provide a pool of on-demand, autonomous AI developers to perform these tasks, creating an internal, circular economy and demonstrating the agents' capabilities in a real-world production environment.
- Phase 2: Enterprise-Grade, Self-Hosted Solutions: The largest market for this technology is the enterprise sector. However, enterprises are highly sensitive about their code security. Ridges will address this by offering the top-performing, open-source agents as white-label, self-hosted solutions. Companies can deploy these powerful AI engineers entirely within their own secure infrastructure, eliminating the privacy concerns associated with cloud-based services. This provides a clear path to monetization that does not rely on transaction fees for TAO.
- Phase 3: A Decentralized Marketplace for AI Labor: As the agents become more sophisticated, Ridges will evolve into a decentralized marketplace where businesses can commission specific software development tasks. This creates a direct economic link between the real-world value generated by the agents and the incentives provided to the miners and validators, ensuring the long-term economic sustainability of the subnet.

By following this phased approach, Ridges can build a robust, self-sustaining ecosystem that not only advances the state of the art in AI but also delivers tangible economic value.

## 6. System Architecture and Workflows

**Architecture Overview**

The following diagram illustrates the high-level architecture and workflow of the Ridges subnet, showing the interaction between miners, validators, the platform, and the Bittensor blockchain.

![image.png](attachment:162f077f-5a56-48d9-8534-e2d33b830d46:image.png)

**Evaluation Workflow**

The following flowchart illustrates the complete evaluation process that an agent undergoes from submission to final scoring:

![image.png](attachment:df594904-a75c-4684-9d21-112863bbd1ac:image.png)

This multi-stage process ensures that only high-quality agents consume validator resources, while maintaining a rigorous and fair evaluation standard for top-performing agents.
hackquest logo
Home
Learn
Learning Progress
Explore Course
Build
My Hackathon
Explore Hackathon
Project Archive
Community
Forum
Global Event
My Community
Co-learning
Organization Hub
More
Glossary
Blog
Faucet
Job Station
Press Kit
Founder
Search for hackathon keywords, topics, etc...
coin
503

Anand Vashishtha
Back
Share Link
Bittensor Subnet Ideathon
Design the most promising subnet within the Bittensor ecosystem


Start Submit
Back
Overview
Prizes & Judging
Schedule
Resource
Project Gallery

Start Submit
Submission Countdown

Schedule Detail
01

D

00

H

37

M

56

S

To Do List

Telegram Group

Check Dev Docs

Bittensor Subnet Ideathon
Registration
1 days left
newbie
Host by

Bittensor

Hackathon Mode

ONLINE

Ecosystem

All
Tech Stack

All
Participants

Aaysuh Patel
Oluwagbohunmi Oredipe
Maulana Asykari Muhammad
523+
Community

≈ 23,000 USD

Available in Prizes


Detail Breakdown
Hackathon Winner
10,000 USD
Hackathon Runner-UP
3,000 USD
Subnet Ideathon Award X 5
1,000 USD
Discretionary Investment
≈ 260,000 USD
Basilica Compute Credit Award
≈ 5,000 USD
Description

Watch the introductory video by Jacob Robert Steeves, co-founder of Bittensor, to quickly learn more about Bittensor.


Watch the following video to learn what makes a great subnet.


Round I — Subnet Ideathon（Dec 23, 2025 - Feb 25, 2026）
Design the most promising subnet with strong incentive logic, clear miner/validator roles, and meaningful use cases within the Bittensor ecosystem.

Submission Requirements
Teams must submit:

1. Subnet Design Proposal
Format: PDF, Slides, Notion doc, or GitHub repository

Example Proposal：https://moonshotcommons.notion.site/Ideathon-Submission-Template-Ridges-AI-2e9e74465ed580c3b8f9d323ff156f7e?source=copy_link

The proposal should include:

Incentive & Mechanism Design

Emission and reward logic

Incentive alignment for miners and validators

Mechanisms to discourage low-quality or adversarial behavior

How this design when implemented as a subnet qualifies as a genuine “proof of intelligence” or more broadly at least a “proof of effort”

High-level algorithm describing task assignment, submission, validation, scoring, and reward allocation.

Miner Design

Miner tasks

Expected input → output format

Performance dimensions (e.g. quality, speed, accuracy)

Validator Design

Scoring and evaluation methodology

Evaluation cadence

Validator incentive alignment

Business Logic & Market Rationale

The problem the subnet aims to solve and why it matters

Competing solutions, both within the Bittensor ecosystem and outside of it

Why this use case is well-suited to a Bittensor subnet

Whether there is a plausible path to long-term adoption and sustainable business

Go-To-Market Strategy

Initial Target Users & Use Cases: Early adopters, pilot partners, or anchor use cases.

Distribution & Growth Channels

Incentives for Early Participation: Bootstrapping strategies for miners, validators, and users.

2. Explanation Video
5–10 minutes recommended

Walkthrough of architecture, mechanism design, and flows

3. Public Introduction Post
Short public introduction of the subnet idea on X (Twitter) or other public platforms

⚠️ No testnet or mainnet deployment is required at this stage.

4. Pitch Deck
Create a 10 pages bussiness pitch deck for your subnet.

Round I Outcome
7 teams will be selected based on idea quality and mechanism design.

Selected teams advance to Round II — Hackathon

Selected teams are expected to be announced on March 2nd.

Round II — Subnet Hackathon(Testnet) （Mar 2,2026 - Mar 30,2026）
Validate that strong subnet ideas can be implemented and operate meaningfully on the Bittensor testnet.

Only teams selected from Round I are eligible to participate.

Execution Requirements
Selected teams must:

Implement their proposed subnet or miner design on the Bittensor testnet

Demonstrate:

Functional miner and/or subnet logic

Working validator evaluation flow

Evidence that incentive mechanisms behave as intended

The winners of various awards are expected to be announced on March 31.

Polish is not necessary, but functional correctness and conceptual integrity are required.


About Bittensor

Bittensor is an open source platform where participants produce best-in-class digital commodities, including compute power, storage space, artificial intelligence (AI) inference and training, protein folding, financial markets prediction, and many more. Bittensor is composed of distinct subnets. Each subnet is an independent community of miners (who produce the commodity), and validators (who evaluate the miners' work). The Bittensor network constantly emits liquidity, in the form of its token, TAO (ττ), to participants in proportion to the value of their contributions. Explore the full vision at bittensor.com.

FAQs

Can teams continue working on their subnet after the ideathon?
Is execution on the testnet a requirement for the winner?
Is the ideathon held online or in person?
What is the exact timeline for Round One and Round Two?
Can’t find what you’re looking for? Reach out to us!
Twitter

Share Link
Bittensor Subnet Ideathon
Design the most promising subnet within the Bittensor ecosystem


Start Submit
Overview
Prizes & Judging
Schedule
Resource
Project Gallery
Hackathon Winner

10,000 USD

Hackathon Winner

Awarded to the team whose subnet demonstrates the strongest overall design and execution on testnet.（1 winner）

Includes Bonus Non-Cash Rewards

Direct entry into the Bitstarter Accelerator

Exclusive pitch session with Jacob Robert Steeves (Co-Founder of Bittensor) or Etienne (President, Opentensor Foundation)

Hackathon Winner

Judging Criteria

Quality and robustness of incentive and mechanism design

Clear definition of miner and validator roles, tasks, and evaluation logic

Relevance and credibility of the subnet use case within the Bittensor ecosystem

Consistency between proposed design and observed testnet behavior

Overall coherence of idea, execution, and outcomes

Hackathon Runner-UP

3,000 USD

Hackathon Runner-UP

Awarded to teams showing strong technical execution, stability, or performance on testnet.（1 winner）


Includes Bonus Non-Cash Rewards

Interview opportunity with Bitstarter

Hackathon Runner-UP

Judging Criteria

Functional correctness and stability of the subnet or miner implementation

Reliability of miner–validator interactions and evaluation flow

Engineering quality, architecture choices, and implementation judgment

Performance, robustness, and operational soundness on testnet

Subnet Ideathon Award X 5

1,000 USD

Subnet Ideathon Award X 5

Awarded for exceptional subnet design（5 Winners/1000 USD/Each）


Includes Bonus Non-Cash Rewards

Interview opportunity with Bitstarter

Subnet Ideathon Award X 5

Judging Criteria

Novelty and originality of incentive, scoring, or coordination design

Clarity and soundness of the underlying mechanism logic

Evidence from testnet execution that the mechanism works as intended

Insightfulness and potential impact on future subnet design

Discretionary Investment

1000 TAO( ≈260,000 USD)

Discretionary Investment

Unsupervised Capital is on the lookout for standout teams building on the Bittensor network. If an exceptional project emerges from this hackathon, UC may explore a discretionary investment of up to 1,000 TAO, along with strategic guidance from one of the most trusted funds in the ecosystem.

Discretionary Investment

Judging Criteria

Judging Mode

Voting Mode

Basilica Compute Credit Award

Compute Credits( ≈5,000 USD)

Basilica Compute Credit Award

Awarded to teams demonstrating strong technical execution and real compute demand for decentralized AI workloads on Bittensor.

This prize is sponsored by Basilica (Bittensor Subnet 39) and is designed to support teams actively building, testing, and scaling their subnet or miner implementations with real-world compute usage.

Prize Breakdown
Round II Qualifiers: 7x $500 compute credits

7 winners

$500 in Basilica compute credits per team

Awarded to teams advancing to Round II — Subnet Hackathon

Overall Winner: 1x $1,500 compute credits

1 winner

$1,500 in Basilica compute credits

Awarded to the 1st place team of the hackathon

Basilica Compute Credit Award

Judging Criteria

Judging Mode

Voting Mode

Disclaimer：
HackQuest, as a hackathon platform provider, is not involved in the final judging and reward distribution for any hackathon unless specifically stated otherwise. If no or few submissions meet the host's quality standards, the prize may be subject to change.
Overview
Prizes & Judging
Schedule
Resource
Project Gallery
Schedule

Live
Registration
Dec 23,2025 15:30 - Feb 25,2026 21:29
Live
Submission
Dec 23,2025 15:30 - Feb 25,2026 21:29
Round I — Subnet Ideation Starts
Dec 22,2025 21:30 - Dec 23,2025 21:29
Round I — Subnet Ideation Ends
Feb 24,2026 21:30 - Feb 25,2026 21:29
Round One Winner Announcement
Mar 1,2026 21:30 - Mar 2,2026 21:29
Round II — Bittensor Subnet Hackathon Starts
Mar 1,2026 21:30 - Mar 1,2026 21:31
Round II — Bittensor Subnet Hackathon Ends
Mar 29,2026 21:30 - Mar 30,2026 21:29
Bittensor Subnet Ideathon Demo Day(Tentative)
Mar 31,2026 16:30 - Mar 31,2026 18:30
Upcoming
Reward Announcement
Mar 31,2026 15:30
Build On Bittensor Kolkata Workshop
Dec 27,2025 06:00 - Dec 27,2025 10:00
Build On Bittensor IIT Delhi Workshop
Jan 18,2026 06:00 - Jan 18,2026 10:00
Build On Bittensor Jakarta Workshop
Jan 24,202
Resource

Bittensor Documentation
Bittensor is an open source platform where participants produce best-in-class digital commodities, including compute power, storage space, artificial intelligence (AI) inference and training, protein folding, financial markets prediction, and many more.

Bittensor is composed of distinct subnets. Each subnet is an independent community of miners (who produce the commodity), and validators (who evaluate the miners' work).

The Bittensor network constantly emits liquidity, in the form of its token, TAO (ττ), to participants in proportion to the value of their contributions. Participants include:

Miners—Work to produce digital commodities. See mining in Bittensor. https://docs.learnbittensor.org/miners

Validators—Evaluate the quality of miners' work. See validating in Bittensor.

https://docs.learnbittensor.org/validators

Subnet Creators—Manage the incentive mechanisms that specify the work miners and validate must perform and evaluate, respectively. See Create a Subnet

https://docs.learnbittensor.org/subnets/create-a-subnet

Stakers—TAO holders can support specific validators by staking TAO to them. See Staking.

https://docs.learnbittensor.org/staking-and-delegation/delegation

Start Submit
Overview
Prizes & Judging
Schedule
Resource
Project Gallery

Prize Track

Sector

Tech Stack

Clear all

Latest to oldest


1508
Trustensor
The missing trust layer for the AI agent economy. A Bittensor subnet where miners compete to score agent safety, feeding wallets, DeFi protocols, and ERC-8004.

Tech Stack
Python
Builder

Abyan Yusuf
AI
Infra
DeFi


961
Hooshdan Research Hub
Hooshdan Research Hub is a Bittensor subnet that decentralizes AI research by rewarding original, ArXiv-verified "masterpiece" papers with $TAO.

Tech Stack
Python
Builder

Mahmoud Mohajer
AI


547
MuseNet
Human Music Creation & Curation

Tech Stack
Python
Builder

Luke Culpitt
RWA


138
MAARIFA
emerging market onchain intelligence subnet

Tech Stack
Web3
Builder

Papa Jams
AI
Infra
SocialFi


1458
NexusNet Project
NexusNet is not another agent subnet. It is the benchmark that proves which AI agents are genuinely intelligent — by stress-testing them under real uncertainty and recording cryptographic proof

Tech Stack
React
Builder

Wisdom Chris
DAO
AI


3277
τaochaτ
Secure, privacy-first communication subnet on Bittensor

Tech Stack
Python
Builder

Daniel Leon
SocialFi
AI


129
Prometheus
Decentralized Intelligence for Scientific Discovery

Tech Stack
Web3
Builder

Kanish Patidar
Other


4018
SPICE
SPICE is a Bittensor subnet where competing AI models analyze satellite imagery, delivering enterprise earth observation at a fraction of centralized costs.

Tech Stack
Python
Builder

Dhia El Hak Ben Dahmeni
AI


3278
MCP Nexus
A decentralized registry and verification layer for MCP

Tech Stack
Bittensor SDK
Builder

Luke Culpitt
Infra
AI


5
VoiceNet
VoiceNet: decentralized audio & video transcription on Bittensor. GPU miners compete on accuracy, validators score WER, TAO rewards flow to the best. 70% cheaper than Whisper API.

Tech Stack
Python
Builder

Al Hadad
Infra
AI


1012
Project Excalibur
Decentralized alchemical framework atop Bittensor: Incentive-aligned purification of training nodes via classical stages. Miners recharge, validators resonate. Building the living Order.

Tech Stack
Python
Builder

Randall Sayers
AI
Infra
DeFi
DAO


1018
Moirai Subnet
The Protocol of Living IP & Consensus Reality

Tech Stack
Bittensor
Builder

Kael Reed
AI
DAO

Previous
1
2
3
4
8

Next